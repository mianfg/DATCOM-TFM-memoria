
@misc{abshari_invarllm_2024,
	title = {{INVARLLM}: {LLM}-assisted {Physical} {Invariant} {Extraction} for {Cyber}-{Physical} {Systems} {Anomaly} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{INVARLLM}},
	url = {https://arxiv.org/abs/2411.10918},
	doi = {10.48550/ARXIV.2411.10918},
	abstract = {Cyber-Physical Systems (CPS) are vulnerable to cyber-physical attacks that violate physical laws. While invariant-based anomaly detection is effective, existing methods are limited: data-driven approaches lack semantic context, and physics-based models require extensive manual work. We propose INVARLLM, a hybrid framework that uses large language models (LLMs) to extract semantic information from CPS documentation and generate physical invariants, then validates these against real system data using a PCMCI+-inspired K-means method. This approach combines LLM semantic understanding with empirical validation to ensure both interpretability and reliability. We evaluate INVARLLM on SWaT and WADI datasets, achieving 100\% precision in anomaly detection with no false alarms, outperforming all existing methods. Our results demonstrate that integrating LLM-derived semantics with statistical validation provides a scalable and dependable solution for CPS security.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Abshari, Danial and Shi, Peiran and Fu, Chenglong and Sridhar, Meera and Du, Xiaojiang},
	year = {2024},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Cryptography and Security (cs.CR), FOS: Computer and information sciences},
}

@inproceedings{zheng_sglang_2024,
	title = {{SGLang}: {Efficient} {Execution} of {Structured} {Language} {Model} {Programs}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/724be4472168f31ba1c9ac630f15dec8-Paper-Conference.pdf},
	doi = {10.52202/079017-2000},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E. and Barrett, Clark and Sheng, Ying},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {62557--62583},
}

@inproceedings{zhang_assessing_2024,
	title = {Assessing the {Code} {Clone} {Detection} {Capability} of {Large} {Language} {Models}},
	doi = {10.1109/ICCQ60895.2024.10576803},
	booktitle = {2024 4th {International} {Conference} on {Code} {Quality} ({ICCQ})},
	author = {Zhang, Zixian and Saber, Takfarinas},
	year = {2024},
	keywords = {Accuracy, Analytical models, Cloning, Code Clone Detection, Codes, Correlation, GPT-3.5, GPT-4, Large Language Models (LLMs), Semantic Analysis, Software, Task analysis},
	pages = {75--83},
}

@inproceedings{yuan_signal--noise_2019,
	title = {Signal-{To}-{Noise} {Ratio}: {A} {Robust} {Distance} {Metric} for {Deep} {Metric} {Learning}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yuan, Tongtong and Deng, Weihong and Tang, Jian and Tang, Yinan and Chen, Binghui},
	month = jun,
	year = {2019},
}

@inproceedings{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	url = {https://openreview.net/forum?id=WE_vluYUL-X},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R. and Cao, Yuan},
	year = {2023},
}

@inproceedings{xia_control_2025,
	title = {Control {Industrial} {Automation} {System} with {Large} {Language} {Model} {Agents}},
	doi = {10.1109/ETFA65518.2025.11205539},
	booktitle = {2025 {IEEE} 30th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Xia, Yuchen and Jazdi, Nasser and Zhang, Jize and Shah, Chaitanya and Weyrich, Michael},
	year = {2025},
	keywords = {Automation, Cognition, Control systems, Digital Twin, Industrial Automation System, Intelligent Robotics, Large Language Model, Large language models, Multi-Agent System, Natural languages, System analysis and design, Systems operation, Testing, Training, Videos},
	pages = {1--8},
}

@inproceedings{xia_enhance_2024,
	title = {Enhance {FMEA} with {Large} {Language} {Models} for {Assisted} {Risk} {Management} in {Technical} {Processes} and {Products}},
	doi = {10.1109/ETFA61755.2024.10710996},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Xia, Yuchen and Jazdi, Nasser and Weyrich, Michael},
	year = {2024},
	keywords = {Cognitive load, Data mining, Databases, FMEA, Intelligent Automation, Large Language Models, Large language models, Manufacturing automation, Multi-Agent System, Multi-agent systems, Prototypes, Risk Management, Risk management, Software development management},
	pages = {1--4},
}

@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {24824--24837},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{su_roformer_2024,
	title = {{RoFormer}: {Enhanced} transformer with {Rotary} {Position} {Embedding}},
	volume = {568},
	issn = {09252312},
	shorttitle = {{RoFormer}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231223011864},
	doi = {10.1016/j.neucom.2023.127063},
	language = {en},
	urldate = {2025-12-10},
	journal = {Neurocomputing},
	author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
	month = feb,
	year = {2024},
	pages = {127063},
}

@inproceedings{shin_constrained_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Constrained {Language} {Models} {Yield} {Few}-{Shot} {Semantic} {Parsers}},
	url = {https://aclanthology.org/2021.emnlp-main.608},
	doi = {10.18653/v1/2021.emnlp-main.608},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shin, Richard and Lin, Christopher and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Van Durme, Benjamin},
	year = {2021},
	pages = {7699--7715},
}

@inproceedings{shi_large_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/shi23a.html},
	abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the {\textless}em{\textgreater}distractibility{\textless}/em{\textgreater} of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H. and Schärli, Nathanael and Zhou, Denny},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {31210--31227},
}

@inproceedings{russell-gilbert_aad-llm_2024,
	title = {{AAD}-{LLM}: {Adaptive} {Anomaly} {Detection} {Using} {Large} {Language} {Models}},
	doi = {10.1109/BigData62323.2024.10825679},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Russell-Gilbert, Alicia and Sommers, Alexander and Thompson, Andrew and Cummins, Logan and Mittal, Sudip and Rahimi, Shahram and Seale, Maria and Jaboure, Joseph and Arnold, Thomas and Church, Joshua},
	year = {2024},
	keywords = {Adaptation models, Anomaly detection, Data models, Decision making, LLMs for time series tasks, Large language models, Manufacturing systems, Semantics, Systematic literature review, Time series analysis, Training, adaptive anomaly detection, large language models, predictive maintenance},
	pages = {4194--4203},
}

@inproceedings{rula_procedural_2023,
	address = {Pensacola FL USA},
	title = {Procedural {Text} {Mining} with {Large} {Language} {Models}},
	isbn = {979-8-4007-0141-2},
	url = {https://dl.acm.org/doi/10.1145/3587259.3627572},
	doi = {10.1145/3587259.3627572},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 12th {Knowledge} {Capture} {Conference} 2023},
	publisher = {ACM},
	author = {Rula, Anisa and D'Souza, Jennifer},
	month = dec,
	year = {2023},
	pages = {9--16},
}

@inproceedings{qin_large_2024,
	address = {Mexico City, Mexico},
	title = {Large {Language} {Models} are {Effective} {Text} {Rankers} with {Pairwise} {Ranking} {Prompting}},
	url = {https://aclanthology.org/2024.findings-naacl.97},
	doi = {10.18653/v1/2024.findings-naacl.97},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Yan, Le and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and Bendersky, Michael},
	year = {2024},
	pages = {1504--1518},
}

@inproceedings{muennighoff_mteb_2023,
	address = {Dubrovnik, Croatia},
	title = {{MTEB}: {Massive} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MTEB}},
	url = {https://aclanthology.org/2023.eacl-main.148},
	doi = {10.18653/v1/2023.eacl-main.148},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loic and Reimers, Nils},
	year = {2023},
	pages = {2014--2037},
}

@article{malkov_efficient_2020,
	title = {Efficient and {Robust} {Approximate} {Nearest} {Neighbor} {Search} {Using} {Hierarchical} {Navigable} {Small} {World} {Graphs}},
	volume = {42},
	doi = {10.1109/TPAMI.2018.2889473},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	year = {2020},
	keywords = {Approximation algorithms, Biological system modeling, Brain modeling, Complexity theory, Data models, Graph and tree search strategies, Routing, Search problems, approximate search, artificial intelligence, big data, data structures, graphs and networks, information search and retrieval, information storage and retrieval, information technology and systems, nearest neighbor search, search process, similarity search},
	pages = {824--836},
}

@article{liu_lost_2024,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	volume = {12},
	issn = {2307-387X},
	shorttitle = {Lost in the {Middle}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long},
	doi = {10.1162/tacl_a_00638},
	abstract = {Abstract
            While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	language = {en},
	urldate = {2025-12-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = feb,
	year = {2024},
	pages = {157--173},
}

@inproceedings{li_musc_2024,
	title = {{MuSc}: {Zero}-{Shot} {Industrial} {Anomaly} {Classification} and {Segmentation} with {Mutual} {Scoring} of the {Unlabeled} {Images}},
	url = {https://openreview.net/forum?id=AHgc5SMdtd},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Li, Xurui and Huang, Ziming and Xue, Feng and Zhou, Yu},
	year = {2024},
}

@inproceedings{li_zero-shot_2023,
	title = {Zero-{Shot} {Anomaly} {Detection} via {Batch} {Normalization}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8078e8c3055303a884ffae2d3ea00338-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Aodong and Qiu, Chen and Kloft, Marius and Smyth, Padhraic and Rudolph, Maja and Mandt, Stephan},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {40963--40993},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {9459--9474},
}

@inproceedings{kusupati_matryoshka_2022,
	title = {Matryoshka {Representation} {Learning}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {30233--30249},
}

@inproceedings{kuratov_cramming_2025,
	address = {Vienna, Austria},
	title = {Cramming 1568 {Tokens} into a {Single} {Vector} and {Back} {Again}: {Exploring} the {Limits} of {Embedding} {Space} {Capacity}},
	shorttitle = {Cramming 1568 {Tokens} into a {Single} {Vector} and {Back} {Again}},
	url = {https://aclanthology.org/2025.acl-long.948},
	doi = {10.18653/v1/2025.acl-long.948},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 63rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kuratov, Yuri and Arkhipov, Mikhail and Bulatov, Aydar and Burtsev, Mikhail},
	year = {2025},
	pages = {19323--19339},
}

@inproceedings{kojima_large_2022,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {22199--22213},
}

@article{joel_survey_2025,
	title = {A {Survey} on {LLM}-based {Code} {Generation} for {Low}-{Resource} and {Domain}-{Specific} {Programming} {Languages}},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3770084},
	doi = {10.1145/3770084},
	abstract = {Large Language Models (LLMs) have shown remarkable capabilities in code generation for popular programming languages. However, their performance in Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a critical challenge. This gap affects millions of developers - with Rust alone having 3.5 million users - who are currently unable to fully leverage LLM capabilities. LRPLs and DSLs face unique challenges, including severe data scarcity and, for DSLs, highly specialized syntax and semantics that are poorly represented in general-purpose datasets. Addressing these challenges is crucial as LRPLs and DSLs significantly enhance development efficiency in specialized domains and applications, including financial and scientific works. While several surveys on LLMs for software engineering and code exist, none comprehensively address the challenges and opportunities specific to LRPLs and DSLs. Our survey fills this gap by providing a systematic review of the current state, methodologies, and challenges in leveraging LLMs for code generation in LRPL and DSL. We filtered 111 papers from over 27,000 published studies from 2020 – 2024 to understand the capabilities and limitations of LLMs in these specialized domains. We also expanded our literature search to include 5 recent papers from 2024 – 2025. We report LLMs used, benchmarks, and metrics to evaluate code generation in LRPLs and DSLs, as well as strategies used to enhance LLM performance, and the collected datasets and curation methods in this context.
            
              We identified four main evaluation techniques used in the literature, along with several metrics to assess code generation in LRPL and DSL. We categorized the methods used for LLM improvement into six main groups and summarized the novel methods and architectures proposed by the researchers. We also classified different approaches used for data collection and preparation. While different techniques, metrics, and datasets are used, there is a lack of a standard approach and a benchmark dataset to evaluate code generation in several LRPLs and DSLs. We discuss several distinctions of the studied approaches with the ones used in high-resource programming languages (HRPLs), as well as several challenges unique to these languages, especially DSLs. The challenges stem from the scarcity of data, the unique requirements, and specialized domains, which often need expertise guidelines or domain-specific tools. Accordingly, we provide insights into different research opportunities for the studied aspects. This survey serves as a comprehensive resource for researchers and practitioners working at the intersection of LLMs, software engineering, and specialized programming languages, providing a foundation for future advancements in LRPL and DSL code generation. A GitHub repository was created to organize the papers of this survey at
              https://github.com/jie-jw-wu/Survey-CodeLLM4LowResource-DSL
              .},
	language = {en},
	urldate = {2025-12-10},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Joel, Sathvik and Wu, Jie and Fard, Fatemeh},
	month = oct,
	year = {2025},
	pages = {3770084},
}

@misc{gill_leveraging_2025,
	title = {Leveraging {LLM} {Agents} and {Digital} {Twins} for {Fault} {Handling} in {Process} {Plants}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2505.02076},
	doi = {10.48550/ARXIV.2505.02076},
	abstract = {Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Gill, Milapji Singh and Vyas, Javal and Markaj, Artan and Gehlhoff, Felix and Mercangöz, Mehmet},
	year = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Multiagent Systems (cs.MA)},
}

@inproceedings{chu_timebench_2024,
	address = {Bangkok, Thailand},
	title = {{TimeBench}: {A} {Comprehensive} {Evaluation} of {Temporal} {Reasoning} {Abilities} in {Large} {Language} {Models}},
	shorttitle = {{TimeBench}},
	url = {https://aclanthology.org/2024.acl-long.66},
	doi = {10.18653/v1/2024.acl-long.66},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and Wang, Haotian and Liu, Ming and Qin, Bing},
	year = {2024},
	pages = {1204--1228},
}

@article{chang_survey_2024,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {15},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/3641289},
	doi = {10.1145/3641289},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions:
              what to evaluate
              ,
              where to evaluate
              , and
              how to evaluate
              . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:
              https://github.com/MLGroupJLU/LLM-eval-survey},
	language = {en},
	number = {3},
	urldate = {2025-12-10},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = jun,
	year = {2024},
	pages = {1--45},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	doi = {10.48550/arXiv.1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhou_steerconf_2025,
	title = {{SteerConf}: {Steering} {LLMs} for {Confidence} {Elicitation}},
	shorttitle = {{SteerConf}},
	url = {http://arxiv.org/abs/2503.02863},
	doi = {10.48550/arXiv.2503.02863},
	abstract = {Large Language Models (LLMs) exhibit impressive performance across diverse domains but often suffer from overconfidence, limiting their reliability in critical applications. We propose SteerConf, a novel framework that systematically steers LLMs' confidence scores to improve their calibration and reliability. SteerConf introduces three key components: (1) a steering prompt strategy that guides LLMs to produce confidence scores in specified directions (e.g., conservative or optimistic) by leveraging prompts with varying steering levels; (2) a steered confidence consistency measure that quantifies alignment across multiple steered confidences to enhance calibration; and (3) a steered confidence calibration method that aggregates confidence scores using consistency measures and applies linear quantization for answer selection. SteerConf operates without additional training or fine-tuning, making it broadly applicable to existing LLMs. Experiments on seven benchmarks spanning professional knowledge, common sense, ethics, and reasoning tasks, using advanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf significantly outperforms existing methods, often by a significant margin. Our findings highlight the potential of steering the confidence of LLMs to enhance their reliability for safer deployment in real-world applications.},
	urldate = {2025-06-01},
	publisher = {arXiv},
	author = {Zhou, Ziang and Jin, Tianyuan and Shi, Jieming and Li, Qing},
	month = may,
	year = {2025},
	note = {arXiv:2503.02863 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{brown_enhancing_2024,
	title = {Enhancing {Trust} in {LLMs}: {Algorithms} for {Comparing} and {Interpreting} {LLMs}},
	shorttitle = {Enhancing {Trust} in {LLMs}},
	url = {http://arxiv.org/abs/2406.01943},
	doi = {10.48550/arXiv.2406.01943},
	abstract = {This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation. Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Brown, Nik Bear},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01943 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{awasthi_humanely_2023,
	title = {{HumanELY}: {Human} evaluation of {LLM} yield, using a novel web-based evaluation tool},
	copyright = {http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{HumanELY}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2023.12.22.23300458},
	doi = {10.1101/2023.12.22.23300458},
	abstract = {A
            bstract
          
          Large language models (LLMs) have caught the imagination of researchers,developers and public in general the world over with their potential for transformation. Vast amounts of research and development resources are being provided to implement these models in all facets of life. Trained using billions of parameters, various measures of their accuracy and performance have been proposed and used in recent times. While many of the automated natural language assessment parameters measure LLM output performance for use of language, contextual outputs are still hard to measure and quantify. Hence, human evaluation is still an important measure of LLM performance,even though it has been applied variably and inconsistently due to lack of guidance and resource limitations.
          To provide a structured way to perform comprehensive human evaluation of LLM output, we propose the first guidance and tool called HumanELY. Our approach and tool built using prior knowledge helps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable manner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm and comparison. Additional submetrics within these five key metrics provide for Likert scale based human evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable LLM evaluation and provide data for comparison against different users performing human evaluation. While all metrics may not be relevant and pertinent to all outputs, it is important to assess and address their use.
          Lastly, we demonstrate comparison of metrics used in HumanELY against some of the recent publications in the healthcare domain. We focused on the healthcare domain due to the need to demonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We anticipate our guidance and tool to be used for any domain where LLMs find an use case.
          
            Link to the HumanELY Tool
            
              https://www.brainxai.com/humanely},
	language = {en},
	urldate = {2025-06-04},
	publisher = {Health Informatics},
	author = {Awasthi, Raghav and Mishra, Shreya and Mahapatra, Dwarikanath and Khanna, Ashish and Maheshwari, Kamal and Cywinski, Jacek and Papay, Frank and Mathur, Piyush},
	month = dec,
	year = {2023},
}

@misc{ansari_chronos_2024,
	title = {Chronos: {Learning} the {Language} of {Time} {Series}},
	shorttitle = {Chronos},
	url = {http://arxiv.org/abs/2403.07815},
	doi = {10.48550/arXiv.2403.07815},
	abstract = {We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.},
	urldate = {2025-05-18},
	publisher = {arXiv},
	author = {Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Wang, Hao and Mahoney, Michael W. and Torkkola, Kari and Wilson, Andrew Gordon and Bohlke-Schneider, Michael and Wang, Yuyang},
	month = nov,
	year = {2024},
	note = {arXiv:2403.07815 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{russell-gilbert_aad-llm_2024-1,
	title = {{AAD}-{LLM}: {Adaptive} {Anomaly} {Detection} {Using} {Large} {Language} {Models}},
	shorttitle = {{AAD}-{LLM}},
	url = {http://arxiv.org/abs/2411.00914},
	doi = {10.48550/arXiv.2411.00914},
	abstract = {For data-constrained, complex and dynamic industrial environments, there is a critical need for transferable and multimodal methodologies to enhance anomaly detection and therefore, prevent costs associated with system failures. Typically, traditional PdM approaches are not transferable or multimodal. This work examines the use of Large Language Models (LLMs) for anomaly detection in complex and dynamic manufacturing systems. The research aims to improve the transferability of anomaly detection models by leveraging Large Language Models (LLMs) and seeks to validate the enhanced effectiveness of the proposed approach in data-sparse industrial applications. The research also seeks to enable more collaborative decision-making between the model and plant operators by allowing for the enriching of input series data with semantics. Additionally, the research aims to address the issue of concept drift in dynamic industrial settings by integrating an adaptability mechanism. The literature review examines the latest developments in LLM time series tasks alongside associated adaptive anomaly detection methods to establish a robust theoretical framework for the proposed architecture. This paper presents a novel model framework (AAD-LLM) that doesn't require any training or finetuning on the dataset it is applied to and is multimodal. Results suggest that anomaly detection can be converted into a "language" task to deliver effective, context-aware detection in data-constrained industrial applications. This work, therefore, contributes significantly to advancements in anomaly detection methodologies.},
	urldate = {2025-05-18},
	publisher = {arXiv},
	author = {Russell-Gilbert, Alicia and Sommers, Alexander and Thompson, Andrew and Cummins, Logan and Mittal, Sudip and Rahimi, Shahram and Seale, Maria and Jaboure, Joseph and Arnold, Thomas and Church, Joshua},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00914 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning},
}

@misc{russell-gilbert_raad-llm_2025,
	title = {{RAAD}-{LLM}: {Adaptive} {Anomaly} {Detection} {Using} {LLMs} and {RAG} {Integration}},
	shorttitle = {{RAAD}-{LLM}},
	url = {http://arxiv.org/abs/2503.02800},
	doi = {10.48550/arXiv.2503.02800},
	abstract = {Anomaly detection in complex industrial environments poses unique challenges, particularly in contexts characterized by data sparsity and evolving operational conditions. Predictive maintenance (PdM) in such settings demands methodologies that are adaptive, transferable, and capable of integrating domain-specific knowledge. In this paper, we present RAAD-LLM, a novel framework for adaptive anomaly detection, leveraging large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach addresses the aforementioned PdM challenges. By effectively utilizing domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time series data without requiring fine-tuning on specific datasets. The framework's adaptability mechanism enables it to adjust its understanding of normal operating conditions dynamically, thus increasing detection accuracy. We validate this methodology through a real-world application for a plastics manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show significant improvements over our previous model with an accuracy increase from 70.7\% to 88.6\% on the real-world dataset. By allowing for the enriching of input series data with semantics, RAAD-LLM incorporates multimodal capabilities that facilitate more collaborative decision-making between the model and plant operators. Overall, our findings support RAAD-LLM's ability to revolutionize anomaly detection methodologies in PdM, potentially leading to a paradigm shift in how anomaly detection is implemented across various industries.},
	urldate = {2025-05-18},
	publisher = {arXiv},
	author = {Russell-Gilbert, Alicia and Mittal, Sudip and Rahimi, Shahram and Seale, Maria and Jabour, Joseph and Arnold, Thomas and Church, Joshua},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02800 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning},
}

@misc{alimin_talking_2025,
	title = {Talking like {Piping} and {Instrumentation} {Diagrams} ({P}\&{IDs})},
	url = {http://arxiv.org/abs/2502.18928},
	doi = {10.48550/arXiv.2502.18928},
	abstract = {We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P\&IDs) using natural language. In particular, we represent P\&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P\&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P\&ID knowledge graphs from pyDEXPI. 3) Integration of the P\&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P\&IDs using natural language. It extends LLM's ability to retrieve contextual data from P\&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P\&IDs, and AI-assisted HAZOP studies.},
	urldate = {2025-05-08},
	publisher = {arXiv},
	author = {Alimin, Achmad Anggawirya and Goldstein, Dominik P. and Balhorn, Lukas Schulze and Schweidtmann, Artur M.},
	month = feb,
	year = {2025},
	note = {arXiv:2502.18928 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{fakih_llm4plc_2024,
	address = {Lisbon Portugal},
	title = {{LLM4PLC}: {Harnessing} {Large} {Language} {Models} for {Verifiable} {Programming} of {PLCs} in {Industrial} {Control} {Systems}},
	isbn = {979-8-4007-0501-4},
	shorttitle = {{LLM4PLC}},
	url = {https://dl.acm.org/doi/10.1145/3639477.3639743},
	doi = {10.1145/3639477.3639743},
	language = {en},
	urldate = {2025-06-02},
	booktitle = {Proceedings of the 46th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Fakih, Mohamad and Dharmaji, Rahul and Moghaddas, Yasamin and Quiros, Gustavo and Ogundare, Oluwatosin and Al Faruque, Mohammad Abdullah},
	month = apr,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
	pages = {192--203},
}

@article{dagdelen_structured_2024,
	title = {Structured information extraction from scientific text with large language models},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-45563-x},
	doi = {10.1038/s41467-024-45563-x},
	abstract = {Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.},
	language = {en},
	number = {1},
	urldate = {2025-05-07},
	journal = {Nature Communications},
	author = {Dagdelen, John and Dunn, Alexander and Lee, Sanghoon and Walker, Nicholas and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin A. and Jain, Anubhav},
	month = feb,
	year = {2024},
	keywords = {Databases, Materials science, Scientific data, Theory and computation},
	pages = {1418},
}

@misc{song_pre-trained_2023,
	title = {Pre-{Trained} {Large} {Language} {Models} for {Industrial} {Control}},
	url = {http://arxiv.org/abs/2308.03028},
	doi = {10.48550/arXiv.2308.03028},
	abstract = {For industrial control, developing high-performance controllers with few samples and low technical debt is appealing. Foundation models, possessing rich prior knowledge obtained from pre-training with Internet-scale corpus, have the potential to be a good controller with proper prompts. In this paper, we take HVAC (Heating, Ventilation, and Air Conditioning) building control as an example to examine the ability of GPT-4 (one of the first-tier foundation models) as the controller. To control HVAC, we wrap the task as a language game by providing text including a short description for the task, several selected demonstrations, and the current observation to GPT-4 on each step and execute the actions responded by GPT-4. We conduct series of experiments to answer the following questions: 1){\textasciitilde}How well can GPT-4 control HVAC? 2){\textasciitilde}How well can GPT-4 generalize to different scenarios for HVAC control? 3) How different parts of the text context affect the performance? In general, we found GPT-4 achieves the performance comparable to RL methods with few samples and low technical debt, indicating the potential of directly applying foundation models to industrial control tasks.},
	urldate = {2025-06-01},
	publisher = {arXiv},
	author = {Song, Lei and Zhang, Chuheng and Zhao, Li and Bian, Jiang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03028 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{abshari_llm-assisted_2024,
	title = {{LLM}-assisted {Physical} {Invariant} {Extraction} for {Cyber}-{Physical} {Systems} {Anomaly} {Detection}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2411.10918},
	doi = {10.48550/ARXIV.2411.10918},
	abstract = {Modern industrial infrastructures rely heavily on Cyber-Physical Systems (CPS), but these are vulnerable to cyber-attacks with potentially catastrophic effects. To reduce these risks, anomaly detection methods based on physical invariants have been developed. However, these methods often require domain-specific expertise to manually define invariants, making them costly and difficult to scale. To address this limitation, we propose a novel approach to extract physical invariants from CPS testbeds for anomaly detection. Our insight is that CPS design documentation often contains semantically rich descriptions of physical procedures, which can profile inter-correlated dynamics among system components. Leveraging the built-in physics and engineering knowledge of recent generative AI models, we aim to automate this traditionally manual process, improving scalability and reducing costs. This work focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG) workflow with a customized prompting system tailored for CPS documentation, enabling accurate extraction of semantic information and inference of physical invariants from complex, multimodal content. Then, rather than directly applying the inferred invariants for anomaly detection, we introduce an innovative statistics-based learning approach that integrates these invariants into the training dataset. This method addresses limitations such as hallucination and concept drift, enhancing the reliability of the model. We evaluate our approach on real-world public CPS security dataset which contains 86 data points and 58 attacking cases. The results show that our approach achieves a high precision of 0.923, accurately detecting anomalies while minimizing false alarms.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Abshari, Danial and Fu, Chenglong and Sridhar, Meera},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Cryptography and Security (cs.CR), FOS: Computer and information sciences},
}

@inproceedings{magnini_understanding_2024,
	address = {Pisa, Italy},
	title = {Understanding {High}-complexity {Technical} {Documents} with {State}-of-{Art} {Models}},
	isbn = {979-12-210-7060-6},
	url = {https://aclanthology.org/2024.clicit-1.64/},
	abstract = {Technical documents, particularly those in civil engineering, contain crucial information that supports critical decision-making in construction, transportation and infrastructure projects. Large language models (LLMs) offer a promising solution for automating the extraction and comprehension of technical documents, potentially transforming our interaction with technical information. However, LLMs may encounter significant challenges when processing technical documents due to their complex structure, specialized terminology and reliance on graphical and visual elements. Moreover, LLMs are known to sometimes produce unexpected or incorrect analyses, a phenomenon referred to as hallucination.This study explores the potential of state-of-the-art LLMs, specifically GPT-4omni, to automate the comprehension of technical documents. The evaluation was performed on two types of PDF documents. The first type is selectable text PDFs, which are extractable and editable, focusing on civil engineering documents from the Italian state railways. The second type is scanned OCR PDFs, where text is derived from scanning or OCR, specifically focusing on the design of an outdoor swimming pool. These documents include textual and visual elements such as tables, figures and photos. Our findings suggest that GPT-4omni has a high potential for real-world use, although it may still be susceptible to producing misleading information.},
	booktitle = {Proceedings of the {Tenth} {Italian} {Conference} on {Computational} {Linguistics} ({CLiC}-it 2024)},
	publisher = {CEUR Workshop Proceedings},
	author = {Magnini, Bernardo and Zanoli, Roberto},
	editor = {Dell'Orletta, Felice and Lenci, Alessandro and Montemagni, Simonetta and Sprugnoli, Rachele},
	month = dec,
	year = {2024},
	pages = {540--547},
}

@inproceedings{tufek_semantic_2023,
	address = {Aveiro, Portugal},
	title = {Semantic {Information} {Extraction} from {Multi}-modal {Technical} {Document}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-989-33-4792-8},
	url = {https://ieeexplore.ieee.org/document/10211635/},
	doi = {10.23919/CISTI58278.2023.10211635},
	urldate = {2025-12-08},
	booktitle = {2023 18th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	publisher = {IEEE},
	author = {Tufek, Nilay},
	month = jun,
	year = {2023},
	pages = {1--4},
}

@article{wagner_knowledge_2002,
	title = {Knowledge acquisition for expert systems in accounting and financial problem domains},
	volume = {15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705102000266},
	doi = {10.1016/S0950-7051(02)00026-6},
	language = {en},
	number = {8},
	urldate = {2025-12-08},
	journal = {Knowledge-Based Systems},
	author = {Wagner, W.P and Otto, J and Chung, Q.B},
	month = nov,
	year = {2002},
	pages = {439--447},
}

@misc{hadar_tavily_2025,
	title = {Tavily {Evaluation} {Part} 1: {Tavily} {Achieves} {SOTA} on {SimpleQA} {Benchmark}},
	url = {https://blog.tavily.com/tavily-evaluation-part-1-tavily-achieves-sota-on-simpleqa-benchmark/?utm_source=chatgpt.com},
	author = {Hadar, Gal},
	month = jun,
	year = {2025},
}

@article{hevner_design_2004,
	title = {Design {Science} in {Information} {Systems} {Research}},
	volume = {28},
	journal = {Management Information Systems Quarterly},
	author = {Hevner, Alan and R, Alan and March, Salvatore and T, Salvatore and Park and Park, Jinsoo and Ram and Sudha},
	month = mar,
	year = {2004},
	pages = {75--},
}

@article{senior_software_developer_anna_university_chennai_india_mapreduce_2018,
	title = {{MapReduce}: {Simplified} {Data} {Processing} on {Large} {Cluster}},
	volume = {5},
	issn = {23487852, 23487860},
	shorttitle = {{MapReduce}},
	url = {http://digital.ijre.org/index.php/int_j_res_eng/article/view/339},
	doi = {10.21276/ijre.2018.5.5.4},
	number = {5},
	urldate = {2025-11-23},
	journal = {International Journal of Research and Engineering},
	author = {{Senior Software Developer, ANNA University, Chennai, India} and Dayalan, Muthu},
	month = apr,
	year = {2018},
	pages = {399--403},
}

@article{little_proof_1961,
	title = {A {Proof} for the {Queuing} {Formula}: \textit{{L}} = λ \textit{{W}}},
	volume = {9},
	issn = {0030-364X, 1526-5463},
	shorttitle = {A {Proof} for the {Queuing} {Formula}},
	url = {https://pubsonline.informs.org/doi/10.1287/opre.9.3.383},
	doi = {10.1287/opre.9.3.383},
	abstract = {In a queuing process, let 1/λ be the mean time between the arrivals of two consecutive units, L be the mean number of units in the system, and W be the mean time spent by a unit in the system. It is shown that, if the three means are finite and the corresponding stochastic processes strictly stationary, and, if the arrival process is metrically transitive with nonzero mean, then L = λW.},
	language = {en},
	number = {3},
	urldate = {2025-11-23},
	journal = {Operations Research},
	author = {Little, John D. C.},
	month = jun,
	year = {1961},
	pages = {383--387},
}

@misc{zhou_llmtimesmapreduce_2024,
	title = {{LLM}\${\textbackslash}times\${MapReduce}: {Simplified} {Long}-{Sequence} {Processing} using {Large} {Language} {Models}},
	shorttitle = {{LLM}\${\textbackslash}times\${MapReduce}},
	url = {http://arxiv.org/abs/2410.09342},
	doi = {10.48550/arXiv.2410.09342},
	abstract = {Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLM\${\textbackslash}times\$MapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLM\${\textbackslash}times\$MapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Zhou, Zihan and Li, Chong and Chen, Xinyi and Wang, Shuo and Chao, Yu and Li, Zhili and Wang, Haoyu and An, Rongqiao and Shi, Qi and Tan, Zhixing and Han, Xu and Shi, Xiaodong and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09342 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{dou_towards_2023,
	title = {Towards {Understanding} the {Capability} of {Large} {Language} {Models} on {Code} {Clone} {Detection}: {A} {Survey}},
	shorttitle = {Towards {Understanding} the {Capability} of {Large} {Language} {Models} on {Code} {Clone} {Detection}},
	url = {http://arxiv.org/abs/2308.01191},
	doi = {10.48550/arXiv.2308.01191},
	abstract = {Code cloning, the duplication of code fragments, is common in software development. While some reuse aids productivity, excessive cloning hurts maintainability and introduces bugs. Hence, automatic code clone detection is vital. Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges. However, LLMs' performance in code clone detection is unclear and needs more study for accurate assessment. In this paper, we provide the first comprehensive evaluation of LLMs for clone detection, covering different clone types, languages, and prompts. We find advanced LLMs excel in detecting complex semantic clones, surpassing existing methods. Adding intermediate reasoning steps via chain-of-thought prompts noticeably enhances performance. Additionally, representing code as vector embeddings, especially with text encoders, effectively aids clone detection.Lastly, the ability of LLMs to detect code clones differs among various programming languages. Our study suggests that LLMs have potential for clone detection due to their language capabilities, offering insights for developing robust LLM-based methods to enhance software engineering.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Dou, Shihan and Shan, Junjie and Jia, Haoxiang and Deng, Wenhao and Xi, Zhiheng and He, Wei and Wu, Yueming and Gui, Tao and Liu, Yang and Huang, Xuanjing},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01191 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{agrawal_fast_1994,
	address = {San Francisco, CA, USA},
	series = {{VLDB} '94},
	title = {Fast {Algorithms} for {Mining} {Association} {Rules} in {Large} {Databases}},
	isbn = {1-55860-153-8},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Very} {Large} {Data} {Bases}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
	year = {1994},
	pages = {487--499},
}

@article{han_mining_2000,
	title = {Mining frequent patterns without candidate generation},
	volume = {29},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/335191.335372},
	doi = {10.1145/335191.335372},
	abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.
            
              In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining
              the complete set of frequent patterns
              by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
	language = {en},
	number = {2},
	urldate = {2025-11-23},
	journal = {ACM SIGMOD Record},
	author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
	month = jun,
	year = {2000},
	pages = {1--12},
}

@misc{xu_mantra_2025,
	title = {{MANTRA}: {Enhancing} {Automated} {Method}-{Level} {Refactoring} with {Contextual} {RAG} and {Multi}-{Agent} {LLM} {Collaboration}},
	shorttitle = {{MANTRA}},
	url = {http://arxiv.org/abs/2503.14340},
	doi = {10.48550/arXiv.2503.14340},
	abstract = {Maintaining and scaling software systems relies heavily on effective code refactoring, yet this process remains labor-intensive, requiring developers to carefully analyze existing codebases and prevent the introduction of new defects. Although recent advancements have leveraged Large Language Models (LLMs) to automate refactoring tasks, current solutions are constrained in scope and lack mechanisms to guarantee code compilability and successful test execution. In this work, we introduce MANTRA, a comprehensive LLM agent-based framework that automates method-level refactoring. MANTRA integrates Context-Aware Retrieval-Augmented Generation, coordinated Multi-Agent Collaboration, and Verbal Reinforcement Learning to emulate human decision-making during refactoring while preserving code correctness and readability. Our empirical study, conducted on 703 instances of "pure refactorings" (i.e., code changes exclusively involving structural improvements), drawn from 10 representative Java projects, covers the six most prevalent refactoring operations. Experimental results demonstrate that MANTRA substantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8\% success rate (582/703) in producing code that compiles and passes all tests, compared to just 8.7\% (61/703) with RawGPT. Moreover, in comparison to IntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50\% improvement in generating Extract Method transformations. A usability study involving 37 professional developers further shows that refactorings performed by MANTRA are perceived to be as readable and reusable as human-written code, and in certain cases, even more favorable. These results highlight the practical advantages of MANTRA and emphasize the growing potential of LLM-based systems in advancing the automation of software refactoring tasks.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Xu, Yisen and Lin, Feng and Yang, Jinqiu and Tse-Hsun and Chen and Tsantalis, Nikolaos},
	month = mar,
	year = {2025},
	note = {arXiv:2503.14340 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{wang_codet5_2021,
	title = {{CodeT5}: {Identifier}-aware {Unified} {Pre}-trained {Encoder}-{Decoder} {Models} for {Code} {Understanding} and {Generation}},
	shorttitle = {{CodeT5}},
	url = {http://arxiv.org/abs/2109.00859},
	doi = {10.48550/arXiv.2109.00859},
	abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven C. H.},
	month = sep,
	year = {2021},
	note = {arXiv:2109.00859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
}

@inproceedings{svajlenko_evaluating_2015,
	address = {Bremen, Germany},
	title = {Evaluating clone detection tools with {BigCloneBench}},
	isbn = {978-1-4673-7532-0},
	url = {http://ieeexplore.ieee.org/document/7332459/},
	doi = {10.1109/ICSM.2015.7332459},
	urldate = {2025-11-23},
	booktitle = {2015 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Svajlenko, Jeffrey and Roy, Chanchal K.},
	month = sep,
	year = {2015},
	pages = {131--140},
}

@misc{poesia_synchromesh_2022,
	title = {Synchromesh: {Reliable} code generation from pre-trained language models},
	shorttitle = {Synchromesh},
	url = {http://arxiv.org/abs/2201.11227},
	doi = {10.48550/arXiv.2201.11227},
	abstract = {Large pre-trained language models have been used to generate code,providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
	month = jan,
	year = {2022},
	note = {arXiv:2201.11227 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{noauthor_ast_nodate,
	title = {ast — {Abstract} {Syntax} {Trees}},
	url = {https://docs.python.org/3.13/library/ast.html},
	urldate = {2025-11-23},
}

@misc{zhou_docprompting_2023,
	title = {{DocPrompting}: {Generating} {Code} by {Retrieving} the {Docs}},
	shorttitle = {{DocPrompting}},
	url = {http://arxiv.org/abs/2207.05987},
	doi = {10.48550/arXiv.2207.05987},
	abstract = {Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85\% in pass@1 (52\% relative gain) and 4.39\% in pass@10 (30\% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9\% exact match.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Zhou, Shuyan and Alon, Uri and Xu, Frank F. and Wang, Zhiruo and Jiang, Zhengbao and Neubig, Graham},
	month = feb,
	year = {2023},
	note = {arXiv:2207.05987 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@misc{austin_program_2021,
	title = {Program {Synthesis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2108.07732},
	doi = {10.48550/arXiv.2108.07732},
	abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{olausson_is_2024,
	title = {Is {Self}-{Repair} a {Silver} {Bullet} for {Code} {Generation}?},
	url = {http://arxiv.org/abs/2306.09896},
	doi = {10.48550/arXiv.2306.09896},
	abstract = {Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Olausson, Theo X. and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando},
	month = feb,
	year = {2024},
	note = {arXiv:2306.09896 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{huang_large_2024,
	title = {Large {Language} {Models} {Cannot} {Self}-{Correct} {Reasoning} {Yet}},
	url = {http://arxiv.org/abs/2310.01798},
	doi = {10.48550/arXiv.2310.01798},
	abstract = {Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	month = mar,
	year = {2024},
	note = {arXiv:2310.01798 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{khattab_demonstrate-search-predict_2023,
	title = {Demonstrate-{Search}-{Predict}: {Composing} retrieval and language models for knowledge-intensive {NLP}},
	shorttitle = {Demonstrate-{Search}-{Predict}},
	url = {http://arxiv.org/abs/2212.14024},
	doi = {10.48550/arXiv.2212.14024},
	abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120\%, 8-39\%, and 80-290\% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	month = jan,
	year = {2023},
	note = {arXiv:2212.14024 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{soni_coding_2025,
	title = {Coding {Agents} with {Multimodal} {Browsing} are {Generalist} {Problem} {Solvers}},
	url = {http://arxiv.org/abs/2506.03011},
	doi = {10.48550/arXiv.2506.03011},
	abstract = {Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Soni, Aditya Bharat and Li, Boxuan and Wang, Xingyao and Chen, Valerie and Neubig, Graham},
	month = jun,
	year = {2025},
	note = {arXiv:2506.03011 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	doi = {10.48550/arXiv.2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lazaridou_internet-augmented_2022,
	title = {Internet-augmented language models through few-shot prompting for open-domain question answering},
	url = {http://arxiv.org/abs/2203.05115},
	doi = {10.48550/arXiv.2203.05115},
	abstract = {In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
	month = may,
	year = {2022},
	note = {arXiv:2203.05115 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {arXiv:2112.09332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{cheng_dated_2024,
	title = {Dated {Data}: {Tracing} {Knowledge} {Cutoffs} in {Large} {Language} {Models}},
	shorttitle = {Dated {Data}},
	url = {http://arxiv.org/abs/2403.12958},
	doi = {10.48550/arXiv.2403.12958},
	abstract = {Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Cheng, Jeffrey and Marone, Marc and Weller, Orion and Lawrie, Dawn and Khashabi, Daniel and Durme, Benjamin Van},
	month = sep,
	year = {2024},
	note = {arXiv:2403.12958 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{strotgen_baseline_2015,
	address = {Lisbon, Portugal},
	title = {A {Baseline} {Temporal} {Tagger} for all {Languages}},
	url = {http://aclweb.org/anthology/D15-1063},
	doi = {10.18653/v1/D15-1063},
	language = {en},
	urldate = {2025-11-23},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Strötgen, Jannik and Gertz, Michael},
	year = {2015},
	pages = {541--547},
}

@inproceedings{chang_sutime_2012,
	title = {Sutime: {A} library for recognizing and normalizing time expressions.},
	volume = {12},
	booktitle = {Lrec},
	author = {Chang, Angel X and Manning, Christopher D},
	year = {2012},
	pages = {3735--3740},
}

@book{brazil_prometheus_2018,
	edition = {1st edition},
	title = {Prometheus: {Up} \& {Running}},
	shorttitle = {Prometheus},
	abstract = {Get up to speed with Prometheus, the metrics-based monitoring system used by tens of thousands of organizations in production. This practical guide provides application developers, sysadmins, and DevOps practitioners with a hands-on introduction to the most important aspects of Prometheus, including dashboarding and alerting, direct code instrumentation, and metric collection from third-party systems with exporters. This open source system has gained popularity over the past few years for good reason. With its simple yet powerful data model and query language, Prometheus does one thing, and it does it well. Author and Prometheus developer Brian Brazil guides you through Prometheus setup, the Node exporter, and the Alertmanager, then demonstrates how to use them for application and infrastructure monitoring. Know where and how much to apply instrumentation to your application code Identify metrics with labels using unique key-value pairs Get an introduction to Grafana, a popular tool for building dashboards Learn how to use the Node Exporter to monitor your infrastructure Use service discovery to provide different views of your machines and services Use Prometheus with Kubernetes and examine exporters you can use with containers Convert data from other monitoring systems into the Prometheus format},
	language = {eng},
	publisher = {O'Reilly Media, Incorporated},
	author = {Brazil, Brian},
	year = {2018},
	note = {OCLC: 1103598677},
}

@misc{shin_constrained_2021-1,
	title = {Constrained {Language} {Models} {Yield} {Few}-{Shot} {Semantic} {Parsers}},
	url = {http://arxiv.org/abs/2104.08768},
	doi = {10.48550/arXiv.2104.08768},
	abstract = {We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Shin, Richard and Lin, Christopher H. and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Durme, Benjamin Van},
	month = nov,
	year = {2021},
	note = {arXiv:2104.08768 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{strotgen_multilingual_2013,
	title = {Multilingual and cross-domain temporal tagging},
	volume = {47},
	copyright = {http://www.springer.com/tdm},
	issn = {1574-020X, 1574-0218},
	url = {http://link.springer.com/10.1007/s10579-012-9179-y},
	doi = {10.1007/s10579-012-9179-y},
	language = {en},
	number = {2},
	urldate = {2025-11-23},
	journal = {Language Resources and Evaluation},
	author = {Strötgen, Jannik and Gertz, Michael},
	month = jun,
	year = {2013},
	pages = {269--298},
}

@article{allen_maintaining_1983,
	title = {Maintaining knowledge about temporal intervals},
	volume = {26},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/182.358434},
	doi = {10.1145/182.358434},
	language = {en},
	number = {11},
	urldate = {2025-11-23},
	journal = {Communications of the ACM},
	author = {Allen, James F.},
	month = nov,
	year = {1983},
	pages = {832--843},
}

@misc{orr_bootleg_2020,
	title = {Bootleg: {Chasing} the {Tail} with {Self}-{Supervised} {Named} {Entity} {Disambiguation}},
	shorttitle = {Bootleg},
	url = {http://arxiv.org/abs/2010.10363},
	doi = {10.48550/arXiv.2010.10363},
	abstract = {A challenge for named entity disambiguation (NED), the task of mapping textual mentions to entities in a knowledge base, is how to disambiguate entities that appear rarely in the training data, termed tail entities. Humans use subtle reasoning patterns based on knowledge of entity facts, relations, and types to disambiguate unfamiliar entities. Inspired by these patterns, we introduce Bootleg, a self-supervised NED system that is explicitly grounded in reasoning patterns for disambiguation. We define core reasoning patterns for disambiguation, create a learning procedure to encourage the self-supervised model to learn the patterns, and show how to use weak supervision to enhance the signals in the training data. Encoding the reasoning patterns in a simple Transformer architecture, Bootleg meets or exceeds state-of-the-art on three NED benchmarks. We further show that the learned representations from Bootleg successfully transfer to other non-disambiguation tasks that require entity-based knowledge: we set a new state-of-the-art in the popular TACRED relation extraction task by 1.0 F1 points and demonstrate up to 8\% performance lift in highly optimized production search and assistant tasks at a major technology company},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Orr, Laurel and Leszczynski, Megan and Arora, Simran and Wu, Sen and Guha, Neel and Ling, Xiao and Re, Christopher},
	month = oct,
	year = {2020},
	note = {arXiv:2010.10363 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{khot_decomposed_2023,
	title = {Decomposed {Prompting}: {A} {Modular} {Approach} for {Solving} {Complex} {Tasks}},
	shorttitle = {Decomposed {Prompting}},
	url = {http://arxiv.org/abs/2210.02406},
	doi = {10.48550/arXiv.2210.02406},
	abstract = {Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
	month = apr,
	year = {2023},
	note = {arXiv:2210.02406 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{cao_autoregressive_2021,
	title = {Autoregressive {Entity} {Retrieval}},
	url = {http://arxiv.org/abs/2010.00904},
	doi = {10.48550/arXiv.2010.00904},
	abstract = {Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. Current approaches can be understood as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach has several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion. This mitigates the aforementioned technical issues since: (i) the autoregressive formulation directly captures relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the softmax loss is computed without subsampling negative data. We experiment with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their names. Code and pre-trained models at https://github.com/facebookresearch/GENRE.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Cao, Nicola De and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
	month = mar,
	year = {2021},
	note = {arXiv:2010.00904 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_chain_2024,
	title = {Chain of {Code}: {Reasoning} with a {Language} {Model}-{Augmented} {Code} {Emulator}},
	shorttitle = {Chain of {Code}},
	url = {http://arxiv.org/abs/2312.04474},
	doi = {10.48550/arXiv.2312.04474},
	abstract = {Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter - we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect\_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect\_sarcasm(string)". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84\%, a gain of 12\% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code".},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian},
	month = jul,
	year = {2024},
	note = {arXiv:2312.04474 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liang_code_2023,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {http://arxiv.org/abs/2209.07753},
	doi = {10.48550/arXiv.2209.07753},
	abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = may,
	year = {2023},
	note = {arXiv:2209.07753 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_program_2023,
	title = {Program of {Thoughts} {Prompting}: {Disentangling} {Computation} from {Reasoning} for {Numerical} {Reasoning} {Tasks}},
	shorttitle = {Program of {Thoughts} {Prompting}},
	url = {http://arxiv.org/abs/2211.12588},
	doi = {10.48550/arXiv.2211.12588},
	abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12{\textbackslash}\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
	month = oct,
	year = {2023},
	note = {arXiv:2211.12588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{enevoldsen_mmteb_2025,
	title = {{MMTEB}: {Massive} {Multilingual} {Text} {Embedding} {Benchmark}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{MMTEB}},
	url = {https://arxiv.org/abs/2502.13595},
	doi = {10.48550/ARXIV.2502.13595},
	abstract = {Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Enevoldsen, Kenneth and Chung, Isaac and Kerboua, Imene and Kardos, Márton and Mathur, Ashwin and Stap, David and Gala, Jay and Siblini, Wissam and Krzemiński, Dominik and Winata, Genta Indra and Sturua, Saba and Utpala, Saiteja and Ciancone, Mathieu and Schaeffer, Marion and Sequeira, Gabriel and Misra, Diganta and Dhakal, Shreeya and Rystrøm, Jonathan and Solomatin, Roman and Çağatan, Ömer and Kundu, Akash and Bernstorff, Martin and Xiao, Shitao and Sukhlecha, Akshita and Pahwa, Bhavish and Poświata, Rafał and GV, Kranthi Kiran and Ashraf, Shawon and Auras, Daniel and Plüster, Björn and Harries, Jan Philipp and Magne, Loïc and Mohr, Isabelle and Hendriksen, Mariya and Zhu, Dawei and Gisserot-Boukhlef, Hippolyte and Aarsen, Tom and Kostkan, Jan and Wojtasik, Konrad and Lee, Taemin and Šuppa, Marek and Zhang, Crystina and Rocca, Roberta and Hamdy, Mohammed and Michail, Andrianos and Yang, John and Faysse, Manuel and Vatolin, Aleksei and Thakur, Nandan and Dey, Manan and Vasani, Dipam and Chitale, Pranjal and Tedeschi, Simone and Tai, Nguyen and Snegirev, Artem and Günther, Michael and Xia, Mengzhou and Shi, Weijia and Lù, Xing Han and Clive, Jordan and Krishnakumar, Gayatri and Maksimova, Anna and Wehrli, Silvan and Tikhonova, Maria and Panchal, Henil and Abramov, Aleksandr and Ostendorff, Malte and Liu, Zheng and Clematide, Simon and Miranda, Lester James and Fenogenova, Alena and Song, Guangyu and Safi, Ruqiya Bin and Li, Wen-Ding and Borghini, Alessia and Cassano, Federico and Su, Hongjin and Lin, Jimmy and Yen, Howard and Hansen, Lasse and Hooker, Sara and Xiao, Chenghao and Adlakha, Vaibhav and Weller, Orion and Reddy, Siva and Muennighoff, Niklas},
	year = {2025},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{enevoldsen_mmteb_2025-1,
	title = {{MMTEB}: {Massive} {Multilingual} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MMTEB}},
	url = {http://arxiv.org/abs/2502.13595},
	doi = {10.48550/arXiv.2502.13595},
	abstract = {Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Enevoldsen, Kenneth and Chung, Isaac and Kerboua, Imene and Kardos, Márton and Mathur, Ashwin and Stap, David and Gala, Jay and Siblini, Wissam and Krzemiński, Dominik and Winata, Genta Indra and Sturua, Saba and Utpala, Saiteja and Ciancone, Mathieu and Schaeffer, Marion and Sequeira, Gabriel and Misra, Diganta and Dhakal, Shreeya and Rystrøm, Jonathan and Solomatin, Roman and Çağatan, Ömer and Kundu, Akash and Bernstorff, Martin and Xiao, Shitao and Sukhlecha, Akshita and Pahwa, Bhavish and Poświata, Rafał and GV, Kranthi Kiran and Ashraf, Shawon and Auras, Daniel and Plüster, Björn and Harries, Jan Philipp and Magne, Loïc and Mohr, Isabelle and Hendriksen, Mariya and Zhu, Dawei and Gisserot-Boukhlef, Hippolyte and Aarsen, Tom and Kostkan, Jan and Wojtasik, Konrad and Lee, Taemin and Šuppa, Marek and Zhang, Crystina and Rocca, Roberta and Hamdy, Mohammed and Michail, Andrianos and Yang, John and Faysse, Manuel and Vatolin, Aleksei and Thakur, Nandan and Dey, Manan and Vasani, Dipam and Chitale, Pranjal and Tedeschi, Simone and Tai, Nguyen and Snegirev, Artem and Günther, Michael and Xia, Mengzhou and Shi, Weijia and Lù, Xing Han and Clive, Jordan and Krishnakumar, Gayatri and Maksimova, Anna and Wehrli, Silvan and Tikhonova, Maria and Panchal, Henil and Abramov, Aleksandr and Ostendorff, Malte and Liu, Zheng and Clematide, Simon and Miranda, Lester James and Fenogenova, Alena and Song, Guangyu and Safi, Ruqiya Bin and Li, Wen-Ding and Borghini, Alessia and Cassano, Federico and Su, Hongjin and Lin, Jimmy and Yen, Howard and Hansen, Lasse and Hooker, Sara and Xiao, Chenghao and Adlakha, Vaibhav and Weller, Orion and Reddy, Siva and Muennighoff, Niklas},
	month = nov,
	year = {2025},
	note = {arXiv:2502.13595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@inproceedings{xu_layoutlm_2020,
	title = {{LayoutLM}: {Pre}-training of {Text} and {Layout} for {Document} {Image} {Understanding}},
	shorttitle = {{LayoutLM}},
	url = {http://arxiv.org/abs/1912.13318},
	doi = {10.1145/3394486.3403172},
	abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
	urldate = {2025-11-22},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	month = aug,
	year = {2020},
	note = {arXiv:1912.13318 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1192--1200},
}

@misc{auer_docling_2024,
	title = {Docling {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.09869},
	doi = {10.48550/arXiv.2408.09869},
	abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and Lima, Rafael Teixeira de and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
	month = dec,
	year = {2024},
	note = {arXiv:2408.09869 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Software Engineering},
}

@misc{geng_jsonschemabench_2025,
	title = {{JSONSchemaBench}: {A} {Rigorous} {Benchmark} of {Structured} {Outputs} for {Language} {Models}},
	shorttitle = {{JSONSchemaBench}},
	url = {http://arxiv.org/abs/2501.10868},
	doi = {10.48550/arXiv.2501.10868},
	abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Geng, Saibo and Cooper, Hudson and Moskal, Michał and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
	month = feb,
	year = {2025},
	note = {arXiv:2501.10868 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{willard_efficient_2023,
	title = {Efficient {Guided} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.09702},
	doi = {10.48550/arXiv.2307.09702},
	abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Willard, Brandon T. and Louf, Rémi},
	month = aug,
	year = {2023},
	note = {arXiv:2307.09702 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_retrieval_2024,
	title = {Retrieval meets {Long} {Context} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.03025},
	doi = {10.48550/arXiv.2310.03025},
	abstract = {Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
	month = jan,
	year = {2024},
	note = {arXiv:2310.03025 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{heineman_signal_2025,
	title = {Signal and {Noise}: {A} {Framework} for {Reducing} {Uncertainty} in {Language} {Model} {Evaluation}},
	shorttitle = {Signal and {Noise}},
	url = {http://arxiv.org/abs/2508.13144},
	doi = {10.48550/arXiv.2508.13144},
	abstract = {Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Heineman, David and Hofmann, Valentin and Magnusson, Ian and Gu, Yuling and Smith, Noah A. and Hajishirzi, Hannaneh and Lo, Kyle and Dodge, Jesse},
	month = aug,
	year = {2025},
	note = {arXiv:2508.13144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_core_2025,
	title = {Core {Context} {Aware} {Transformers} for {Long} {Context} {Language} {Modeling}},
	url = {http://arxiv.org/abs/2412.12465},
	doi = {10.48550/arXiv.2412.12465},
	abstract = {Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Chen, Yaofo and You, Zeng and Zhang, Shuhai and Li, Haokun and Li, Yirui and Wang, Yaowei and Tan, Mingkui},
	month = aug,
	year = {2025},
	note = {arXiv:2412.12465 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{maghakian_embedding-free_2025,
	address = {Suzhou, China},
	title = {Embedding-{Free} {RAG}},
	url = {https://aclanthology.org/2025.findings-emnlp.1360},
	doi = {10.18653/v1/2025.findings-emnlp.1360},
	language = {en},
	urldate = {2025-11-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Maghakian, Jessica and Sinha, Raunak and Schettewi, Max and Kaur, Gunkirat},
	year = {2025},
	pages = {24974--24985},
}

@misc{weller_theoretical_2025,
	title = {On the {Theoretical} {Limitations} of {Embedding}-{Based} {Retrieval}},
	url = {http://arxiv.org/abs/2508.21038},
	doi = {10.48550/arXiv.2508.21038},
	abstract = {Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Weller, Orion and Boratko, Michael and Naim, Iftekhar and Lee, Jinhyuk},
	month = aug,
	year = {2025},
	note = {arXiv:2508.21038 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{li_long_2024,
	title = {Long {Context} vs. {RAG} for {LLMs}: {An} {Evaluation} and {Revisits}},
	shorttitle = {Long {Context} vs. {RAG} for {LLMs}},
	url = {http://arxiv.org/abs/2501.01880},
	doi = {10.48550/arXiv.2501.01880},
	abstract = {Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Li, Xinze and Cao, Yixin and Ma, Yubo and Sun, Aixin},
	month = dec,
	year = {2024},
	note = {arXiv:2501.01880 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kuratov_search_2024,
	title = {In {Search} of {Needles} in a {11M} {Haystack}: {Recurrent} {Memory} {Finds} {What} {LLMs} {Miss}},
	shorttitle = {In {Search} of {Needles} in a {11M} {Haystack}},
	url = {http://arxiv.org/abs/2402.10790},
	doi = {10.48550/arXiv.2402.10790},
	abstract = {This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to \$10{\textasciicircum}4\$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to \$11{\textbackslash}times 10{\textasciicircum}6\$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10790 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{comanici_gemini_2025,
	title = {Gemini 2.5: {Pushing} the {Frontier} with {Advanced} {Reasoning}, {Multimodality}, {Long} {Context}, and {Next} {Generation} {Agentic} {Capabilities}},
	shorttitle = {Gemini 2.5},
	url = {http://arxiv.org/abs/2507.06261},
	doi = {10.48550/arXiv.2507.06261},
	abstract = {In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and Marris, Luke and Petulla, Sam and Gaffney, Colin and Aharoni, Asaf and Lintz, Nathan and Pais, Tiago Cardal and Jacobsson, Henrik and Szpektor, Idan and Jiang, Nan-Jiang and Haridasan, Krishna and Omran, Ahmed and Saunshi, Nikunj and Bahri, Dara and Mishra, Gaurav and Chu, Eric and Boyd, Toby and Hekman, Brad and Parisi, Aaron and Zhang, Chaoyi and Kawintiranon, Kornraphop and Bedrax-Weiss, Tania and Wang, Oliver and Xu, Ya and Purkiss, Ollie and Mendlovic, Uri and Deutel, Ilaï and Nguyen, Nam and Langley, Adam and Korn, Flip and Rossazza, Lucia and Ramé, Alexandre and Waghmare, Sagar and Miller, Helen and Byrd, Nathan and Sheshan, Ashrith and Hadsell, Raia and Bhardwaj, Sangnie and Janus, Pawel and Rissa, Tero and Horgan, Dan and Abdagic, Alvin and Belenki, Lior and Allingham, James and Singh, Anima and Guidroz, Theo and Srinivasan, Srivatsan and Schmit, Herman and Chiafullo, Kristen and Elisseeff, Andre and Jha, Nilpa and Kolhar, Prateek and Berrada, Leonard and Ding, Frank and Si, Xiance and Mallick, Shrestha Basu and Och, Franz and Erell, Sofia and Ni, Eric and Latkar, Tejasi and Yang, Sherry and Sirkovic, Petar and Feng, Ziqiang and Leland, Robert and Hornung, Rachel and Wu, Gang and Blundell, Charles and Alvari, Hamidreza and Huang, Po-Sen and Yip, Cathy and Deur, Sanja and Liu, Li and Surita, Gabriela and Duque, Pablo and Damen, Dima and Jia, Johnson and Guez, Arthur and Mircea, Markus and Sinha, Animesh and Magni, Alberto and Stradomski, Paweł and Marian, Tal and Galić, Vlado and Chen, Wenhu and Husain, Hisham and Singhal, Achintya and Grewe, Dominik and Aubet, François-Xavier and Song, Shuang and Blanco, Lorenzo and Rechis, Leland and Ho, Lewis and Munoz, Rich and Zheng, Kelvin and Hamrick, Jessica and Mather, Kevin and Taitelbaum, Hagai and Rutherford, Eliza and Lei, Yun and Chen, Kuangyuan and Shukla, Anand and Moreira, Erica and Doi, Eric and Isik, Berivan and Shabat, Nir and Rogozińska, Dominika and Kolipaka, Kashyap and Chang, Jason and Vušak, Eugen and Venkatachary, Srinivasan and Noghabi, Shadi and Bharti, Tarun and Jun, Younghoon and Zaks, Aleksandr and Green, Simon and Challagundla, Jeshwanth and Wong, William and Mohammad, Muqthar and Hirsch, Dean and Cheng, Yong and Naim, Iftekhar and Proleev, Lev and Vincent, Damien and Singh, Aayush and Krikun, Maxim and Krishnan, Dilip and Ghahramani, Zoubin and Atias, Aviel and Aggarwal, Rajeev and Kirov, Christo and Vytiniotis, Dimitrios and Koh, Christy and Chronopoulou, Alexandra and Dogra, Pawan and Ion, Vlad-Doru and Tyen, Gladys and Lee, Jason and Weissenberger, Felix and Strohman, Trevor and Balakrishna, Ashwin and Rae, Jack and Velic, Marko and Liedekerke, Raoul de and Elyada, Oded and Yuan, Wentao and Liu, Canoee and Shani, Lior and Kishchenko, Sergey and Alessio, Bea and Li, Yandong and Song, Richard and Kwei, Sam and Jankowski, Orion and Pappu, Aneesh and Namiki, Youhei and Ma, Yenai and Tripuraneni, Nilesh and Cherry, Colin and Ikonomidis, Marissa and Ling, Yu-Cheng and Ji, Colin and Westberg, Beka and Wright, Auriel and Yu, Da and Parkinson, David and Ramaswamy, Swaroop and Connor, Jerome and Yeganeh, Soheil Hassas and Grover, Snchit and Kenwright, George and Litchev, Lubo and Apps, Chris and Tomala, Alex and Halim, Felix and Castro-Ros, Alex and Li, Zefei and Boral, Anudhyan and Sho, Pauline and Yarom, Michal and Malmi, Eric and Klinghoffer, David and Lin, Rebecca and Ansell, Alan and S, Pradeep Kumar and Zhao, Shubin and Zuo, Siqi and Santoro, Adam and Cheng, Heng-Tze and Demmessie, Solomon and Liu, Yuchi and Brichtova, Nicole and Culp, Allie and Braun, Nathaniel and Graur, Dan and Ng, Will and Mehta, Nikhil and Phillips, Aaron and Sundberg, Patrik and Godbole, Varun and Liu, Fangyu and Katariya, Yash and Rim, David and Seyedhosseini, Mojtaba and Ammirati, Sean and Valfridsson, Jonas and Malihi, Mahan and Knight, Timothy and Toor, Andeep and Lampe, Thomas and Ittycheriah, Abe and Chiang, Lewis and Yeung, Chak and Fréchette, Alexandre and Rao, Jinmeng and Wang, Huisheng and Srivastava, Himanshu and Zhang, Richard and Rhodes, Rocky and Brand, Ariel and Weesner, Dean and Figotin, Ilya and Gimeno, Felix and Fellinger, Rachana and Marcenac, Pierre and Leal, José and Marcus, Eyal and Cotruta, Victor and Cabrera, Rodrigo and Luo, Sheryl and Garrette, Dan and Axelrod, Vera and Baltateanu, Sorin and Barker, David and Chen, Dongkai and Toma, Horia and Ingram, Ben and Riesa, Jason and Kulkarni, Chinmay and Zhang, Yujing and Liu, Hongbin and Wang, Chao and Polacek, Martin and Wu, Will and Hui, Kai and Reyes, Adrian N. and Su, Yi and Barnes, Megan and Malhi, Ishaan and Siddiqui, Anfal and Feng, Qixuan and Damaschin, Mihai and Pighin, Daniele and Steiner, Andreas and Yang, Samuel and Boppana, Ramya Sree and Ivanov, Simeon and Kandoor, Arun and Shah, Aditya and Mujika, Asier and Huang, Da and Choquette-Choo, Christopher A. and Patel, Mohak and Yu, Tianhe and Creswell, Toni and Jerry and Liu and Barros, Catarina and Razeghi, Yasaman and Roy, Aurko and Culliton, Phil and Xiong, Binbin and Pan, Jiaqi and Strohmann, Thomas and Powell, Tolly and Seal, Babi and DeCarlo, Doug and Shyam, Pranav and Katircioglu, Kaan and Wang, Xuezhi and Hardin, Cassidy and Odisho, Immanuel and Broder, Josef and Chang, Oscar and Nair, Arun and Shtefan, Artem and O'Brien, Maura and Agarwal, Manu and Potluri, Sahitya and Goyal, Siddharth and Jhindal, Amit and Thakur, Saksham and Stuken, Yury and Lyon, James and Toutanova, Kristina and Feng, Fangxiaoyu and Wu, Austin and Horn, Ben and Wang, Alek and Cullum, Alex and Taubman, Gabe and Shrivastava, Disha and Shi, Chongyang and Tomlinson, Hamish and Patel, Roma and Tu, Tao and Oflazer, Ada Maksutaj and Pongetti, Francesco and Yang, Mingyao and Taïga, Adrien Ali and Perot, Vincent and Pierse, Nuo Wang and Han, Feng and Drori, Yoel and Iturrate, Iñaki and Chakrabarti, Ayan and Yeung, Legg and Dopson, Dave and Chen, Yi-ting and Kulshreshtha, Apoorv and Guo, Tongfei and Pham, Philip and Schuster, Tal and Chen, Junquan and Polozov, Alex and Xing, Jinwei and Zhou, Huanjie and Kacham, Praneeth and Kukliansky, Doron and Miech, Antoine and Yaroshenko, Sergey and Chi, Ed and Douglas, Sholto and Fei, Hongliang and Blondel, Mathieu and Myla, Preethi and Madmoni, Lior and Wu, Xing and Keysers, Daniel and Kjems, Kristian and Albuquerque, Isabela and Yu, Lijun and D'sa, Joel and Plantan, Michelle and Ionescu, Vlad and Elias, Jaume Sanchez and Gupta, Abhirut and Vuyyuru, Manish Reddy and Alcober, Fred and Zhou, Tong and Ji, Kaiyang and Hartmann, Florian and Puttagunta, Subha and Song, Hugo and Amid, Ehsan and Stefanoiu, Anca and Lee, Andrew and Pucciarelli, Paul and Wang, Emma and Raul, Amit and Petrov, Slav and Tian, Isaac and Anklin, Valentin and Nti, Nana and Gomes, Victor and Schumacher, Max and Vesom, Grace and Panagopoulos, Alex and Bousmalis, Konstantinos and Andor, Daniel and Jacob, Josh and Zhang, Yuan and Rosgen, Bill and Kecman, Matija and Tung, Matthew and Belias, Alexandra and Goodman, Noah and Covington, Paul and Wieder, Brian and Saxena, Nikita and Davoodi, Elnaz and Huang, Muhuan and Maddineni, Sharath and Roulet, Vincent and Campbell-Ajala, Folawiyo and Sessa, Pier Giuseppe and Xintian and Wu and Lai, Guangda and Collins, Paul and Haig, Alex and Sakenas, Vytenis and Xu, Xiaowei and Giustina, Marissa and Shafey, Laurent El and Charoenpanit, Pichi and Garg, Shefali and Ainslie, Joshua and Severson, Boone and Arenas, Montse Gonzalez and Pathak, Shreya and Rajayogam, Sujee and Feng, Jie and Bakker, Michiel and Li, Sheng and Wichers, Nevan and Rogers, Jamie and Geng, Xinyang and Li, Yeqing and Jagerman, Rolf and Jia, Chao and Olmert, Nadav and Sharon, David and Mauger, Matthew and Mariserla, Sandeep and Ma, Hongxu and Mohabey, Megha and Kim, Kyuyeun and Andreev, Alek and Pollom, Scott and Love, Juliette and Jain, Vihan and Agrawal, Priyanka and Schroecker, Yannick and Fortin, Alisa and Warmuth, Manfred and Liu, Ji and Leach, Andrew and Blok, Irina and Girirajan, Ganesh Poomal and Aharoni, Roee and Uria, Benigno and Sozanschi, Andrei and Goldberg, Dan and Ionita, Lucian and Ribeiro, Marco Tulio and Zlocha, Martin and Birodkar, Vighnesh and Lachgar, Sami and Yuan, Liangzhe and Choudhury, Himadri and Ginsberg, Matt and Zheng, Fei and Dibb, Gregory and Graves, Emily and Lokhande, Swachhand and Rasskin, Gabriel and Muraru, George-Cristian and Quick, Corbin and Tata, Sandeep and Sermanet, Pierre and Chawla, Aditya and Karo, Itay and Wang, Yan and Zhang, Susan and Keller, Orgad and Dragan, Anca and Su, Guolong and Chou, Ian and Liu, Xi and Tao, Yiqing and Prabhakara, Shruthi and Wilson, Marc and Liu, Ruibo and Wang, Shibo and Evans, Georgie and Du, David and Castaño, Alfonso and Prasad, Gautam and Mahdy, Mona El and Gerlach, Sebastian and Reid, Machel and Kahn, Jarrod and Zait, Amir and Pillai, Thanumalayan Sankaranarayana and Ulrich, Thatcher and Wang, Guanyu and Wassenberg, Jan and Farkash, Efrat and Yalasangi, Kiran and Wang, Congchao and Bauza, Maria and Bucher, Simon and Liu, Ting and Yan, Jun and Leung, Gary and Sindhwani, Vikas and Barnes, Parker and Singh, Avi and Jurin, Ivan and Chang, Jichuan and Bhumihar, Niket Kumar and Eiger, Sivan and Citovsky, Gui and Withbroe, Ben and Li, Zhang and Xue, Siyang and Santo, Niccolò Dal and Stoyanov, Georgi and Raimond, Yves and Zheng, Steven and Gao, Yilin and Listík, Vít and Kwasiborski, Sławek and Saputro, Rachel and Ozturel, Adnan and Mallya, Ganesh and Majmundar, Kushal and West, Ross and Caron, Paul and Wei, Jinliang and Castrejon, Lluis and Vikram, Sharad and Ramachandran, Deepak and Dhawan, Nikhil and Park, Jiho and Smoot, Sara and Driessche, George van den and Blau, Yochai and Malik, Chase and Liang, Wei and Hirsch, Roy and Santos, Cicero Nogueira dos and Weinstein, Eugene and Oord, Aäron van den and Lall, Sid and FitzGerald, Nicholas and Jiang, Zixuan and Yang, Xuan and Webster, Dale and Elqursh, Ali and Pope, Aedan and Rotival, Georges and Raposo, David and Zhu, Wanzheng and Dean, Jeff and Alabed, Sami and Tran, Dustin and Gupta, Arushi and Gleicher, Zach and Austin, Jessica and Rosseel, Edouard and Umekar, Megh and Das, Dipanjan and Sun, Yinghao and Chen, Kai and Misiunas, Karolis and Zhou, Xiang and Di, Yixian and Loo, Alyssa and Newlan, Josh and Li, Bo and Ramasesh, Vinay and Xu, Ying and Chen, Alex and Gandhe, Sudeep and Soricut, Radu and Gupta, Nikita and Hu, Shuguang and El-Sayed, Seliem and Garcia, Xavier and Brusilovsky, Idan and Chen, Pu-Chin and Bolt, Andrew and Huang, Lu and Gurney, Alex and Zhang, Zhiying and Pritzel, Alexander and Wilkiewicz, Jarek and Seybold, Bryan and Shamanna, Bhargav Kanagal and Fischer, Felix and Dean, Josef and Gill, Karan and Mcilroy, Ross and Bhowmick, Abhishek and Selier, Jeremy and Yang, Antoine and Cheng, Derek and Magay, Vladimir and Tan, Jie and Varma, Dhriti and Walder, Christian and Kocisky, Tomas and Nakashima, Ryo and Natsev, Paul and Kwong, Mike and Gog, Ionel and Zhang, Chiyuan and Dieleman, Sander and Jimma, Thomas and Ryabtsev, Andrey and Brahma, Siddhartha and Steiner, David and Du, Dayou and Žužul, Ante and Žanić, Mislav and Raghavachari, Mukund and Gierke, Willi and Zheng, Zeyu and Petrova, Dessie and Dauphin, Yann and Liu, Yuchuan and Kessler, Ido and Hand, Steven and Duvarney, Chris and Kim, Seokhwan and Lee, Hyo and Hussenot, Léonard and Hui, Jeffrey and Smith, Josh and Jain, Deepali and Xia, Jiawei and Tomar, Gaurav Singh and Amiri, Keyvan and Phan, Du and Fuchs, Fabian and Weyand, Tobias and Tomasev, Nenad and Cordell, Alexandra and Liu, Xin and Mallinson, Jonathan and Joshi, Pankaj and Crawford, Andy and Suggala, Arun and Chien, Steve and Fernando, Nick and Sanchez-Vargas, Mariella and Williams, Duncan and Crone, Phil and Luo, Xiyang and Karpov, Igor and Shan, Jyn and Thurk, Terry and Strudel, Robin and Voigtlaender, Paul and Patil, Piyush and Dozat, Tim and Khodaei, Ali and Singla, Sahil and Ambroszczyk, Piotr and Wu, Qiyin and Chang, Yifan and Roark, Brian and Hegde, Chaitra and Ding, Tianli and Filos, Angelos and Wu, Zhongru and Pinto, André Susano and Liu, Shuang and Khanna, Saarthak and Pandey, Aditya and Mcloughlin, Siobhan and Li, Qiujia and Haves, Sam and Zhou, Allan and Buchatskaya, Elena and Leal, Isabel and Boursac, Peter de and Akazawa, Nami and Anderson, Nina and Chen, Terry and Somandepalli, Krishna and Liang, Chen and Goenka, Sheela and Winkler, Stephanie and Grushetsky, Alexander and Ding, Yifan and Smith, Jamie and Ye, Fan and Pont-Tuset, Jordi and Li, Eric and Li, Ruichao and Golany, Tomer and Wegner, Dawid and Jiang, Tao and Barak, Omer and Shangguan, Yuan and Vértes, Eszter and Wong, Renee and Bornschein, Jörg and Tudor, Alex and Bevilacqua, Michele and Schaul, Tom and Rawat, Ankit Singh and Zhao, Yang and Axiotis, Kyriakos and Meng, Lei and McLean, Cory and Lai, Jonathan and Beattie, Jennifer and Kushman, Nate and Liu, Yaxin and Kutzman, Blair and Lang, Fiona and Ye, Jingchen and Netrapalli, Praneeth and Mishra, Pushkar and Khan, Myriam and Goel, Megha and Willoughby, Rob and Tian, David and Zhuang, Honglei and Chen, J. D. and Tsai, Zak and Kementsietsidis, Tasos and Khare, Arjun and Keeling, James and Xu, Keyang and Waters, Nathan and Altché, Florent and Popat, Ashok and Mittal, Bhavishya and Saxton, David and Badawy, Dalia El and Mathieu, Michael and Zheng, Zheng and Zhou, Hao and Ranka, Nishant and Shin, Richard and Duan, Qingnan and Salimans, Tim and Mihailescu, Ioana and Shaham, Uri and Chang, Ming-Wei and Assael, Yannis and Dikkala, Nishanth and Izzard, Martin and Cohen-Addad, Vincent and Graves, Cat and Feinberg, Vlad and Chung, Grace and Strouse, D. J. and Karmon, Danny and Sharifzadeh, Sahand and Ashwood, Zoe and Pham, Khiem and Blanton, Jon and Vasiloff, Alex and Barber, Jarred and Geller, Mark and Zhou, Aurick and Zubach, Fedir and Huang, Tzu-Kuo and Zhang, Lei and Gupta, Himanshu and Young, Matt and Proskurnia, Julia and Votel, Ronny and Gabeur, Valentin and Barcik, Gabriel and Tripathi, Aditya and Yu, Hongkun and Yan, Geng and Changpinyo, Beer and Pavetić, Filip and Coyle, Amy and Fujii, Yasuhisa and Mendez, Jorge Gonzalez and Zhou, Tianhao and Rajamani, Harish and Hechtman, Blake and Cao, Eddie and Juan, Da-Cheng and Tan, Yi-Xuan and Dalibard, Valentin and Du, Yilun and Clay, Natalie and Yao, Kaisheng and Jia, Wenhao and Vijaykumar, Dimple and Zhou, Yuxiang and Bai, Xinyi and Hung, Wei-Chih and Pecht, Steven and Todorov, Georgi and Khadke, Nikhil and Gupta, Pramod and Lahoti, Preethi and Autef, Arnaud and Duddu, Karthik and Lee-Thorp, James and Bykovsky, Alexander and Misiunas, Tautvydas and Flennerhag, Sebastian and Thangaraj, Santhosh and McGiffin, Jed and Nado, Zack and Kunesch, Markus and Noever, Andreas and Hertz, Amir and Liang, Marco and Stone, Victor and Palmer, Evan and Daruki, Samira and Pramanik, Arijit and Põder, Siim and Kyker, Austin and Khan, Mina and Sluzhaev, Evgeny and Ritter, Marvin and Ruderman, Avraham and Zhou, Wenlei and Nagpal, Chirag and Vodrahalli, Kiran and Necula, George and Barham, Paul and Pavlick, Ellie and Hartford, Jay and Shafran, Izhak and Zhao, Long and Mikuła, Maciej and Eccles, Tom and Shimokawa, Hidetoshi and Garg, Kanav and Vilnis, Luke and Chen, Hanwen and Shumailov, Ilia and Lee, Kuang-Huei and Abdelhamed, Abdelrahman and Xie, Meiyan and Cohen, Vered and Hlavnova, Ester and Malkin, Dan and Sitawarin, Chawin and Lottes, James and Coquinot, Pauline and Yu, Tianli and Kumar, Sandeep and Zhang, Jingwei and Mahendru, Aroma and Ahmed, Zafarali and Martens, James and Chen, Tao and Boag, Aviel and Peng, Daiyi and Devin, Coline and Klimovskiy, Arseniy and Phuong, Mary and Vainstein, Danny and Xie, Jin and Ramabhadran, Bhuvana and Howard, Nathan and Yu, Xinxin and Goswami, Gitartha and Cui, Jingyu and Shleifer, Sam and Pinto, Mario and Yeh, Chih-Kuan and Yang, Ming-Hsuan and Javanmardi, Sara and Ethier, Dan and Lee, Chace and Orbay, Jordi and Kotecha, Suyog and Bromberg, Carla and Shaw, Pete and Thornton, James and Rosenthal, Adi Gerzi and Gu, Shane and Thomas, Matt and Gemp, Ian and Ayyar, Aditya and Ushio, Asahi and Selvan, Aarush and Wee, Joel and Liu, Chenxi and Majzoubi, Maryam and Yu, Weiren and Abernethy, Jake and Liechty, Tyler and Pan, Renke and Nguyen, Hoang and Qiong and Hu and Perrin, Sarah and Arora, Abhinav and Pitler, Emily and Wang, Weiyi and Shivakumar, Kaushik and Prost, Flavien and Limonchik, Ben and Wang, Jing and Gao, Yi and Cour, Timothee and Buch, Shyamal and Gui, Huan and Ivanova, Maria and Neubeck, Philipp and Chan, Kelvin and Kim, Lucy and Chen, Huizhong and Goyal, Naman and Chung, Da-Woon and Liu, Lu and Su, Yao and Petrushkina, Anastasia and Shen, Jiajun and Joulin, Armand and Xu, Yuanzhong and Lin, Stein Xudong and Kulizhskaya, Yana and Chelba, Ciprian and Vasudevan, Shobha and Collins, Eli and Bashlovkina, Vasilisa and Lu, Tony and Fritz, Doug and Park, Jongbin and Zhou, Yanqi and Su, Chen and Tanburn, Richard and Sushkov, Mikhail and Rasquinha, Mitchelle and Li, Jinning and Prendki, Jennifer and Li, Yiming and LV, Pallavi and Sharma, Shriya and Fitoussi, Hen and Huang, Hui and Dai, Andrew and Dao, Phuong and Burrows, Mike and Prior, Henry and Qin, Danfeng and Pundak, Golan and Sjoesund, Lars Lowe and Khurshudov, Art and Zhu, Zhenkai and Webson, Albert and Kemp, Elizabeth and Tan, Tat and Agrawal, Saurabh and Sargsyan, Susie and Cheng, Liqun and Stephan, Jim and Kwiatkowski, Tom and Reid, David and Byravan, Arunkumar and Michaely, Assaf Hurwitz and Heess, Nicolas and Zhou, Luowei and Goenka, Sonam and Carpenter, Viral and Levskaya, Anselm and Wang, Bo and Roberts, Reed and Leblond, Rémi and Chikkerur, Sharat and Ginzburg, Stav and Chang, Max and Riachi, Robert and Chuqiao and Xu and Borsos, Zalán and Pliskin, Michael and Pawar, Julia and Lustman, Morgane and Kirkwood, Hannah and Anand, Ankit and Chaudhary, Aditi and Kalb, Norbert and Milan, Kieran and Augenstein, Sean and Goldie, Anna and Prince, Laurel and Raman, Karthik and Sun, Yanhua and Xia, Vivian and Cohen, Aaron and Huo, Zhouyuan and Camp, Josh and Ellis, Seher and Zilka, Lukas and Torres, David Vilar and Patel, Lisa and Arora, Sho and Chan, Betty and Adler, Jonas and Ayoub, Kareem and Liang, Jacky and Jamil, Fayaz and Jiang, Jiepu and Baumgartner, Simon and Sun, Haitian and Karov, Yael and Akulov, Yaroslav and Zheng, Hui and Cai, Irene and Fantacci, Claudio and Rubin, James and Acha, Alex Rav and Wang, Mengchao and D'Souza, Nina and Sathyanarayana, Rohit and Dai, Shengyang and Rowe, Simon and Simanovsky, Andrey and Goldman, Omer and Kuang, Yuheng and Pan, Xiaoyue and Rosenberg, Andrew and Rojas-Esponda, Tania and Dutta, Praneet and Zeng, Amy and Jurenka, Irina and Farquhar, Greg and Bansal, Yamini and Iqbal, Shariq and Roelofs, Becca and Joung, Ga-Young and Beak, Parker and Ryu, Changwan and Poplin, Ryan and Wu, Yan and Alayrac, Jean-Baptiste and Buthpitiya, Senaka and Ronneberger, Olaf and Habtegebriel, Caleb and Li, Wei and Cavallaro, Paul and Wei, Aurora and Bensky, Guy and Denk, Timo and Ganapathy, Harish and Stanway, Jeff and Joshi, Pratik and Bertolini, Francesco and Lo, Jessica and Ma, Olivia and Charles, Zachary and Sampemane, Geta and Sahni, Himanshu and Chen, Xu and Askham, Harry and Gaddy, David and Young, Peter and Tan, Jiewen and Eyal, Matan and Bražinskas, Arthur and Zhong, Li and Wu, Zhichun and Epstein, Mark and Bailey, Kai and Hard, Andrew and Lee, Kamyu and Goldshtein, Sasha and Ruiz, Alex and Badawi, Mohammed and Lochbrunner, Matthias and Kearns, J. K. and Brown, Ashley and Pardo, Fabio and Weber, Theophane and Yang, Haichuan and Jiang, Pan-Pan and Akin, Berkin and Fu, Zhao and Wainwright, Marcus and Zou, Chi and Gaba, Meenu and Manzagol, Pierre-Antoine and Kan, Wendy and Song, Yang and Zainullina, Karina and Lin, Rui and Ko, Jeongwoo and Deshmukh, Salil and Jindal, Apoorv and Svensson, James and Tyam, Divya and Zhao, Heri and Kaeser-Chen, Christine and Baird, Scott and Moradi, Pooya and Hall, Jamie and Guo, Qiuchen and Tsang, Vincent and Liang, Bowen and Pereira, Fernando and Ganesh, Suhas and Korotkov, Ivan and Adamek, Jakub and Thiagarajan, Sridhar and Tran, Vinh and Chen, Charles and Tar, Chris and Jain, Sanil and Dasgupta, Ishita and Bilal, Taylan and Reitter, David and Zhao, Kai and Vezzani, Giulia and Gehman, Yasmin and Mehta, Pulkit and Beltrone, Lauren and Dotiwalla, Xerxes and Guadarrama, Sergio and Abbas, Zaheer and Karp, Stefani and Georgiev, Petko and Ferng, Chun-Sung and Brockschmidt, Marc and Peng, Liqian and Hirnschall, Christoph and Verma, Vikas and Bi, Yingying and Xiao, Ying and Dabush, Avigail and Xu, Kelvin and Wallis, Phil and Parker, Randall and Wang, Qifei and Xu, Yang and Safarli, Ilkin and Tewari, Dinesh and Zhang, Yin and Kim, Seungyeon and Gesmundo, Andrea and Thomas, Mackenzie and Levi, Sergey and Chowdhury, Ahmed and Rao, Kanishka and Garst, Peter and Conway-Rahman, Sam and Ran, Helen and McKinney, Kay and Xiao, Zhisheng and Yu, Wenhao and Agrawal, Rohan and Stjerngren, Axel and Ionescu, Catalin and Chen, Jingjing and Sharma, Vivek and Chiu, Justin and Liu, Fei and Franko, Ken and Sanford, Clayton and Cai, Xingyu and Michel, Paul and Ganapathy, Sanjay and Labanowski, Jane and Garrett, Zachary and Vargas, Ben and Sun, Sean and Gale, Bryan and Buschmann, Thomas and Desjardins, Guillaume and Ghelani, Nimesh and Jain, Palak and Verma, Mudit and Asawaroengchai, Chulayuth and Eisenschlos, Julian and Harlalka, Jitendra and Kazawa, Hideto and Metzler, Don and Howland, Joshua and Jian, Ying and Ades, Jake and Shah, Viral and Gangwani, Tynan and Lee, Seungji and Ring, Roman and Hernandez, Steven M. and Reich, Dean and Sinha, Amer and Sathe, Ashutosh and Kovac, Joe and Gill, Ashleah and Kannan, Ajay and D'olimpio, Andrea and Sevenich, Martin and Whang, Jay and Kim, Been and Sim, Khe Chai and Chen, Jilin and Zhang, Jiageng and Lall, Shuba and Matias, Yossi and Jia, Bill and Friesen, Abe and Nasso, Sara and Thapliyal, Ashish and Perozzi, Bryan and Yu, Ting and Shekhawat, Anna and Huda, Safeen and Grabowski, Peter and Wang, Eric and Sreevatsa, Ashwin and Dib, Hilal and Hassen, Mehadi and Schuh, Parker and Milutinovic, Vedrana and Welty, Chris and Quinn, Michael and Shah, Ali and Wang, Bangju and Barth-Maron, Gabe and Frye, Justin and Axelsson, Natalie and Zhu, Tao and Ma, Yukun and Giannoumis, Irene and Sedghi, Hanie and Ye, Chang and Luan, Yi and Aydin, Kevin and Chandra, Bilva and Sampathkumar, Vivek and Huang, Ronny and Lavrenko, Victor and Eleryan, Ahmed and Hong, Zhi and Hansen, Steven and Carthy, Sara Mc and Samanta, Bidisha and Ćevid, Domagoj and Wang, Xin and Li, Fangtao and Voznesensky, Michael and Hoffman, Matt and Terzis, Andreas and Sehwag, Vikash and Fidel, Gil and He, Luheng and Cai, Mu and He, Yanzhang and Feng, Alex and Nikoltchev, Martin and Phatale, Samrat and Chase, Jason and Lawton, Rory and Zhang, Ming and Ouyang, Tom and Tragut, Manuel and Manshadi, Mehdi Hafezi and Narayanan, Arjun and Shen, Jiaming and Gao, Xu and Bolukbasi, Tolga and Roy, Nick and Li, Xin and Golovin, Daniel and Panait, Liviu and Qin, Zhen and Han, Guangxing and Anthony, Thomas and Kudugunta, Sneha and Patraucean, Viorica and Ray, Aniket and Chen, Xinyun and Yang, Xiaochen and Bhatia, Tanuj and Talluri, Pranav and Morris, Alex and Ražnatović, Andrija and Brownfield, Bethanie and An, James and Peng, Sheng and Kane, Patrick and Zheng, Ce and Duduta, Nico and Kessinger, Joshua and Noraky, James and Liu, Siqi and Rong, Keran and Veličković, Petar and Rush, Keith and Goldin, Alex and Wei, Fanny and Garlapati, Shiva Mohan Reddy and Pantofaru, Caroline and Kwon, Okwan and Ni, Jianmo and Noland, Eric and Trapani, Julia Di and Beaufays, Françoise and Roy, Abhijit Guha and Chow, Yinlam and Turker, Aybuke and Cideron, Geoffrey and Mei, Lantao and Clark, Jon and Dou, Qingyun and Bošnjak, Matko and Leith, Ralph and Du, Yuqing and Yazdanbakhsh, Amir and Nasr, Milad and Kwak, Chester and Sheth, Suraj Satishkumar and Kaskasoli, Alex and Anand, Ankesh and Lakshminarayanan, Balaji and Jerome, Sammy and Bieber, David and Chu, Chun-Te and Senges, Alexandre and Shen, Tianxiao and Sridhar, Mukund and Ndebele, Ndaba and Beyret, Benjamin and Mohamed, Shakir and Chen, Mia and Freitag, Markus and Guo, Jiaxian and Liu, Luyang and Roit, Paul and Chen, Heng and Yan, Shen and Stone, Tom and Co-Reyes, J. D. and Cole, Jeremy and Scellato, Salvatore and Azizi, Shekoofeh and Hashemi, Hadi and Jin, Alicia and Iyer, Anand and Valentine, Marcella and György, András and Ahuja, Arun and Diaz, Daniel Hernandez and Lee, Chen-Yu and Clement, Nathan and Kong, Weize and Garmon, Drew and Watts, Ishaan and Bhatia, Kush and Gupta, Khyatti and Miecnikowski, Matt and Vallet, Hugo and Taly, Ankur and Loper, Edward and Joshi, Saket and Atwood, James and Chick, Jo and Collier, Mark and Iliopoulos, Fotis and Trostle, Ryan and Gunel, Beliz and Leal-Cavazos, Ramiro and Hrafnkelsson, Arnar Mar and Guzman, Michael and Ju, Xiaoen and Forbes, Andy and Emond, Jesse and Chauhan, Kushal and Caine, Ben and Xiao, Li and Zeng, Wenjun and Moufarek, Alexandre and Murphy, Daniel and Meng, Maya and Gupta, Nitish and Riedel, Felix and Das, Anil and Lawal, Elijah and Narayan, Shashi and Sosea, Tiberiu and Swirhun, James and Friso, Linda and Neyshabur, Behnam and Lu, Jing and Girgin, Sertan and Wunder, Michael and Yvinec, Edouard and Pyne, Aroonalok and Carbune, Victor and Rijhwani, Shruti and Guo, Yang and Doshi, Tulsee and Briukhov, Anton and Bain, Max and Hitron, Ayal and Wang, Xuanhui and Gupta, Ashish and Chen, Ke and Du, Cosmo and Zhang, Weiyang and Shah, Dhruv and Akula, Arjun and Dylla, Max and Kachra, Ashyana and Kuo, Weicheng and Zou, Tingting and Wang, Lily and Xu, Luyao and Zhu, Jifan and Snyder, Justin and Menon, Sachit and Firat, Orhan and Mordatch, Igor and Yuan, Yuan and Ponomareva, Natalia and Blevins, Rory and Moore, Lawrence and Wang, Weijun and Chen, Phil and Scholz, Martin and Dwornik, Artur and Lin, Jason and Li, Sicheng and Antognini, Diego and I, Te and Song, Xiaodan and Miller, Matt and Kalra, Uday and Raveret, Adam and Akerlund, Oscar and Wu, Felix and Nystrom, Andrew and Godbole, Namrata and Liu, Tianqi and DeBalsi, Hannah and Zhao, Jewel and Liu, Buhuang and Caciularu, Avi and Lax, Lauren and Khandelwal, Urvashi and Langston, Victoria and Bailey, Eric and Lattanzi, Silvio and Wang, Yufei and Kovelamudi, Neel and Mondal, Sneha and Guruganesh, Guru and Hua, Nan and Roval, Ofir and Wesołowski, Paweł and Ingale, Rishikesh and Halcrow, Jonathan and Sohn, Tim and Angermueller, Christof and Raad, Bahram and Stickgold, Eli and Lu, Eva and Kosik, Alec and Xie, Jing and Lillicrap, Timothy and Huang, Austin and Zhang, Lydia Lihui and Paulus, Dominik and Farabet, Clement and Wertheim, Alex and Wang, Bing and Joshi, Rishabh and Ko, Chu-ling and Wu, Yonghui and Agrawal, Shubham and Lin, Lily and Sheng, XiangHai and Sung, Peter and Breland-King, Tyler and Butterfield, Christina and Gawde, Swapnil and Singh, Sumeet and Zhang, Qiao and Apte, Raj and Shetty, Shilpa and Hutter, Adrian and Li, Tao and Salesky, Elizabeth and Lebron, Federico and Kanerva, Jonni and Paganini, Michela and Nguyen, Arthur and Vallu, Rohith and Peter, Jan-Thorsten and Velury, Sarmishta and Kao, David and Hoover, Jay and Bortsova, Anna and Bishop, Colton and Jakobovits, Shoshana and Agostini, Alessandro and Agarwal, Alekh and Liu, Chang and Kwong, Charles and Tavakkol, Sasan and Bica, Ioana and Greve, Alex and GP, Anirudh and Marcus, Jake and Hou, Le and Duerig, Tom and Moroshko, Rivka and Lacey, Dave and Davis, Andy and Amelot, Julien and Wang, Guohui and Kim, Frank and Strinopoulos, Theofilos and Wan, Hui and Lan, Charline Le and Krishnan, Shankar and Tang, Haotian and Humphreys, Peter and Bai, Junwen and Shtacher, Idan Heimlich and Machado, Diego and Pang, Chenxi and Burke, Ken and Liu, Dangyi and Aravamudhan, Renga and Song, Yue and Hirst, Ed and Singh, Abhimanyu and Jou, Brendan and Bai, Liang and Piccinno, Francesco and Fu, Chuyuan Kelly and Alazard, Robin and Meiri, Barak and Winter, Daniel and Chen, Charlie and Zhang, Mingda and Heitkaemper, Jens and Lambert, John and Lee, Jinhyuk and Frömmgen, Alexander and Rogulenko, Sergey and Nair, Pranav and Niemczyk, Paul and Bulyenov, Anton and Xu, Bibo and Shemtov, Hadar and Zadimoghaddam, Morteza and Toropov, Serge and Wirth, Mateo and Dai, Hanjun and Gollapudi, Sreenivas and Zheng, Daniel and Kurakin, Alex and Lee, Chansoo and Bullard, Kalesha and Serrano, Nicolas and Balazevic, Ivana and Li, Yang and Schalkwyk, Johan and Murphy, Mark and Zhang, Mingyang and Sequeira, Kevin and Datta, Romina and Agrawal, Nishant and Sutton, Charles and Attaluri, Nithya and Chiang, Mencher and Farhan, Wael and Thornton, Gregory and Lin, Kate and Choma, Travis and Nguyen, Hung and Dasgupta, Kingshuk and Robinson, Dirk and Comşa, Iulia and Riley, Michael and Pillai, Arjun and Mustafa, Basil and Golan, Ben and Zandieh, Amir and Lespiau, Jean-Baptiste and Porter, Billy and Ross, David and Rajayogam, Sujeevan and Agarwal, Mohit and Venugopalan, Subhashini and Shahriari, Bobak and Yan, Qiqi and Xu, Hao and Tobin, Taylor and Dubov, Pavel and Shi, Hongzhi and Recasens, Adrià and Kovsharov, Anton and Borgeaud, Sebastian and Dery, Lucio and Vasanth, Shanthal and Gribovskaya, Elena and Qiu, Linhai and Mahdieh, Mahdis and Skut, Wojtek and Nielsen, Elizabeth and Zheng, C. J. and Yu, Adams and Bostock, Carrie Grimes and Gupta, Shaleen and Archer, Aaron and Rawles, Chris and Davies, Elinor and Svyatkovskiy, Alexey and Tsai, Tomy and Halpern, Yoni and Reisswig, Christian and Wydrowski, Bartek and Chang, Bo and Puigcerver, Joan and Taege, Mor Hazan and Li, Jian and Schnider, Eva and Li, Xinjian and Dena, Dragos and Xu, Yunhan and Telang, Umesh and Shi, Tianze and Zen, Heiga and Kastner, Kyle and Ko, Yeongil and Subramaniam, Neesha and Kumar, Aviral and Blois, Pete and Dai, Zhuyun and Wieting, John and Lu, Yifeng and Zeldes, Yoel and Xie, Tian and Hauth, Anja and Ţifrea, Alexandru and Li, Yuqi and El-Husseini, Sam and Abolafia, Dan and Zhou, Howard and Ding, Wen and Ghalebikesabi, Sahra and Guía, Carlos and Maksai, Andrii and Weisz, Ágoston and Arik, Sercan and Sukhanov, Nick and Świetlik, Aga and Jia, Xuhui and Yu, Luo and Wang, Weiyue and Brand, Mark and Bloxwich, Dawn and Kirmani, Sean and Chen, Zhe and Go, Alec and Sprechmann, Pablo and Kannen, Nithish and Carin, Alen and Sandhu, Paramjit and Edkins, Isabel and Nooteboom, Leslie and Gupta, Jai and Maggiore, Loren and Azizi, Javad and Pritch, Yael and Yin, Pengcheng and Gupta, Mansi and Tarlow, Danny and Smith, Duncan and Ivanov, Desi and Babaeizadeh, Mohammad and Goel, Ankita and Kambala, Satish and Chu, Grace and Kastelic, Matej and Liu, Michelle and Soltau, Hagen and Stone, Austin and Agrawal, Shivani and Kim, Min and Soparkar, Kedar and Tadepalli, Srinivas and Bunyan, Oskar and Soh, Rachel and Kannan, Arvind and Kim, D. Y. and Chen, Blake JianHang and Halumi, Afief and Roy, Sudeshna and Wang, Yulong and Sercinoglu, Olcan and Gibson, Gena and Bhatnagar, Sijal and Sano, Motoki and Dincklage, Daniel von and Ren, Qingchun and Mitrevski, Blagoj and Olšák, Mirek and She, Jennifer and Doersch, Carl and Jilei and Wang and Liu, Bingyuan and Tan, Qijun and Yakar, Tamar and Warkentin, Tris and Ramirez, Alex and Lebsack, Carl and Dillon, Josh and Mathews, Rajiv and Cobley, Tom and Wu, Zelin and Chen, Zhuoyuan and Simon, Jon and Nath, Swaroop and Sainath, Tara and Bendebury, Alexei and Julian, Ryan and Mankalale, Bharath and Ćurko, Daria and Zacchello, Paulo and Brown, Adam R. and Sodhia, Kiranbir and Howard, Heidi and Caelles, Sergi and Gupta, Abhinav and Evans, Gareth and Bulanova, Anna and Katzen, Lesley and Goldenberg, Roman and Tsitsulin, Anton and Stanton, Joe and Schillings, Benoit and Kovalev, Vitaly and Fry, Corey and Shah, Rushin and Lin, Kuo and Upadhyay, Shyam and Li, Cheng and Radpour, Soroush and Maggioni, Marcello and Xiong, Jing and Haas, Lukas and Brennan, Jenny and Kamath, Aishwarya and Savinov, Nikolay and Nagrani, Arsha and Yacovone, Trevor and Kappedal, Ryan and Andriopoulos, Kostas and Lao, Li and Li, YaGuang and Rozhdestvenskiy, Grigory and Hashimoto, Kazuma and Audibert, Andrew and Austin, Sophia and Rodriguez, Daniel and Ruoss, Anian and Honke, Garrett and Karkhanis, Deep and Xiong, Xi and Wei, Qing and Huang, James and Leng, Zhaoqi and Premachandran, Vittal and Bileschi, Stan and Evangelopoulos, Georgios and Mensink, Thomas and Pavagadhi, Jay and Teplyashin, Denis and Chang, Paul and Xue, Linting and Tanzer, Garrett and Goldman, Sally and Patel, Kaushal and Li, Shixin and Wiesner, Jeremy and Zheng, Ivy and Stewart-Binks, Ian and Han, Jie and Li, Zhi and Luo, Liangchen and Lenc, Karel and Lučić, Mario and Xue, Fuzhao and Mullins, Ryan and Guseynov, Alexey and Chang, Chung-Ching and Galatzer-Levy, Isaac and Zhang, Adam and Bingham, Garrett and Hu, Grace and Hartman, Ale and Ma, Yue and Griffith, Jordan and Irpan, Alex and Radebaugh, Carey and Yue, Summer and Fan, Lijie and Ungureanu, Victor and Sorokin, Christina and Teufel, Hannah and Li, Peiran and Anil, Rohan and Paparas, Dimitris and Wang, Todd and Lin, Chu-Cheng and Peng, Hui and Shum, Megan and Petrovic, Goran and Brady, Demetra and Nguyen, Richard and Macherey, Klaus and Li, Zhihao and Singh, Harman and Yenugula, Madhavi and Iinuma, Mariko and Chen, Xinyi and Kopparapu, Kavya and Stern, Alexey and Dave, Shachi and Thekkath, Chandu and Perot, Florence and Kumar, Anurag and Li, Fangda and Xiao, Yang and Bilotti, Matthew and Bateni, Mohammad Hossein and Noble, Isaac and Lee, Lisa and Vázquez-Reina, Amelio and Salazar, Julian and Yang, Xiaomeng and Wang, Boyu and Gruzewska, Ela and Rao, Anand and Raghuram, Sindhu and Xu, Zheng and Ben-David, Eyal and Mei, Jieru and Dalmia, Sid and Zhang, Zhaoyi and Liu, Yuchen and Bansal, Gagan and Pankov, Helena and Schwarcz, Steven and Burns, Andrea and Chan, Christine and Sanghai, Sumit and Liang, Ricky and Liang, Ethan and He, Antoine and Stuart, Amy and Narayanan, Arun and Zhu, Yukun and Frank, Christian and Fatemi, Bahar and Sabne, Amit and Lang, Oran and Bhattacharya, Indro and Settle, Shane and Wang, Maria and McMahan, Brendan and Tacchetti, Andrea and Soares, Livio Baldini and Hadian, Majid and Cabi, Serkan and Chung, Timothy and Putikhin, Nikita and Li, Gang and Chen, Jeremy and Tarango, Austin and Michalewski, Henryk and Kazemi, Mehran and Masoom, Hussain and Sheftel, Hila and Shivanna, Rakesh and Vadali, Archita and Comanescu, Ramona and Reid, Doug and Moore, Joss and Neelakantan, Arvind and Sander, Michaël and Herzig, Jonathan and Rosenberg, Aviv and Dehghani, Mostafa and Choi, J. D. and Fink, Michael and Hayes, Reid and Ge, Eric and Weng, Shitao and Ho, Chia-Hua and Karro, John and Krishna, Kalpesh and Thiet, Lam Nguyen and Skerry-Ryan, Amy and Eppens, Daniel and Andreetto, Marco and Sarma, Navin and Bonacina, Silvano and Ayan, Burcu Karagol and Nawhal, Megha and Shan, Zhihao and Dusenberry, Mike and Thakoor, Shantanu and Gubbi, Sagar and Nguyen, Duc Dung and Tsarfaty, Reut and Albanie, Samuel and Mitrović, Jovana and Gandhi, Meet and Chen, Bo-Juen and Epasto, Alessandro and Stephanov, Georgi and Jin, Ye and Gehman, Samuel and Amini, Aida and Weber, Jack and Behbahani, Feryal and Xu, Shawn and Allamanis, Miltos and Chen, Xi and Ott, Myle and Sha, Claire and Jastrzebski, Michal and Qi, Hang and Greene, David and Wu, Xinyi and Toki, Abodunrinwa and Vlasic, Daniel and Shapiro, Jane and Kotikalapudi, Ragha and Shen, Zhe and Saeki, Takaaki and Xie, Sirui and Cassirer, Albin and Bharadwaj, Shikhar and Kiyono, Tatsuya and Bhojanapalli, Srinadh and Rosenfeld, Elan and Ritter, Sam and Mao, Jieming and Oliveira, João Gabriel and Egyed, Zoltan and Bandemer, Bernd and Parisotto, Emilio and Kinoshita, Keisuke and Pluto, Juliette and Maniatis, Petros and Li, Steve and Guo, Yaohui and Ghiasi, Golnaz and Tarbouriech, Jean and Chatterjee, Srimon and Jin, Julie and Katrina and Xu and Palomaki, Jennimaria and Arnold, Séb and Sewak, Madhavi and Piccinini, Federico and Sharma, Mohit and Albrecht, Ben and Purser-haskell, Sean and Vaswani, Ashwin and Chen, Chongyan and Wisniewski, Matheus and Cao, Qin and Aslanides, John and Phu, Nguyet Minh and Sieb, Maximilian and Agubuzu, Lauren and Zheng, Anne and Sohn, Daniel and Selvi, Marco and Andreassen, Anders and Subudhi, Krishan and Eruvbetine, Prem and Woodman, Oliver and Mery, Tomas and Krause, Sebastian and Ren, Xiaoqi and Ma, Xiao and Luo, Jincheng and Chen, Dawn and Fan, Wei and Griffiths, Henry and Schuler, Christian and Li, Alice and Zhang, Shujian and Sarr, Jean-Michel and Luo, Shixin and Patana, Riccardo and Watson, Matthew and Naboulsi, Dani and Collins, Michael and Sidhwani, Sailesh and Hoogeboom, Emiel and Silver, Sharon and Caveness, Emily and Zhao, Xiaokai and Rodriguez, Mikel and Deines, Maxine and Bai, Libin and Griffin, Patrick and Tagliasacchi, Marco and Xue, Emily and Babbula, Spandana Raj and Pang, Bo and Ding, Nan and Shen, Gloria and Peake, Elijah and Crocker, Remi and Raghvendra, Shubha Srinivas and Swisher, Danny and Han, Woohyun and Singh, Richa and Wu, Ling and Pchelin, Vladimir and Munkhdalai, Tsendsuren and Alon, Dana and Bacon, Geoff and Robles, Efren and Bulian, Jannis and Johnson, Melvin and Powell, George and Ferreira, Felipe Tiengo and Li, Yaoyiran and Benzing, Frederik and Velimirović, Mihajlo and Soyer, Hubert and Kong, William and Tony and Nguyên and Yang, Zhen and Liu, Jeremiah and Amersfoort, Joost van and Gillick, Daniel and Sun, Baochen and Rauschmayr, Nathalie and Zhang, Katie and Zhan, Serena and Zhou, Tao and Frolov, Alexey and Yang, Chengrun and Vnukov, Denis and Rouillard, Louis and Li, Hongji and Mandhane, Amol and Fallen, Nova and Venkataraman, Rajesh and Hu, Clara Huiyi and Brennan, Jennifer and Lee, Jenny and Chang, Jerry and Sundermeyer, Martin and Pan, Zhufeng and Ke, Rosemary and Tong, Simon and Fabrikant, Alex and Bono, William and Gu, Jindong and Foley, Ryan and Mao, Yiran and Delakis, Manolis and Bhaswar, Dhruva and Frostig, Roy and Li, Nick and Zipori, Avital and Hope, Cath and Kozlova, Olga and Mishra, Swaroop and Djolonga, Josip and Schiff, Craig and Merey, Majd Al and Briakou, Eleftheria and Morgan, Peter and Wan, Andy and Hassidim, Avinatan and Skerry-Ryan, R. J. and Sengupta, Kuntal and Jasarevic, Mary and Kallakuri, Praveen and Kunkle, Paige and Brennan, Hannah and Lieber, Tom and Mansoor, Hassan and Walker, Julian and Zhang, Bing and Xie, Annie and Žužić, Goran and Chukwuka, Adaeze and Druinsky, Alex and Cho, Donghyun and Yao, Rui and Naeem, Ferjad and Butt, Shiraz and Kim, Eunyoung and Jia, Zhipeng and Jordan, Mandy and Lelkes, Adam and Kurzeja, Mark and Wang, Sophie and Zhao, James and Over, Andrew and Chakladar, Abhishek and Prasetya, Marcel and Jha, Neha and Ganapathy, Sriram and Cong, Yale and Shroff, Prakash and Saroufim, Carl and Miryoosefi, Sobhan and Hammad, Mohamed and Nasir, Tajwar and Xi, Weijuan and Gao, Yang and Maeng, Young and Hora, Ben and Cheng, Chin-Yi and Haghani, Parisa and Lewenberg, Yoad and Lu, Caden and Matysiak, Martin and Raisinghani, Naina and Wang, Huiyu and Baugher, Lexi and Sukthankar, Rahul and Giang, Minh and Schultz, John and Fiedel, Noah and Chen, Minmin and Lee, Cheng-Chun and Dey, Tapomay and Zheng, Hao and Paul, Shachi and Smith, Celine and Ly, Andy and Wang, Yicheng and Bansal, Rishabh and Perz, Bartek and Ricco, Susanna and Blank, Stasha and Keshava, Vaishakh and Sharma, Deepak and Chow, Marvin and Lad, Kunal and Jalan, Komal and Osindero, Simon and Swanson, Craig and Scott, Jacob and Ilić, Anastasija and Li, Xiaowei and Jonnalagadda, Siddhartha Reddy and Soudagar, Afzal Shama and Xiong, Yan and Batsaikhan, Bat-Orgil and Jarrett, Daniel and Kumar, Naveen and Shah, Maulik and Lawlor, Matt and Waters, Austin and Graham, Mark and May, Rhys and Ramos, Sabela and Lefdal, Sandra and Cankara, Zeynep and Cano, Nacho and O'Donoghue, Brendan and Borovik, Jed and Liu, Frederick and Grimstad, Jordan and Alnahlawi, Mahmoud and Tsihlas, Katerina and Hudson, Tom and Grigorev, Nikolai and Jia, Yiling and Huang, Terry and Igwe, Tobenna Peter and Lebedev, Sergei and Tang, Xiaodan and Krivokon, Igor and Garcia, Frankie and Tan, Melissa and Jia, Eric and Stys, Peter and Vashishth, Shikhar and Liang, Yu and Venkatraman, Balaji and Gu, Chenjie and Kementsietsidis, Anastasios and Zhu, Chen and Jung, Junehyuk and Bai, Yunfei and Hosseini, Mohammad Javad and Ahmed, Faruk and Gupta, Aditya and Yuan, Xin and Ashraf, Shereen and Nigam, Shitij and Vasudevan, Gautam and Awasthi, Pranjal and Gilady, Adi Mayrav and Mariet, Zelda and Eskander, Ramy and Li, Haiguang and Hu, Hexiang and Garrido, Guillermo and Schlattner, Philippe and Zhang, George and Saxena, Rohun and Dević, Petar and Muralidharan, Kritika and Murthy, Ashwin and Zhou, Yiqian and Choi, Min and Wongpanich, Arissa and Wang, Zhengdong and Shah, Premal and Xu, Yuntao and Huang, Yiling and Spencer, Stephen and Chen, Alice and Cohan, James and Wang, Junjie and Tompson, Jonathan and Wu, Junru and Haroun, Ruba and Li, Haiqiong and Huergo, Blanca and Yang, Fan and Yin, Tongxin and Wendt, James and Bendersky, Michael and Chaabouni, Rahma and Snaider, Javier and Ferret, Johan and Jindal, Abhishek and Thompson, Tara and Xue, Andrew and Bishop, Will and Phal, Shubham Milind and Sharma, Archit and Sung, Yunhsuan and Radhakrishnan, Prabakar and Shomrat, Mo and Ingle, Reeve and Vij, Roopali and Gilmer, Justin and Istin, Mihai Dorin and Sobell, Sam and Lu, Yang and Nottage, Emily and Sadigh, Dorsa and Willcock, Jeremiah and Zhang, Tingnan and Xu, Steve and Brown, Sasha and Lee, Katherine and Wang, Gary and Zhu, Yun and Tay, Yi and Kim, Cheolmin and Gutierrez, Audrey and Sharma, Abhanshu and Xian, Yongqin and Seo, Sungyong and Cui, Claire and Pochernina, Elena and Baetu, Cip and Jastrzębski, Krzysztof and Ly, Mimi and Elhawaty, Mohamed and Suh, Dan and Sezener, Eren and Wang, Pidong and Yuen, Nancy and Tucker, George and Cai, Jiahao and Yang, Zuguang and Wang, Cindy and Muzio, Alex and Qian, Hai and Yoo, Jae and Lockhart, Derek and McKee, Kevin R. and Guo, Mandy and Mehrotra, Malika and Mendonça, Artur and Mehta, Sanket Vaibhav and Ben, Sherry and Tekur, Chetan and Mu, Jiaqi and Zhu, Muye and Krakovna, Victoria and Lee, Hongrae and Maschinot, A. J. and Cevey, Sébastien and Choe, HyunJeong and Bai, Aijun and Srinivasan, Hansa and Gasaway, Derek and Young, Nick and Siegler, Patrick and Holtmann-Rice, Dan and Piratla, Vihari and Baumli, Kate and Yogev, Roey and Hofer, Alex and Hasselt, Hado van and Grant, Svetlana and Chervonyi, Yuri and Silver, David and Hogue, Andrew and Agarwal, Ayushi and Wang, Kathie and Singh, Preeti and Flynn, Four and Lipschultz, Josh and David, Robert and Bellot, Lizzetth and Yang, Yao-Yuan and Le, Long and Graziano, Filippo and Olszewska, Kate and Hui, Kevin and Maurya, Akanksha and Parotsidis, Nikos and Chen, Weijie and Oguntebi, Tayo and Kelley, Joe and Baddepudi, Anirudh and Mauerer, Johannes and Shaw, Gregory and Siegman, Alex and Yang, Lin and Shetty, Shravya and Roy, Subhrajit and Song, Yunting and Stokowiec, Wojciech and Burnell, Ryan and Savant, Omkar and Busa-Fekete, Robert and Miao, Jin and Ghosh, Samrat and MacDermed, Liam and Lippe, Phillip and Dektiarev, Mikhail and Behrman, Zach and Mentzer, Fabian and Nguyen, Kelvin and Wei, Meng and Verma, Siddharth and Knutsen, Chris and Dasari, Sudeep and Yan, Zhipeng and Mitrichev, Petr and Wang, Xingyu and Shejwalkar, Virat and Austin, Jacob and Sunkara, Srinivas and Potti, Navneet and Virin, Yan and Wright, Christian and Liu, Gaël and Riva, Oriana and Pot, Etienne and Kochanski, Greg and Le, Quoc and Balasubramaniam, Gargi and Dhar, Arka and Liao, Yuguo and Bloniarz, Adam and Shukla, Divyansh and Cole, Elizabeth and Lee, Jong and Zhang, Sheng and Kafle, Sushant and Vashishtha, Siddharth and Mahmoudieh, Parsa and Chen, Grace and Hoffmann, Raphael and Srinivasan, Pranesh and Lago, Agustin Dal and Shalom, Yoav Ben and Wang, Zi and Elabd, Michael and Sharma, Anuj and Oh, Junhyuk and Kothawade, Suraj and Le, Maigo and Monteiro, Marianne and Yang, Shentao and Alarakyia, Kaiz and Geirhos, Robert and Mincu, Diana and Garnes, Håvard and Kobayashi, Hayato and Mariooryad, Soroosh and Krasowiak, Kacper and Zhixin and Lai and Mourad, Shibl and Wang, Mingqiu and Bu, Fan and Aharoni, Ophir and Chen, Guanjie and Goyal, Abhimanyu and Zubov, Vadim and Bapna, Ankur and Dabir, Elahe and Kothari, Nisarg and Lamerigts, Kay and Cao, Nicola De and Shar, Jeremy and Yew, Christopher and Kulkarni, Nitish and Mahaarachchi, Dre and Joshi, Mandar and Zhu, Zhenhai and Lichtarge, Jared and Zhou, Yichao and Muckenhirn, Hannah and Selo, Vittorio and Vinyals, Oriol and Chen, Peter and Brohan, Anthony and Mehta, Vaibhav and Cogan, Sarah and Wang, Ruth and Geri, Ty and Ko, Wei-Jen and Chen, Wei and Viola, Fabio and Shivam, Keshav and Wang, Lisa and Elish, Madeleine Clare and Popa, Raluca Ada and Pereira, Sébastien and Liu, Jianqiao and Koster, Raphael and Kim, Donnie and Zhang, Gufeng and Ebrahimi, Sayna and Talukdar, Partha and Zheng, Yanyan and Poklukar, Petra and Mikhalap, Ales and Johnson, Dale and Vijayakumar, Anitha and Omernick, Mark and Dibb, Matt and Dubey, Ayush and Hu, Qiong and Suman, Apurv and Aggarwal, Vaibhav and Kornakov, Ilya and Xia, Fei and Lowe, Wing and Kolganov, Alexey and Xiao, Ted and Nikolaev, Vitaly and Hemingray, Steven and Li, Bonnie and Iljazi, Joana and Rybiński, Mikołaj and Sandhu, Ballie and Lu, Peggy and Luong, Thang and Jenatton, Rodolphe and Govindaraj, Vineetha and Hui and Li and Dulac-Arnold, Gabriel and Park, Wonpyo and Wang, Henry and Modi, Abhinit and Pouget-Abadie, Jean and Greller, Kristina and Gupta, Rahul and Berry, Robert and Ramachandran, Prajit and Xie, Jinyu and McCafferty, Liam and Wang, Jianling and Gupta, Kilol and Lim, Hyeontaek and Bratanič, Blaž and Brock, Andy and Akolzin, Ilia and Sproch, Jim and Karliner, Dan and Kim, Duhyeon and Goedeckemeyer, Adrian and Shazeer, Noam and Schmid, Cordelia and Calandriello, Daniele and Bhatia, Parul and Choromanski, Krzysztof and Montgomery, Ceslee and Dua, Dheeru and Ramalho, Ana and King, Helen and Gao, Yue and Nguyen, Lynn and Lindner, David and Pitta, Divya and Johnson, Oleaser and Salama, Khalid and Ardila, Diego and Han, Michael and Farnese, Erin and Odoom, Seth and Wang, Ziyue and Ding, Xiangzhuo and Rink, Norman and Smith, Ray and Lehri, Harshal Tushar and Cohen, Eden and Vats, Neera and He, Tong and Gopavarapu, Parthasarathy and Paszke, Adam and Patel, Miteyan and Gansbeke, Wouter Van and Loher, Lucia and Castro, Luis and Voitovich, Maria and Glehn, Tamara von and George, Nelson and Niklaus, Simon and Eaton-Rosen, Zach and Rakićević, Nemanja and Jue, Erik and Perel, Sagi and Zhang, Carrie and Bahat, Yuval and Pouget, Angéline and Xing, Zhi and Huot, Fantine and Shenoy, Ashish and Bos, Taylor and Coriou, Vincent and Richter, Bryan and Noy, Natasha and Wang, Yaqing and Ontanon, Santiago and Qin, Siyang and Makarchuk, Gleb and Hassabis, Demis and Li, Zhuowan and Sharma, Mandar and Venkatesan, Kumaran and Kemaev, Iurii and Daniel, Roxanne and Huang, Shiyu and Shah, Saloni and Ponce, Octavio and Warren and Chen and Faruqui, Manaal and Wu, Jialin and Andačić, Slavica and Payrits, Szabolcs and McDuff, Daniel and Hume, Tom and Cao, Yuan and Tessler, M. H. and Wang, Qingze and Wang, Yinan and Rendulic, Ivor and Agustsson, Eirikur and Johnson, Matthew and Lando, Tanya and Howard, Andrew and Padmanabhan, Sri Gayatri Sundara and Daswani, Mayank and Banino, Andrea and Kilgore, Michael and Heek, Jonathan and Ji, Ziwei and Caceres, Alvaro and Li, Conglong and Kassner, Nora and Vlaskin, Alexey and Liu, Zeyu and Grills, Alex and Hou, Yanhan and Sukkerd, Roykrong and Cheon, Gowoon and Shetty, Nishita and Markeeva, Larisa and Stanczyk, Piotr and Iyer, Tejas and Gong, Yuan and Gao, Shawn and Gopalakrishnan, Keerthana and Blyth, Tim and Reynolds, Malcolm and Bhoopchand, Avishkar and Bilenko, Misha and Gharibian, Dero and Zayats, Vicky and Faust, Aleksandra and Singh, Abhinav and Ma, Min and Jiao, Hongyang and Vijayanarasimhan, Sudheendra and Aroyo, Lora and Yadav, Vikas and Chakera, Sarah and Kakarla, Ashwin and Meshram, Vilobh and Gregor, Karol and Botea, Gabriela and Senter, Evan and Jia, Dawei and Kovacs, Geza and Sharma, Neha and Baur, Sebastien and Kang, Kai and He, Yifan and Zhuo, Lin and Kostelac, Marija and Laish, Itay and Peng, Songyou and O'Bryan, Louis and Kasenberg, Daniel and Rao, Girish Ramchandra and Leurent, Edouard and Zhang, Biao and Stevens, Sage and Salazar, Ana and Zhang, Ye and Lobov, Ivan and Walker, Jake and Porter, Allen and Redshaw, Morgan and Ke, Han and Rao, Abhishek and Lee, Alex and Lam, Hoi and Moffitt, Michael and Kim, Jaeyoun and Qiao, Siyuan and Koo, Terry and Dadashi, Robert and Song, Xinying and Sundararajan, Mukund and Xu, Peng and Kawamoto, Chizu and Zhong, Yan and Barbu, Clara and Reddy, Apoorv and Verzetti, Mauro and Li, Leon and Papamakarios, George and Klimczak-Plucińska, Hanna and Cassin, Mary and Kavukcuoglu, Koray and Swavely, Rigel and Vaucher, Alain and Zhao, Jeffrey and Hemsley, Ross and Tschannen, Michael and Ge, Heming and Menghani, Gaurav and Yu, Yang and Ha, Natalie and He, Wei and Wu, Xiao and Song, Maggie and Sterneck, Rachel and Zinke, Stefan and Calian, Dan A. and Marsden, Annie and Ruiz, Alejandro Cruzado and Hessel, Matteo and Gueta, Almog and Lee, Benjamin and Farris, Brian and Gupta, Manish and Li, Yunjie and Saleh, Mohammad and Misra, Vedant and Xiao, Kefan and Mendolicchio, Piermaria and Buttimore, Gavin and Krayvanova, Varvara and Nayakanti, Nigamaa and Wiethoff, Matthew and Pande, Yash and Mirhoseini, Azalia and Lao, Ni and Liu, Jasmine and Hua, Yiqing and Chen, Angie and Malkov, Yury and Kalashnikov, Dmitry and Gupta, Shubham and Audhkhasi, Kartik and Zhai, Yuexiang and Kopalle, Sudhindra and Jain, Prateek and Ofek, Eran and Meyer, Clemens and Baatarsukh, Khuslen and Strejček, Hana and Qian, Jun and Freedman, James and Figueira, Ricardo and Sokolik, Michal and Bachem, Olivier and Lin, Raymond and Kharrat, Dia and Hidey, Chris and Xu, Pingmei and Duan, Dennis and Li, Yin and Ersoy, Muge and Everett, Richard and Cen, Kevin and Santamaria-Fernandez, Rebeca and Taubenfeld, Amir and Mackinnon, Ian and Deng, Linda and Zablotskaia, Polina and Viswanadha, Shashank and Goel, Shivanker and Yates, Damion and Deng, Yunxiao and Choy, Peter and Chen, Mingqing and Sinha, Abhishek and Mossin, Alex and Wang, Yiming and Szlam, Arthur and Hao, Susan and Rubenstein, Paul Kishan and Toksoz-Exley, Metin and Aperghis, Miranda and Zhong, Yin and Ahn, Junwhan and Isard, Michael and Lacombe, Olivier and Luisier, Florian and Anastasiou, Chrysovalantis and Kalley, Yogesh and Prabhu, Utsav and Dunleavy, Emma and Bijwadia, Shaan and Mao-Jones, Justin and Chen, Kelly and Pasumarthi, Rama and Wood, Emily and Dostmohamed, Adil and Hurley, Nate and Simsa, Jiri and Parrish, Alicia and Pajarskas, Mantas and Harvey, Matt and Skopek, Ondrej and Kochinski, Yony and Rey, Javier and Rieser, Verena and Zhou, Denny and Lee, Sun Jae and Acharya, Trilok and Li, Guowang and Jiang, Joe and Zhang, Xiaofan and Gipson, Bryant and Mahintorabi, Ethan and Gelmi, Marco and Khajehnouri, Nima and Yeh, Angel and Lee, Kayi and Matthey, Loic and Baker, Leslie and Pham, Trang and Fu, Han and Pak, Alex and Gupta, Prakhar and Vasconcelos, Cristina and Sadovsky, Adam and Walker, Brian and Hsiao, Sissie and Zochbauer, Patrik and Marzoca, Andreea and Velan, Noam and Zeng, Junhao and Baechler, Gilles and Driess, Danny and Jain, Divya and Huang, Yanping and Tao, Lizzie and Maggs, John and Levine, Nir and Schneider, Jon and Gemzer, Erika and Petit, Samuel and Han, Shan and Fisher, Zach and Zelle, Dustin and Biles, Courtney and Ie, Eugene and Fadeeva, Asya and Liu, Casper and Franco, Juliana Vicente and Collister, Adrian and Zhang, Hao and Wang, Renshen and Zhao, Ruizhe and Kieliger, Leandro and Shuster, Kurt and Zhu, Rui and Gong, Boqing and Chan, Lawrence and Sun, Ruoxi and Basu, Sujoy and Zimmermann, Roland and Hayes, Jamie and Bapna, Abhishek and Snoek, Jasper and Yang, Weel and Datta, Puranjay and Abdallah, Jad Al and Kilgour, Kevin and Li, Lu and Mah, S. Q. and Jun, Yennie and Rivière, Morgane and Karmarkar, Abhijit and Spalink, Tammo and Huang, Tao and Gonzalez, Lucas and Tran, Duc-Hieu and Nowak, Averi and Palowitch, John and Chadwick, Martin and Talius, Ellie and Mehta, Harsh and Sellam, Thibault and Fränken, Philipp and Nicosia, Massimo and He, Kyle and Kini, Aditya and Amos, David and Basu, Sugato and Jobe, Harrison and Shaw, Eleni and Xu, Qiantong and Evans, Colin and Ikeda, Daisuke and Yan, Chaochao and Jin, Larry and Wang, Lun and Yadav, Sachin and Labzovsky, Ilia and Sampath, Ramesh and Ma, Ada and Schumann, Candice and Siddhant, Aditya and Shah, Rohin and Youssef, John and Agarwal, Rishabh and Dabney, Natalie and Tonioni, Alessio and Ambar, Moran and Li, Jing and Guyon, Isabelle and Li, Benny and Soergel, David and Fang, Boya and Karadzhov, Georgi and Udrescu, Cristian and Trinh, Trieu and Raunak, Vikas and Noury, Seb and Guo, Dee and Gupta, Sonal and Finkelstein, Mara and Petek, Denis and Liang, Lihao and Billock, Greg and Sun, Pei and Wood, David and Song, Yiwen and Yu, Xiaobin and Matejovicova, Tatiana and Cohen, Regev and Andra, Kalyan and D'Ambrosio, David and Deng, Zhiwei and Nallatamby, Vincent and Songhori, Ebrahim and Dangovski, Rumen and Lampinen, Andrew and Botadra, Pankil and Hillier, Adam and Cao, Jiawei and Baddi, Nagabhushan and Kuncoro, Adhi and Yoshino, Toshihiro and Bhagatwala, Ankit and Ranzato, Marcáurelio and Schaeffer, Rylan and Liu, Tianlin and Ye, Shuai and Sarvana, Obaid and Nham, John and Kuang, Chenkai and Gao, Isabel and Baek, Jinoo and Mittal, Shubham and Wahid, Ayzaan and Gergely, Anita and Ni, Bin and Feldman, Josh and Muir, Carrie and Lamblin, Pascal and Macherey, Wolfgang and Dyer, Ethan and Kilpatrick, Logan and Campos, Víctor and Bhutani, Mukul and Fort, Stanislav and Ahmad, Yanif and Severyn, Aliaksei and Chatziprimou, Kleopatra and Ferludin, Oleksandr and Dimarco, Mason and Kusupati, Aditya and Heyward, Joe and Bahir, Dan and Villela, Kevin and Millican, Katie and Marcus, Dror and Bahargam, Sanaz and Unlu, Caglar and Roth, Nicholas and Wei, Zichuan and Gopal, Siddharth and Ghoshal, Deepanway and Lee, Edward and Lin, Sharon and Lees, Jennie and Lee, Dayeong and Hosseini, Anahita and Fan, Connie and Neel, Seth and Wu, Marcus and Altun, Yasemin and Cai, Honglong and Piqueras, Enrique and Woodward, Josh and Bissacco, Alessandro and Haykal, Salem and Bordbar, Mahyar and Sundaram, Prasha and Hodkinson, Sarah and Toyama, Daniel and Polovets, George and Myers, Austin and Sinha, Anu and Levinboim, Tomer and Krishnakumar, Kashyap and Chhaparia, Rachita and Sholokhova, Tatiana and Gundavarapu, Nitesh Bharadwaj and Jawahar, Ganesh and Qureshi, Haroon and Hu, Jieru and Momchev, Nikola and Rahtz, Matthew and Wu, Renjie and S, Aishwarya P. and Dhamdhere, Kedar and Guo, Meiqi and Gupta, Umang and Eslami, Ali and Schain, Mariano and Blokzijl, Michiel and Welling, David and Orr, Dave and Bolelli, Levent and Perez-Nieves, Nicolas and Sirotenko, Mikhail and Prasad, Aman and Kar, Arjun and Pigem, Borja De Balle and Terzi, Tayfun and Weisz, Gellért and Ghosh, Dipankar and Mavalankar, Aditi and Madeka, Dhruv and Daugaard, Kaspar and Adam, Hartwig and Shah, Viraj and Berman, Dana and Tran, Maggie and Baker, Steven and Andrejczuk, Ewa and Chole, Grishma and Raboshchuk, Ganna and Mirzazadeh, Mahdi and Kagohara, Thais and Wu, Shimu and Schallhart, Christian and Orlando, Bernett and Wang, Chen and Rrustemi, Alban and Xiong, Hao and Liu, Hao and Vezer, Arpi and Ramsden, Nolan and Chang, Shuo-yiin and Mudgal, Sidharth and Li, Yan and Vieillard, Nino and Hoshen, Yedid and Ahmad, Farooq and Slone, Ambrose and Hua, Amy and Potikha, Natan and Rossini, Mirko and Stritar, Jon and Prakash, Sushant and Wang, Zifeng and Dong, Xuanyi and Nazari, Alireza and Nehoran, Efrat and Tekelioglu, Kaan and Li, Yinxiao and Badola, Kartikeya and Funkhouser, Tom and Li, Yuanzhen and Yerram, Varun and Ganeshan, Ramya and Formoso, Daniel and Langner, Karol and Shi, Tian and Li, Huijian and Yamamori, Yumeya and Panda, Amayika and Saade, Alaa and Scarpati, Angelo Scorza and Breaux, Chris and Carey, C. J. and Zhou, Zongwei and Hsieh, Cho-Jui and Bridgers, Sophie and Butryna, Alena and Gupta, Nishesh and Tulsyan, Vaibhav and Woo, Sanghyun and Eltyshev, Evgenii and Grathwohl, Will and Parks, Chanel and Benjamin, Seth and Panigrahy, Rina and Dodhia, Shenil and Freitas, Daniel De and Sauer, Chris and Song, Will and Alet, Ferran and Tolins, Jackson and Paduraru, Cosmin and Zhou, Xingyi and Albert, Brian and Zhang, Zizhao and Shu, Lei and Bansal, Mudit and Nguyen, Sarah and Globerson, Amir and Xiao, Owen and Manyika, James and Hennigan, Tom and Rong, Rong and Matak, Josip and Bakalov, Anton and Sharma, Ankur and Sinopalnikov, Danila and Pierson, Andrew and Roller, Stephen and Brown, Geoff and Gao, Mingcen and Fukuzawa, Toshiyuki and Ghafouri, Amin and Vassigh, Kenny and Barr, Iain and Wang, Zhicheng and Korsun, Anna and Jayaram, Rajesh and Ren, Lijie and Zaman, Tim and Khan, Samira and Lunts, Yana and Deutsch, Dan and Uthus, Dave and Katz, Nitzan and Samsikova, Masha and Khalifa, Amr and Sethi, Nikhil and Sun, Jiao and Tang, Luming and Alon, Uri and Luo, Xianghong and Yu, Dian and Nayyar, Abhishek and Petrini, Bryce and Truong, Will and Hellendoorn, Vincent and Chinaev, Nikolai and Alberti, Chris and Wang, Wei and Hu, Jingcao and Mirrokni, Vahab and Balashankar, Ananth and Aharon, Avia and Mehta, Aahil and Iscen, Ahmet and Kready, Joseph and Manning, Lucas and Mohananey, Anhad and Chen, Yuankai and Tripathi, Anshuman and Wu, Allen and Petrovski, Igor and Hwang, Dawsen and Baeuml, Martin and Chandrakaladharan, Shreyas and Liu, Yuan and Coaguila, Rey and Chen, Maxwell and Ma, Sally and Tafti, Pouya and Tatineni, Susheel and Spitz, Terry and Ye, Jiayu and Vicol, Paul and Rosca, Mihaela and Puigdomènech, Adrià and Yahav, Zohar and Ghemawat, Sanjay and Lin, Hanzhao and Kirk, Phoebe and Nabulsi, Zaid and Brin, Sergey and Bohnet, Bernd and Caluwaerts, Ken and Veerubhotla, Aditya Srikanth and Zheng, Dan and Dai, Zihang and Petrov, Petre and Xu, Yichong and Mehran, Ramin and Xu, Zhuo and Zintgraf, Luisa and Choi, Jiho and Hombaiah, Spurthi Amba and Thoppilan, Romal and Reddi, Sashank and Lew, Lukasz and Li, Li and Webster, Kellie and Sawhney, K. P. and Lamprou, Lampros and Shakeri, Siamak and Lunayach, Mayank and Chen, Jianmin and Bagri, Sumit and Salcianu, Alex and Chen, Ying and Donchev, Yani and Magister, Charlotte and Nørly, Signe and Rodrigues, Vitor and Izo, Tomas and Noga, Hila and Zou, Joe and Köppe, Thomas and Zhou, Wenxuan and Lee, Kenton and Long, Xiangzhu and Eisenbud, Danielle and Chen, Anthony and Schenck, Connor and To, Chi Ming and Zhong, Peilin and Taropa, Emanuel and Truong, Minh and Levy, Omer and Martins, Danilo and Zhang, Zhiyuan and Semturs, Christopher and Zhang, Kelvin and Yakubovich, Alex and Moreno, Pol and McConnaughey, Lara and Lu, Di and Redmond, Sam and Weerts, Lotte and Bitton, Yonatan and Refice, Tiziana and Lacasse, Nicolas and Conmy, Arthur and Tallec, Corentin and Odell, Julian and Forbes-Pollard, Hannah and Socala, Arkadiusz and Hoech, Jonathan and Kohli, Pushmeet and Walton, Alanna and Wang, Rui and Sazanovich, Mikita and Zhu, Kexin and Kapishnikov, Andrei and Galt, Rich and Denton, Matthew and Murdoch, Ben and Sikora, Caitlin and Mohamed, Kareem and Wei, Wei and First, Uri and McConnell, Tim and Cobo, Luis C. and Qin, James and Avrahami, Thi and Balle, Daniel and Watanabe, Yu and Louis, Annie and Kraft, Adam and Ariafar, Setareh and Gu, Yiming and Rives, Eugénie and Yoon, Charles and Rusu, Andrei and Cobon-Kerr, James and Hahn, Chris and Luo, Jiaming and Yuvein and Zhu and Ahuja, Niharika and Benenson, Rodrigo and Kaufman, Raphaël Lopez and Yu, Honglin and Hightower, Lloyd and Zhang, Junlin and Ni, Darren and Hendricks, Lisa Anne and Wang, Gabby and Yona, Gal and Jain, Lalit and Barrio, Pablo and Bhupatiraju, Surya and Velusamy, Siva and Dafoe, Allan and Riedel, Sebastian and Thomas, Tara and Yuan, Zhe and Bellaiche, Mathias and Panthaplackel, Sheena and Kloboves, Klemen and Jauhari, Sarthak and Akbulut, Canfer and Davchev, Todor and Gladchenko, Evgeny and Madras, David and Chuklin, Aleksandr and Hill, Tyrone and Yuan, Quan and Madhavan, Mukundan and Leonhard, Luke and Scandinaro, Dylan and Chen, Qihang and Niu, Ning and Douillard, Arthur and Damoc, Bogdan and Onoe, Yasumasa and Pedregosa, Fabian and Bertsch, Fred and Leichner, Chas and Pagadora, Joseph and Malmaud, Jonathan and Ponda, Sameera and Twigg, Andy and Duzhyi, Oleksii and Shen, Jingwei and Wang, Miaosen and Garg, Roopal and Chen, Jing and Evci, Utku and Lee, Jonathan and Liu, Leon and Kojima, Koji and Yamaguchi, Masa and Rajendran, Arunkumar and Piergiovanni, A. J. and Rajendran, Vinodh Kumar and Fornoni, Marco and Ibagon, Gabriel and Ragan, Harry and Khan, Sadh MNM and Blitzer, John and Bunner, Andrew and Sun, Guan and Kosakai, Takahiro and Lundberg, Scott and Elue, Ndidi and Guu, Kelvin and Park, S. K. and Park, Jane and Narayanaswamy, Arunachalam and Wu, Chengda and Mudigonda, Jayaram and Cohn, Trevor and Mu, Hairong and Kumar, Ravi and Graesser, Laura and Zhang, Yichi and Killam, Richard and Zhuang, Vincent and Giménez, Mai and Jishi, Wael Al and Ley-Wild, Ruy and Zhai, Alex and Osawa, Kazuki and Cedillo, Diego and Liu, Jialu and Upadhyay, Mayank and Sieniek, Marcin and Sharma, Roshan and Paine, Tom and Angelova, Anelia and Addepalli, Sravanti and Parada, Carolina and Majumder, Kingshuk and Lamp, Avery and Kumar, Sanjiv and Deng, Xiang and Myaskovsky, Artiom and Sabolić, Tea and Dudek, Jeffrey and York, Sarah and Quitry, Félix de Chaumont and Nie, Jiazhong and Cattle, Dee and Gunjan, Alok and Piot, Bilal and Khawaja, Waleed and Bang, Seojin and Wang, Simon and Khodadadeh, Siavash and R, Raghavender and Rawlani, Praynaa and Powell, Richard and Lee, Kevin and Griesser, Johannes and Oh, G. S. and Magalhaes, Cesar and Li, Yujia and Tokumine, Simon and Vogel, Hadas Natalie and Hsu, Dennis and BC, Arturo and Jindal, Disha and Cohen, Matan and Yang, Zi and Yuan, Junwei and Cesare, Dario de and Bruguier, Tony and Xu, Jun and Roy, Monica and Jacovi, Alon and Belov, Dan and Arya, Rahul and Meadowlark, Phoenix and Cohen-Ganor, Shlomi and Ye, Wenting and Morris-Suzuki, Patrick and Banzal, Praseem and Song, Gan and Ponnuramu, Pranavaraj and Zhang, Fred and Scrivener, George and Zaiem, Salah and Rochman, Alif Raditya and Han, Kehang and Ghazi, Badih and Lee, Kate and Drath, Shahar and Suo, Daniel and Girgis, Antonious and Shenoy, Pradeep and Nguyen, Duy and Eck, Douglas and Gupta, Somit and Yan, Le and Carreira, Joao and Gulati, Anmol and Sang, Ruoxin and Mirylenka, Daniil and Cooney, Emma and Chou, Edward and Ling, Mingyang and Fan, Cindy and Coleman, Ben and Tubone, Guilherme and Kumar, Ravin and Baldridge, Jason and Hernandez-Campos, Felix and Lazaridou, Angeliki and Besley, James and Yona, Itay and Bulut, Neslihan and Wellens, Quentin and Pierigiovanni, A. J. and George, Jasmine and Green, Richard and Han, Pu and Tao, Connie and Clark, Geoff and You, Chong and Abdolmaleki, Abbas and Fu, Justin and Chen, Tongzhou and Chaugule, Ashwin and Chandorkar, Angad and Rahman, Altaf and Thompson, Will and Koanantakool, Penporn and Bernico, Mike and Ren, Jie and Vlasov, Andrey and Vassilvitskii, Sergei and Kula, Maciej and Liang, Yizhong and Kim, Dahun and Huang, Yangsibo and Ye, Chengxi and Lepikhin, Dmitry and Helmholz, Wesley},
	month = oct,
	year = {2025},
	note = {arXiv:2507.06261 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	doi = {10.48550/arXiv.2403.05530},
	abstract = {In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textgreater}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and Mariooryad, Soroosh and Ding, Yifan and Geng, Xinyang and Alcober, Fred and Frostig, Roy and Omernick, Mark and Walker, Lexi and Paduraru, Cosmin and Sorokin, Christina and Tacchetti, Andrea and Gaffney, Colin and Daruki, Samira and Sercinoglu, Olcan and Gleicher, Zach and Love, Juliette and Voigtlaender, Paul and Jain, Rohan and Surita, Gabriela and Mohamed, Kareem and Blevins, Rory and Ahn, Junwhan and Zhu, Tao and Kawintiranon, Kornraphop and Firat, Orhan and Gu, Yiming and Zhang, Yujing and Rahtz, Matthew and Faruqui, Manaal and Clay, Natalie and Gilmer, Justin and Co-Reyes, J. D. and Penchev, Ivo and Zhu, Rui and Morioka, Nobuyuki and Hui, Kevin and Haridasan, Krishna and Campos, Victor and Mahdieh, Mahdis and Guo, Mandy and Hassan, Samer and Kilgour, Kevin and Vezer, Arpi and Cheng, Heng-Tze and Liedekerke, Raoul de and Goyal, Siddharth and Barham, Paul and Strouse, D. J. and Noury, Seb and Adler, Jonas and Sundararajan, Mukund and Vikram, Sharad and Lepikhin, Dmitry and Paganini, Michela and Garcia, Xavier and Yang, Fan and Valter, Dasha and Trebacz, Maja and Vodrahalli, Kiran and Asawaroengchai, Chulayuth and Ring, Roman and Kalb, Norbert and Soares, Livio Baldini and Brahma, Siddhartha and Steiner, David and Yu, Tianhe and Mentzer, Fabian and He, Antoine and Gonzalez, Lucas and Xu, Bibo and Kaufman, Raphael Lopez and Shafey, Laurent El and Oh, Junhyuk and Hennigan, Tom and Driessche, George van den and Odoom, Seth and Lucic, Mario and Roelofs, Becca and Lall, Sid and Marathe, Amit and Chan, Betty and Ontanon, Santiago and He, Luheng and Teplyashin, Denis and Lai, Jonathan and Crone, Phil and Damoc, Bogdan and Ho, Lewis and Riedel, Sebastian and Lenc, Karel and Yeh, Chih-Kuan and Chowdhery, Aakanksha and Xu, Yang and Kazemi, Mehran and Amid, Ehsan and Petrushkina, Anastasia and Swersky, Kevin and Khodaei, Ali and Chen, Gowoon and Larkin, Chris and Pinto, Mario and Yan, Geng and Badia, Adria Puigdomenech and Patil, Piyush and Hansen, Steven and Orr, Dave and Arnold, Sebastien M. R. and Grimstad, Jordan and Dai, Andrew and Douglas, Sholto and Sinha, Rishika and Yadav, Vikas and Chen, Xi and Gribovskaya, Elena and Austin, Jacob and Zhao, Jeffrey and Patel, Kaushal and Komarek, Paul and Austin, Sophia and Borgeaud, Sebastian and Friso, Linda and Goyal, Abhimanyu and Caine, Ben and Cao, Kris and Chung, Da-Woon and Lamm, Matthew and Barth-Maron, Gabe and Kagohara, Thais and Olszewska, Kate and Chen, Mia and Shivakumar, Kaushik and Agarwal, Rishabh and Godhia, Harshal and Rajwar, Ravi and Snaider, Javier and Dotiwalla, Xerxes and Liu, Yuan and Barua, Aditya and Ungureanu, Victor and Zhang, Yuan and Batsaikhan, Bat-Orgil and Wirth, Mateo and Qin, James and Danihelka, Ivo and Doshi, Tulsee and Chadwick, Martin and Chen, Jilin and Jain, Sanil and Le, Quoc and Kar, Arjun and Gurumurthy, Madhu and Li, Cheng and Sang, Ruoxin and Liu, Fangyu and Lamprou, Lampros and Munoz, Rich and Lintz, Nathan and Mehta, Harsh and Howard, Heidi and Reynolds, Malcolm and Aroyo, Lora and Wang, Quan and Blanco, Lorenzo and Cassirer, Albin and Griffith, Jordan and Das, Dipanjan and Lee, Stephan and Sygnowski, Jakub and Fisher, Zach and Besley, James and Powell, Richard and Ahmed, Zafarali and Paulus, Dominik and Reitter, David and Borsos, Zalan and Joshi, Rishabh and Pope, Aedan and Hand, Steven and Selo, Vittorio and Jain, Vihan and Sethi, Nikhil and Goel, Megha and Makino, Takaki and May, Rhys and Yang, Zhen and Schalkwyk, Johan and Butterfield, Christina and Hauth, Anja and Goldin, Alex and Hawkins, Will and Senter, Evan and Brin, Sergey and Woodman, Oliver and Ritter, Marvin and Noland, Eric and Giang, Minh and Bolina, Vijay and Lee, Lisa and Blyth, Tim and Mackinnon, Ian and Reid, Machel and Sarvana, Obaid and Silver, David and Chen, Alexander and Wang, Lily and Maggiore, Loren and Chang, Oscar and Attaluri, Nithya and Thornton, Gregory and Chiu, Chung-Cheng and Bunyan, Oskar and Levine, Nir and Chung, Timothy and Eltyshev, Evgenii and Si, Xiance and Lillicrap, Timothy and Brady, Demetra and Aggarwal, Vaibhav and Wu, Boxi and Xu, Yuanzhong and McIlroy, Ross and Badola, Kartikeya and Sandhu, Paramjit and Moreira, Erica and Stokowiec, Wojciech and Hemsley, Ross and Li, Dong and Tudor, Alex and Shyam, Pranav and Rahimtoroghi, Elahe and Haykal, Salem and Sprechmann, Pablo and Zhou, Xiang and Mincu, Diana and Li, Yujia and Addanki, Ravi and Krishna, Kalpesh and Wu, Xiao and Frechette, Alexandre and Eyal, Matan and Dafoe, Allan and Lacey, Dave and Whang, Jay and Avrahami, Thi and Zhang, Ye and Taropa, Emanuel and Lin, Hanzhao and Toyama, Daniel and Rutherford, Eliza and Sano, Motoki and Choe, HyunJeong and Tomala, Alex and Safranek-Shrader, Chalence and Kassner, Nora and Pajarskas, Mantas and Harvey, Matt and Sechrist, Sean and Fortunato, Meire and Lyu, Christina and Elsayed, Gamaleldin and Kuang, Chenkai and Lottes, James and Chu, Eric and Jia, Chao and Chen, Chih-Wei and Humphreys, Peter and Baumli, Kate and Tao, Connie and Samuel, Rajkumar and Santos, Cicero Nogueira dos and Andreassen, Anders and Rakićević, Nemanja and Grewe, Dominik and Kumar, Aviral and Winkler, Stephanie and Caton, Jonathan and Brock, Andrew and Dalmia, Sid and Sheahan, Hannah and Barr, Iain and Miao, Yingjie and Natsev, Paul and Devlin, Jacob and Behbahani, Feryal and Prost, Flavien and Sun, Yanhua and Myaskovsky, Artiom and Pillai, Thanumalayan Sankaranarayana and Hurt, Dan and Lazaridou, Angeliki and Xiong, Xi and Zheng, Ce and Pardo, Fabio and Li, Xiaowei and Horgan, Dan and Stanton, Joe and Ambar, Moran and Xia, Fei and Lince, Alejandro and Wang, Mingqiu and Mustafa, Basil and Webson, Albert and Lee, Hyo and Anil, Rohan and Wicke, Martin and Dozat, Timothy and Sinha, Abhishek and Piqueras, Enrique and Dabir, Elahe and Upadhyay, Shyam and Boral, Anudhyan and Hendricks, Lisa Anne and Fry, Corey and Djolonga, Josip and Su, Yi and Walker, Jake and Labanowski, Jane and Huang, Ronny and Misra, Vedant and Chen, Jeremy and Skerry-Ryan, R. J. and Singh, Avi and Rijhwani, Shruti and Yu, Dian and Castro-Ros, Alex and Changpinyo, Beer and Datta, Romina and Bagri, Sumit and Hrafnkelsson, Arnar Mar and Maggioni, Marcello and Zheng, Daniel and Sulsky, Yury and Hou, Shaobo and Paine, Tom Le and Yang, Antoine and Riesa, Jason and Rogozinska, Dominika and Marcus, Dror and Badawy, Dalia El and Zhang, Qiao and Wang, Luyu and Miller, Helen and Greer, Jeremy and Sjos, Lars Lowe and Nova, Azade and Zen, Heiga and Chaabouni, Rahma and Rosca, Mihaela and Jiang, Jiepu and Chen, Charlie and Liu, Ruibo and Sainath, Tara and Krikun, Maxim and Polozov, Alex and Lespiau, Jean-Baptiste and Newlan, Josh and Cankara, Zeyncep and Kwak, Soo and Xu, Yunhan and Chen, Phil and Coenen, Andy and Meyer, Clemens and Tsihlas, Katerina and Ma, Ada and Gottweis, Juraj and Xing, Jinwei and Gu, Chenjie and Miao, Jin and Frank, Christian and Cankara, Zeynep and Ganapathy, Sanjay and Dasgupta, Ishita and Hughes-Fitt, Steph and Chen, Heng and Reid, David and Rong, Keran and Fan, Hongmin and Amersfoort, Joost van and Zhuang, Vincent and Cohen, Aaron and Gu, Shixiang Shane and Mohananey, Anhad and Ilic, Anastasija and Tobin, Taylor and Wieting, John and Bortsova, Anna and Thacker, Phoebe and Wang, Emma and Caveness, Emily and Chiu, Justin and Sezener, Eren and Kaskasoli, Alex and Baker, Steven and Millican, Katie and Elhawaty, Mohamed and Aisopos, Kostas and Lebsack, Carl and Byrd, Nathan and Dai, Hanjun and Jia, Wenhao and Wiethoff, Matthew and Davoodi, Elnaz and Weston, Albert and Yagati, Lakshman and Ahuja, Arun and Gao, Isabel and Pundak, Golan and Zhang, Susan and Azzam, Michael and Sim, Khe Chai and Caelles, Sergi and Keeling, James and Sharma, Abhanshu and Swing, Andy and Li, YaGuang and Liu, Chenxi and Bostock, Carrie Grimes and Bansal, Yamini and Nado, Zachary and Anand, Ankesh and Lipschultz, Josh and Karmarkar, Abhijit and Proleev, Lev and Ittycheriah, Abe and Yeganeh, Soheil Hassas and Polovets, George and Faust, Aleksandra and Sun, Jiao and Rrustemi, Alban and Li, Pen and Shivanna, Rakesh and Liu, Jeremiah and Welty, Chris and Lebron, Federico and Baddepudi, Anirudh and Krause, Sebastian and Parisotto, Emilio and Soricut, Radu and Xu, Zheng and Bloxwich, Dawn and Johnson, Melvin and Neyshabur, Behnam and Mao-Jones, Justin and Wang, Renshen and Ramasesh, Vinay and Abbas, Zaheer and Guez, Arthur and Segal, Constant and Nguyen, Duc Dung and Svensson, James and Hou, Le and York, Sarah and Milan, Kieran and Bridgers, Sophie and Gworek, Wiktor and Tagliasacchi, Marco and Lee-Thorp, James and Chang, Michael and Guseynov, Alexey and Hartman, Ale Jakse and Kwong, Michael and Zhao, Ruizhe and Kashem, Sheleem and Cole, Elizabeth and Miech, Antoine and Tanburn, Richard and Phuong, Mary and Pavetic, Filip and Cevey, Sebastien and Comanescu, Ramona and Ives, Richard and Yang, Sherry and Du, Cosmo and Li, Bo and Zhang, Zizhao and Iinuma, Mariko and Hu, Clara Huiyi and Roy, Aurko and Bijwadia, Shaan and Zhu, Zhenkai and Martins, Danilo and Saputro, Rachel and Gergely, Anita and Zheng, Steven and Jia, Dawei and Antonoglou, Ioannis and Sadovsky, Adam and Gu, Shane and Bi, Yingying and Andreev, Alek and Samangooei, Sina and Khan, Mina and Kocisky, Tomas and Filos, Angelos and Kumar, Chintu and Bishop, Colton and Yu, Adams and Hodkinson, Sarah and Mittal, Sid and Shah, Premal and Moufarek, Alexandre and Cheng, Yong and Bloniarz, Adam and Lee, Jaehoon and Pejman, Pedram and Michel, Paul and Spencer, Stephen and Feinberg, Vladimir and Xiong, Xuehan and Savinov, Nikolay and Smith, Charlotte and Shakeri, Siamak and Tran, Dustin and Chesus, Mary and Bohnet, Bernd and Tucker, George and Glehn, Tamara von and Muir, Carrie and Mao, Yiran and Kazawa, Hideto and Slone, Ambrose and Soparkar, Kedar and Shrivastava, Disha and Cobon-Kerr, James and Sharman, Michael and Pavagadhi, Jay and Araya, Carlos and Misiunas, Karolis and Ghelani, Nimesh and Laskin, Michael and Barker, David and Li, Qiujia and Briukhov, Anton and Houlsby, Neil and Glaese, Mia and Lakshminarayanan, Balaji and Schucher, Nathan and Tang, Yunhao and Collins, Eli and Lim, Hyeontaek and Feng, Fangxiaoyu and Recasens, Adria and Lai, Guangda and Magni, Alberto and Cao, Nicola De and Siddhant, Aditya and Ashwood, Zoe and Orbay, Jordi and Dehghani, Mostafa and Brennan, Jenny and He, Yifan and Xu, Kelvin and Gao, Yang and Saroufim, Carl and Molloy, James and Wu, Xinyi and Arnold, Seb and Chang, Solomon and Schrittwieser, Julian and Buchatskaya, Elena and Radpour, Soroush and Polacek, Martin and Giordano, Skye and Bapna, Ankur and Tokumine, Simon and Hellendoorn, Vincent and Sottiaux, Thibault and Cogan, Sarah and Severyn, Aliaksei and Saleh, Mohammad and Thakoor, Shantanu and Shefey, Laurent and Qiao, Siyuan and Gaba, Meenu and Chang, Shuo-yiin and Swanson, Craig and Zhang, Biao and Lee, Benjamin and Rubenstein, Paul Kishan and Song, Gan and Kwiatkowski, Tom and Koop, Anna and Kannan, Ajay and Kao, David and Schuh, Parker and Stjerngren, Axel and Ghiasi, Golnaz and Gibson, Gena and Vilnis, Luke and Yuan, Ye and Ferreira, Felipe Tiengo and Kamath, Aishwarya and Klimenko, Ted and Franko, Ken and Xiao, Kefan and Bhattacharya, Indro and Patel, Miteyan and Wang, Rui and Morris, Alex and Strudel, Robin and Sharma, Vivek and Choy, Peter and Hashemi, Sayed Hadi and Landon, Jessica and Finkelstein, Mara and Jhakra, Priya and Frye, Justin and Barnes, Megan and Mauger, Matthew and Daun, Dennis and Baatarsukh, Khuslen and Tung, Matthew and Farhan, Wael and Michalewski, Henryk and Viola, Fabio and Quitry, Felix de Chaumont and Lan, Charline Le and Hudson, Tom and Wang, Qingze and Fischer, Felix and Zheng, Ivy and White, Elspeth and Dragan, Anca and Alayrac, Jean-baptiste and Ni, Eric and Pritzel, Alexander and Iwanicki, Adam and Isard, Michael and Bulanova, Anna and Zilka, Lukas and Dyer, Ethan and Sachan, Devendra and Srinivasan, Srivatsan and Muckenhirn, Hannah and Cai, Honglong and Mandhane, Amol and Tariq, Mukarram and Rae, Jack W. and Wang, Gary and Ayoub, Kareem and FitzGerald, Nicholas and Zhao, Yao and Han, Woohyun and Alberti, Chris and Garrette, Dan and Krishnakumar, Kashyap and Gimenez, Mai and Levskaya, Anselm and Sohn, Daniel and Matak, Josip and Iturrate, Inaki and Chang, Michael B. and Xiang, Jackie and Cao, Yuan and Ranka, Nishant and Brown, Geoff and Hutter, Adrian and Mirrokni, Vahab and Chen, Nanxin and Yao, Kaisheng and Egyed, Zoltan and Galilee, Francois and Liechty, Tyler and Kallakuri, Praveen and Palmer, Evan and Ghemawat, Sanjay and Liu, Jasmine and Tao, David and Thornton, Chloe and Green, Tim and Jasarevic, Mimi and Lin, Sharon and Cotruta, Victor and Tan, Yi-Xuan and Fiedel, Noah and Yu, Hongkun and Chi, Ed and Neitz, Alexander and Heitkaemper, Jens and Sinha, Anu and Zhou, Denny and Sun, Yi and Kaed, Charbel and Hulse, Brice and Mishra, Swaroop and Georgaki, Maria and Kudugunta, Sneha and Farabet, Clement and Shafran, Izhak and Vlasic, Daniel and Tsitsulin, Anton and Ananthanarayanan, Rajagopal and Carin, Alen and Su, Guolong and Sun, Pei and V, Shashank and Carvajal, Gabriel and Broder, Josef and Comsa, Iulia and Repina, Alena and Wong, William and Chen, Warren Weilun and Hawkins, Peter and Filonov, Egor and Loher, Lucia and Hirnschall, Christoph and Wang, Weiyi and Ye, Jingchen and Burns, Andrea and Cate, Hardie and Wright, Diana Gage and Piccinini, Federico and Zhang, Lei and Lin, Chu-Cheng and Gog, Ionel and Kulizhskaya, Yana and Sreevatsa, Ashwin and Song, Shuang and Cobo, Luis C. and Iyer, Anand and Tekur, Chetan and Garrido, Guillermo and Xiao, Zhuyun and Kemp, Rupert and Zheng, Huaixiu Steven and Li, Hui and Agarwal, Ananth and Ngani, Christel and Goshvadi, Kati and Santamaria-Fernandez, Rebeca and Fica, Wojciech and Chen, Xinyun and Gorgolewski, Chris and Sun, Sean and Garg, Roopal and Ye, Xinyu and Eslami, S. M. Ali and Hua, Nan and Simon, Jon and Joshi, Pratik and Kim, Yelin and Tenney, Ian and Potluri, Sahitya and Thiet, Lam Nguyen and Yuan, Quan and Luisier, Florian and Chronopoulou, Alexandra and Scellato, Salvatore and Srinivasan, Praveen and Chen, Minmin and Koverkathu, Vinod and Dalibard, Valentin and Xu, Yaming and Saeta, Brennan and Anderson, Keith and Sellam, Thibault and Fernando, Nick and Huot, Fantine and Jung, Junehyuk and Varadarajan, Mani and Quinn, Michael and Raul, Amit and Le, Maigo and Habalov, Ruslan and Clark, Jon and Jalan, Komal and Bullard, Kalesha and Singhal, Achintya and Luong, Thang and Wang, Boyu and Rajayogam, Sujeevan and Eisenschlos, Julian and Jia, Johnson and Finchelstein, Daniel and Yakubovich, Alex and Balle, Daniel and Fink, Michael and Agarwal, Sameer and Li, Jing and Dvijotham, Dj and Pal, Shalini and Kang, Kai and Konzelmann, Jaclyn and Beattie, Jennifer and Dousse, Olivier and Wu, Diane and Crocker, Remi and Elkind, Chen and Jonnalagadda, Siddhartha Reddy and Lee, Jong and Holtmann-Rice, Dan and Kallarackal, Krystal and Liu, Rosanne and Vnukov, Denis and Vats, Neera and Invernizzi, Luca and Jafari, Mohsen and Zhou, Huanjie and Taylor, Lilly and Prendki, Jennifer and Wu, Marcus and Eccles, Tom and Liu, Tianqi and Kopparapu, Kavya and Beaufays, Francoise and Angermueller, Christof and Marzoca, Andreea and Sarcar, Shourya and Dib, Hilal and Stanway, Jeff and Perbet, Frank and Trdin, Nejc and Sterneck, Rachel and Khorlin, Andrey and Li, Dinghua and Wu, Xihui and Goenka, Sonam and Madras, David and Goldshtein, Sasha and Gierke, Willi and Zhou, Tong and Liu, Yaxin and Liang, Yannie and White, Anais and Li, Yunjie and Singh, Shreya and Bahargam, Sanaz and Epstein, Mark and Basu, Sujoy and Lao, Li and Ozturel, Adnan and Crous, Carl and Zhai, Alex and Lu, Han and Tung, Zora and Gaur, Neeraj and Walton, Alanna and Dixon, Lucas and Zhang, Ming and Globerson, Amir and Uy, Grant and Bolt, Andrew and Wiles, Olivia and Nasr, Milad and Shumailov, Ilia and Selvi, Marco and Piccinno, Francesco and Aguilar, Ricardo and McCarthy, Sara and Khalman, Misha and Shukla, Mrinal and Galic, Vlado and Carpenter, John and Villela, Kevin and Zhang, Haibin and Richardson, Harry and Martens, James and Bosnjak, Matko and Belle, Shreyas Rammohan and Seibert, Jeff and Alnahlawi, Mahmoud and McWilliams, Brian and Singh, Sankalp and Louis, Annie and Ding, Wen and Popovici, Dan and Simicich, Lenin and Knight, Laura and Mehta, Pulkit and Gupta, Nishesh and Shi, Chongyang and Fatehi, Saaber and Mitrovic, Jovana and Grills, Alex and Pagadora, Joseph and Munkhdalai, Tsendsuren and Petrova, Dessie and Eisenbud, Danielle and Zhang, Zhishuai and Yates, Damion and Mittal, Bhavishya and Tripuraneni, Nilesh and Assael, Yannis and Brovelli, Thomas and Jain, Prateek and Velimirovic, Mihajlo and Akbulut, Canfer and Mu, Jiaqi and Macherey, Wolfgang and Kumar, Ravin and Xu, Jun and Qureshi, Haroon and Comanici, Gheorghe and Wiesner, Jeremy and Gong, Zhitao and Ruddock, Anton and Bauer, Matthias and Felt, Nick and GP, Anirudh and Arnab, Anurag and Zelle, Dustin and Rothfuss, Jonas and Rosgen, Bill and Shenoy, Ashish and Seybold, Bryan and Li, Xinjian and Mudigonda, Jayaram and Erdogan, Goker and Xia, Jiawei and Simsa, Jiri and Michi, Andrea and Yao, Yi and Yew, Christopher and Kan, Steven and Caswell, Isaac and Radebaugh, Carey and Elisseeff, Andre and Valenzuela, Pedro and McKinney, Kay and Paterson, Kim and Cui, Albert and Latorre-Chimoto, Eri and Kim, Solomon and Zeng, William and Durden, Ken and Ponnapalli, Priya and Sosea, Tiberiu and Choquette-Choo, Christopher A. and Manyika, James and Robenek, Brona and Vashisht, Harsha and Pereira, Sebastien and Lam, Hoi and Velic, Marko and Owusu-Afriyie, Denese and Lee, Katherine and Bolukbasi, Tolga and Parrish, Alicia and Lu, Shawn and Park, Jane and Venkatraman, Balaji and Talbert, Alice and Rosique, Lambert and Cheng, Yuchung and Sozanschi, Andrei and Paszke, Adam and Kumar, Praveen and Austin, Jessica and Li, Lu and Salama, Khalid and Perz, Bartek and Kim, Wooyeol and Dukkipati, Nandita and Baryshnikov, Anthony and Kaplanis, Christos and Sheng, XiangHai and Chervonyi, Yuri and Unlu, Caglar and Casas, Diego de Las and Askham, Harry and Tunyasuvunakool, Kathryn and Gimeno, Felix and Poder, Siim and Kwak, Chester and Miecnikowski, Matt and Mirrokni, Vahab and Dimitriev, Alek and Parisi, Aaron and Liu, Dangyi and Tsai, Tomy and Shevlane, Toby and Kouridi, Christina and Garmon, Drew and Goedeckemeyer, Adrian and Brown, Adam R. and Vijayakumar, Anitha and Elqursh, Ali and Jazayeri, Sadegh and Huang, Jin and Carthy, Sara Mc and Hoover, Jay and Kim, Lucy and Kumar, Sandeep and Chen, Wei and Biles, Courtney and Bingham, Garrett and Rosen, Evan and Wang, Lisa and Tan, Qijun and Engel, David and Pongetti, Francesco and Cesare, Dario de and Hwang, Dongseong and Yu, Lily and Pullman, Jennifer and Narayanan, Srini and Levin, Kyle and Gopal, Siddharth and Li, Megan and Aharoni, Asaf and Trinh, Trieu and Lo, Jessica and Casagrande, Norman and Vij, Roopali and Matthey, Loic and Ramadhana, Bramandia and Matthews, Austin and Carey, C. J. and Johnson, Matthew and Goranova, Kremena and Shah, Rohin and Ashraf, Shereen and Dasgupta, Kingshuk and Larsen, Rasmus and Wang, Yicheng and Vuyyuru, Manish Reddy and Jiang, Chong and Ijazi, Joana and Osawa, Kazuki and Smith, Celine and Boppana, Ramya Sree and Bilal, Taylan and Koizumi, Yuma and Xu, Ying and Altun, Yasemin and Shabat, Nir and Bariach, Ben and Korchemniy, Alex and Choo, Kiam and Ronneberger, Olaf and Iwuanyanwu, Chimezie and Zhao, Shubin and Soergel, David and Hsieh, Cho-Jui and Cai, Irene and Iqbal, Shariq and Sundermeyer, Martin and Chen, Zhe and Bursztein, Elie and Malaviya, Chaitanya and Biadsy, Fadi and Shroff, Prakash and Dhillon, Inderjit and Latkar, Tejasi and Dyer, Chris and Forbes, Hannah and Nicosia, Massimo and Nikolaev, Vitaly and Greene, Somer and Georgiev, Marin and Wang, Pidong and Martin, Nina and Sedghi, Hanie and Zhang, John and Banzal, Praseem and Fritz, Doug and Rao, Vikram and Wang, Xuezhi and Zhang, Jiageng and Patraucean, Viorica and Du, Dayou and Mordatch, Igor and Jurin, Ivan and Liu, Lewis and Dubey, Ayush and Mohan, Abhi and Nowakowski, Janek and Ion, Vlad-Doru and Wei, Nan and Tojo, Reiko and Raad, Maria Abi and Hudson, Drew A. and Keshava, Vaishakh and Agrawal, Shubham and Ramirez, Kevin and Wu, Zhichun and Nguyen, Hoang and Liu, Ji and Sewak, Madhavi and Petrini, Bryce and Choi, DongHyun and Philips, Ivan and Wang, Ziyue and Bica, Ioana and Garg, Ankush and Wilkiewicz, Jarek and Agrawal, Priyanka and Li, Xiaowei and Guo, Danhao and Xue, Emily and Shaik, Naseer and Leach, Andrew and Khan, Sadh MNM and Wiesinger, Julia and Jerome, Sammy and Chakladar, Abhishek and Wang, Alek Wenjiao and Ornduff, Tina and Abu, Folake and Ghaffarkhah, Alireza and Wainwright, Marcus and Cortes, Mario and Liu, Frederick and Maynez, Joshua and Terzis, Andreas and Samangouei, Pouya and Mansour, Riham and Kępa, Tomasz and Aubet, François-Xavier and Algymr, Anton and Banica, Dan and Weisz, Agoston and Orban, Andras and Senges, Alexandre and Andrejczuk, Ewa and Geller, Mark and Santo, Niccolo Dal and Anklin, Valentin and Merey, Majd Al and Baeuml, Martin and Strohman, Trevor and Bai, Junwen and Petrov, Slav and Wu, Yonghui and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeff and Vinyals, Oriol},
	month = dec,
	year = {2024},
	note = {arXiv:2403.05530 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o {System} {Card}},
	url = {http://arxiv.org/abs/2410.21276},
	doi = {10.48550/arXiv.2410.21276},
	abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {OpenAI and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and Mądry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and Lohmann, Fred von and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and Gu-Lemberg, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and Castro, Miguel Oom Temudo de and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and Campbell-Moore, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and Shuaiqi and Xia and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wei_chatie_2024,
	title = {{ChatIE}: {Zero}-{Shot} {Information} {Extraction} via {Chatting} with {ChatGPT}},
	shorttitle = {{ChatIE}},
	url = {http://arxiv.org/abs/2302.10205},
	doi = {10.48550/arXiv.2302.10205},
	abstract = {Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
	month = may,
	year = {2024},
	note = {arXiv:2302.10205 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_extending_2023,
	title = {Extending {Context} {Window} of {Large} {Language} {Models} via {Positional} {Interpolation}},
	url = {http://arxiv.org/abs/2306.15595},
	doi = {10.48550/arXiv.2306.15595},
	abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
}
