
@misc{li_chain_2024,
	title = {Chain of {Code}: {Reasoning} with a {Language} {Model}-{Augmented} {Code} {Emulator}},
	shorttitle = {Chain of {Code}},
	url = {http://arxiv.org/abs/2312.04474},
	doi = {10.48550/arXiv.2312.04474},
	abstract = {Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter - we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect\_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect\_sarcasm(string)". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84\%, a gain of 12\% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by "thinking in code".},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and Fei-Fei, Li and Xia, Fei and Ichter, Brian},
	month = jul,
	year = {2024},
	note = {arXiv:2312.04474 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liang_code_2023,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {http://arxiv.org/abs/2209.07753},
	doi = {10.48550/arXiv.2209.07753},
	abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = may,
	year = {2023},
	note = {arXiv:2209.07753 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{joel_survey_2025,
	title = {A {Survey} on {LLM}-based {Code} {Generation} for {Low}-{Resource} and {Domain}-{Specific} {Programming} {Languages}},
	url = {http://arxiv.org/abs/2410.03981},
	doi = {10.48550/arXiv.2410.03981},
	abstract = {Large Language Models (LLMs) have shown impressive capabilities in code generation for popular programming languages. However, their performance on Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a significant challenge, affecting millions of developers-3.5 million users in Rust alone-who cannot fully utilize LLM capabilities. LRPLs and DSLs encounter unique obstacles, including data scarcity and, for DSLs, specialized syntax that is poorly represented in general-purpose datasets. Addressing these challenges is crucial, as LRPLs and DSLs enhance development efficiency in specialized domains, such as finance and science. While several surveys discuss LLMs in software engineering, none focus specifically on the challenges and opportunities associated with LRPLs and DSLs. Our survey fills this gap by systematically reviewing the current state, methodologies, and challenges in leveraging LLMs for code generation in these languages. We filtered 111 papers from over 27,000 published studies between 2020 and 2024 to evaluate the capabilities and limitations of LLMs in LRPLs and DSLs. We report the LLMs used, benchmarks, and metrics for evaluation, strategies for enhancing performance, and methods for dataset collection and curation. We identified four main evaluation techniques and several metrics for assessing code generation in LRPLs and DSLs. Our analysis categorizes improvement methods into six groups and summarizes novel architectures proposed by researchers. Despite various techniques and metrics, a standard approach and benchmark dataset for evaluating code generation in LRPLs and DSLs are lacking. This survey serves as a resource for researchers and practitioners at the intersection of LLMs, software engineering, and specialized programming languages, laying the groundwork for future advancements in code generation for LRPLs and DSLs.},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Joel, Sathvik and Wu, Jie JW and Fard, Fatemeh H.},
	month = sep,
	year = {2025},
	note = {arXiv:2410.03981 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{chen_program_2023,
	title = {Program of {Thoughts} {Prompting}: {Disentangling} {Computation} from {Reasoning} for {Numerical} {Reasoning} {Tasks}},
	shorttitle = {Program of {Thoughts} {Prompting}},
	url = {http://arxiv.org/abs/2211.12588},
	doi = {10.48550/arXiv.2211.12588},
	abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12{\textbackslash}\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts},
	urldate = {2025-11-23},
	publisher = {arXiv},
	author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
	month = oct,
	year = {2023},
	note = {arXiv:2211.12588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{enevoldsen_mmteb_2025,
	title = {{MMTEB}: {Massive} {Multilingual} {Text} {Embedding} {Benchmark}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{MMTEB}},
	url = {https://arxiv.org/abs/2502.13595},
	doi = {10.48550/ARXIV.2502.13595},
	abstract = {Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Enevoldsen, Kenneth and Chung, Isaac and Kerboua, Imene and Kardos, Márton and Mathur, Ashwin and Stap, David and Gala, Jay and Siblini, Wissam and Krzemiński, Dominik and Winata, Genta Indra and Sturua, Saba and Utpala, Saiteja and Ciancone, Mathieu and Schaeffer, Marion and Sequeira, Gabriel and Misra, Diganta and Dhakal, Shreeya and Rystrøm, Jonathan and Solomatin, Roman and Çağatan, Ömer and Kundu, Akash and Bernstorff, Martin and Xiao, Shitao and Sukhlecha, Akshita and Pahwa, Bhavish and Poświata, Rafał and GV, Kranthi Kiran and Ashraf, Shawon and Auras, Daniel and Plüster, Björn and Harries, Jan Philipp and Magne, Loïc and Mohr, Isabelle and Hendriksen, Mariya and Zhu, Dawei and Gisserot-Boukhlef, Hippolyte and Aarsen, Tom and Kostkan, Jan and Wojtasik, Konrad and Lee, Taemin and Šuppa, Marek and Zhang, Crystina and Rocca, Roberta and Hamdy, Mohammed and Michail, Andrianos and Yang, John and Faysse, Manuel and Vatolin, Aleksei and Thakur, Nandan and Dey, Manan and Vasani, Dipam and Chitale, Pranjal and Tedeschi, Simone and Tai, Nguyen and Snegirev, Artem and Günther, Michael and Xia, Mengzhou and Shi, Weijia and Lù, Xing Han and Clive, Jordan and Krishnakumar, Gayatri and Maksimova, Anna and Wehrli, Silvan and Tikhonova, Maria and Panchal, Henil and Abramov, Aleksandr and Ostendorff, Malte and Liu, Zheng and Clematide, Simon and Miranda, Lester James and Fenogenova, Alena and Song, Guangyu and Safi, Ruqiya Bin and Li, Wen-Ding and Borghini, Alessia and Cassano, Federico and Su, Hongjin and Lin, Jimmy and Yen, Howard and Hansen, Lasse and Hooker, Sara and Xiao, Chenghao and Adlakha, Vaibhav and Weller, Orion and Reddy, Siva and Muennighoff, Niklas},
	year = {2025},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Information Retrieval (cs.IR)},
}

@misc{enevoldsen_mmteb_2025-1,
	title = {{MMTEB}: {Massive} {Multilingual} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MMTEB}},
	url = {http://arxiv.org/abs/2502.13595},
	doi = {10.48550/arXiv.2502.13595},
	abstract = {Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Enevoldsen, Kenneth and Chung, Isaac and Kerboua, Imene and Kardos, Márton and Mathur, Ashwin and Stap, David and Gala, Jay and Siblini, Wissam and Krzemiński, Dominik and Winata, Genta Indra and Sturua, Saba and Utpala, Saiteja and Ciancone, Mathieu and Schaeffer, Marion and Sequeira, Gabriel and Misra, Diganta and Dhakal, Shreeya and Rystrøm, Jonathan and Solomatin, Roman and Çağatan, Ömer and Kundu, Akash and Bernstorff, Martin and Xiao, Shitao and Sukhlecha, Akshita and Pahwa, Bhavish and Poświata, Rafał and GV, Kranthi Kiran and Ashraf, Shawon and Auras, Daniel and Plüster, Björn and Harries, Jan Philipp and Magne, Loïc and Mohr, Isabelle and Hendriksen, Mariya and Zhu, Dawei and Gisserot-Boukhlef, Hippolyte and Aarsen, Tom and Kostkan, Jan and Wojtasik, Konrad and Lee, Taemin and Šuppa, Marek and Zhang, Crystina and Rocca, Roberta and Hamdy, Mohammed and Michail, Andrianos and Yang, John and Faysse, Manuel and Vatolin, Aleksei and Thakur, Nandan and Dey, Manan and Vasani, Dipam and Chitale, Pranjal and Tedeschi, Simone and Tai, Nguyen and Snegirev, Artem and Günther, Michael and Xia, Mengzhou and Shi, Weijia and Lù, Xing Han and Clive, Jordan and Krishnakumar, Gayatri and Maksimova, Anna and Wehrli, Silvan and Tikhonova, Maria and Panchal, Henil and Abramov, Aleksandr and Ostendorff, Malte and Liu, Zheng and Clematide, Simon and Miranda, Lester James and Fenogenova, Alena and Song, Guangyu and Safi, Ruqiya Bin and Li, Wen-Ding and Borghini, Alessia and Cassano, Federico and Su, Hongjin and Lin, Jimmy and Yen, Howard and Hansen, Lasse and Hooker, Sara and Xiao, Chenghao and Adlakha, Vaibhav and Weller, Orion and Reddy, Siva and Muennighoff, Niklas},
	month = nov,
	year = {2025},
	note = {arXiv:2502.13595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@inproceedings{xu_layoutlm_2020,
	title = {{LayoutLM}: {Pre}-training of {Text} and {Layout} for {Document} {Image} {Understanding}},
	shorttitle = {{LayoutLM}},
	url = {http://arxiv.org/abs/1912.13318},
	doi = {10.1145/3394486.3403172},
	abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
	urldate = {2025-11-22},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	month = aug,
	year = {2020},
	note = {arXiv:1912.13318 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1192--1200},
}

@misc{muennighoff_mteb_2023,
	title = {{MTEB}: {Massive} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MTEB}},
	url = {http://arxiv.org/abs/2210.07316},
	doi = {10.48550/arXiv.2210.07316},
	abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
	month = mar,
	year = {2023},
	note = {arXiv:2210.07316 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using {Hierarchical} {Navigable} {Small} {World} graphs},
	url = {http://arxiv.org/abs/1603.09320},
	doi = {10.48550/arXiv.1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = aug,
	year = {2018},
	note = {arXiv:1603.09320 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kusupati_matryoshka_2024,
	title = {Matryoshka {Representation} {Learning}},
	url = {http://arxiv.org/abs/2205.13147},
	doi = {10.48550/arXiv.2205.13147},
	abstract = {Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2\% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
	month = feb,
	year = {2024},
	note = {arXiv:2205.13147 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{auer_docling_2024,
	title = {Docling {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.09869},
	doi = {10.48550/arXiv.2408.09869},
	abstract = {This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and Lima, Rafael Teixeira de and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
	month = dec,
	year = {2024},
	note = {arXiv:2408.09869 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Software Engineering},
}

@misc{zheng_sglang_2024,
	title = {{SGLang}: {Efficient} {Execution} of {Structured} {Language} {Model} {Programs}},
	shorttitle = {{SGLang}},
	url = {http://arxiv.org/abs/2312.07104},
	doi = {10.48550/arXiv.2312.07104},
	abstract = {Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E. and Barrett, Clark and Sheng, Ying},
	month = jun,
	year = {2024},
	note = {arXiv:2312.07104 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
}

@misc{geng_jsonschemabench_2025,
	title = {{JSONSchemaBench}: {A} {Rigorous} {Benchmark} of {Structured} {Outputs} for {Language} {Models}},
	shorttitle = {{JSONSchemaBench}},
	url = {http://arxiv.org/abs/2501.10868},
	doi = {10.48550/arXiv.2501.10868},
	abstract = {Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Geng, Saibo and Cooper, Hudson and Moskal, Michał and Jenkins, Samuel and Berman, Julian and Ranchin, Nathan and West, Robert and Horvitz, Eric and Nori, Harsha},
	month = feb,
	year = {2025},
	note = {arXiv:2501.10868 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{willard_efficient_2023,
	title = {Efficient {Guided} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.09702},
	doi = {10.48550/arXiv.2307.09702},
	abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Willard, Brandon T. and Louf, Rémi},
	month = aug,
	year = {2023},
	note = {arXiv:2307.09702 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_retrieval_2024,
	title = {Retrieval meets {Long} {Context} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.03025},
	doi = {10.48550/arXiv.2310.03025},
	abstract = {Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
	month = jan,
	year = {2024},
	note = {arXiv:2310.03025 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{shi_large_2023,
	title = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
	url = {http://arxiv.org/abs/2302.00093},
	doi = {10.48550/arXiv.2302.00093},
	abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Schärli, Nathanael and Zhou, Denny},
	month = jun,
	year = {2023},
	note = {arXiv:2302.00093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yuan_signal--noise_2019,
	title = {Signal-to-{Noise} {Ratio}: {A} {Robust} {Distance} {Metric} for {Deep} {Metric} {Learning}},
	shorttitle = {Signal-to-{Noise} {Ratio}},
	url = {http://arxiv.org/abs/1904.02616},
	doi = {10.48550/arXiv.1904.02616},
	abstract = {Deep metric learning, which learns discriminative features to process image clustering and retrieval tasks, has attracted extensive attention in recent years. A number of deep metric learning methods, which ensure that similar examples are mapped close to each other and dissimilar examples are mapped farther apart, have been proposed to construct effective structures for loss functions and have shown promising results. In this paper, different from the approaches on learning the loss structures, we propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. By exploring the properties of our SNR distance metric from the view of geometry space and statistical theory, we analyze the properties of our metric and show that it can preserve the semantic similarity between image pairs, which well justify its suitability for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features. Leveraging our SNR distance metric, we propose Deep SNR-based Metric Learning (DSML) to generate discriminative feature embeddings. By extensive experiments on three widely adopted benchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its superiority over other state-of-the-art methods. Additionally, we extend our SNR distance metric to deep hashing learning, and conduct experiments on two benchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness and generality of our SNR distance metric.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Yuan, Tongtong and Deng, Weihong and Tang, Jian and Tang, Yinan and Chen, Binghui},
	month = apr,
	year = {2019},
	note = {arXiv:1904.02616 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{heineman_signal_2025,
	title = {Signal and {Noise}: {A} {Framework} for {Reducing} {Uncertainty} in {Language} {Model} {Evaluation}},
	shorttitle = {Signal and {Noise}},
	url = {http://arxiv.org/abs/2508.13144},
	doi = {10.48550/arXiv.2508.13144},
	abstract = {Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Heineman, David and Hofmann, Valentin and Magnusson, Ian and Gu, Yuling and Smith, Noah A. and Hajishirzi, Hannaneh and Lo, Kyle and Dodge, Jesse},
	month = aug,
	year = {2025},
	note = {arXiv:2508.13144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_core_2025,
	title = {Core {Context} {Aware} {Transformers} for {Long} {Context} {Language} {Modeling}},
	url = {http://arxiv.org/abs/2412.12465},
	doi = {10.48550/arXiv.2412.12465},
	abstract = {Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Chen, Yaofo and You, Zeng and Zhang, Shuhai and Li, Haokun and Li, Yirui and Wang, Yaowei and Tan, Mingkui},
	month = aug,
	year = {2025},
	note = {arXiv:2412.12465 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_lost_2023,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = nov,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{maghakian_embedding-free_2025,
	address = {Suzhou, China},
	title = {Embedding-{Free} {RAG}},
	url = {https://aclanthology.org/2025.findings-emnlp.1360},
	doi = {10.18653/v1/2025.findings-emnlp.1360},
	language = {en},
	urldate = {2025-11-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2025},
	publisher = {Association for Computational Linguistics},
	author = {Maghakian, Jessica and Sinha, Raunak and Schettewi, Max and Kaur, Gunkirat},
	year = {2025},
	pages = {24974--24985},
}

@misc{kuratov_cramming_2025,
	title = {Cramming 1568 {Tokens} into a {Single} {Vector} and {Back} {Again}: {Exploring} the {Limits} of {Embedding} {Space} {Capacity}},
	shorttitle = {Cramming 1568 {Tokens} into a {Single} {Vector} and {Back} {Again}},
	url = {http://arxiv.org/abs/2502.13063},
	doi = {10.48550/arXiv.2502.13063},
	abstract = {A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Kuratov, Yuri and Arkhipov, Mikhail and Bulatov, Aydar and Burtsev, Mikhail},
	month = jun,
	year = {2025},
	note = {arXiv:2502.13063 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{weller_theoretical_2025,
	title = {On the {Theoretical} {Limitations} of {Embedding}-{Based} {Retrieval}},
	url = {http://arxiv.org/abs/2508.21038},
	doi = {10.48550/arXiv.2508.21038},
	abstract = {Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Weller, Orion and Boratko, Michael and Naim, Iftekhar and Lee, Jinhyuk},
	month = aug,
	year = {2025},
	note = {arXiv:2508.21038 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{li_long_2024,
	title = {Long {Context} vs. {RAG} for {LLMs}: {An} {Evaluation} and {Revisits}},
	shorttitle = {Long {Context} vs. {RAG} for {LLMs}},
	url = {http://arxiv.org/abs/2501.01880},
	doi = {10.48550/arXiv.2501.01880},
	abstract = {Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Li, Xinze and Cao, Yixin and Ma, Yubo and Sun, Aixin},
	month = dec,
	year = {2024},
	note = {arXiv:2501.01880 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kuratov_search_2024,
	title = {In {Search} of {Needles} in a {11M} {Haystack}: {Recurrent} {Memory} {Finds} {What} {LLMs} {Miss}},
	shorttitle = {In {Search} of {Needles} in a {11M} {Haystack}},
	url = {http://arxiv.org/abs/2402.10790},
	doi = {10.48550/arXiv.2402.10790},
	abstract = {This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to \$10{\textasciicircum}4\$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to \$11{\textbackslash}times 10{\textasciicircum}6\$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10790 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{comanici_gemini_2025,
	title = {Gemini 2.5: {Pushing} the {Frontier} with {Advanced} {Reasoning}, {Multimodality}, {Long} {Context}, and {Next} {Generation} {Agentic} {Capabilities}},
	shorttitle = {Gemini 2.5},
	url = {http://arxiv.org/abs/2507.06261},
	doi = {10.48550/arXiv.2507.06261},
	abstract = {In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and Marris, Luke and Petulla, Sam and Gaffney, Colin and Aharoni, Asaf and Lintz, Nathan and Pais, Tiago Cardal and Jacobsson, Henrik and Szpektor, Idan and Jiang, Nan-Jiang and Haridasan, Krishna and Omran, Ahmed and Saunshi, Nikunj and Bahri, Dara and Mishra, Gaurav and Chu, Eric and Boyd, Toby and Hekman, Brad and Parisi, Aaron and Zhang, Chaoyi and Kawintiranon, Kornraphop and Bedrax-Weiss, Tania and Wang, Oliver and Xu, Ya and Purkiss, Ollie and Mendlovic, Uri and Deutel, Ilaï and Nguyen, Nam and Langley, Adam and Korn, Flip and Rossazza, Lucia and Ramé, Alexandre and Waghmare, Sagar and Miller, Helen and Byrd, Nathan and Sheshan, Ashrith and Hadsell, Raia and Bhardwaj, Sangnie and Janus, Pawel and Rissa, Tero and Horgan, Dan and Abdagic, Alvin and Belenki, Lior and Allingham, James and Singh, Anima and Guidroz, Theo and Srinivasan, Srivatsan and Schmit, Herman and Chiafullo, Kristen and Elisseeff, Andre and Jha, Nilpa and Kolhar, Prateek and Berrada, Leonard and Ding, Frank and Si, Xiance and Mallick, Shrestha Basu and Och, Franz and Erell, Sofia and Ni, Eric and Latkar, Tejasi and Yang, Sherry and Sirkovic, Petar and Feng, Ziqiang and Leland, Robert and Hornung, Rachel and Wu, Gang and Blundell, Charles and Alvari, Hamidreza and Huang, Po-Sen and Yip, Cathy and Deur, Sanja and Liu, Li and Surita, Gabriela and Duque, Pablo and Damen, Dima and Jia, Johnson and Guez, Arthur and Mircea, Markus and Sinha, Animesh and Magni, Alberto and Stradomski, Paweł and Marian, Tal and Galić, Vlado and Chen, Wenhu and Husain, Hisham and Singhal, Achintya and Grewe, Dominik and Aubet, François-Xavier and Song, Shuang and Blanco, Lorenzo and Rechis, Leland and Ho, Lewis and Munoz, Rich and Zheng, Kelvin and Hamrick, Jessica and Mather, Kevin and Taitelbaum, Hagai and Rutherford, Eliza and Lei, Yun and Chen, Kuangyuan and Shukla, Anand and Moreira, Erica and Doi, Eric and Isik, Berivan and Shabat, Nir and Rogozińska, Dominika and Kolipaka, Kashyap and Chang, Jason and Vušak, Eugen and Venkatachary, Srinivasan and Noghabi, Shadi and Bharti, Tarun and Jun, Younghoon and Zaks, Aleksandr and Green, Simon and Challagundla, Jeshwanth and Wong, William and Mohammad, Muqthar and Hirsch, Dean and Cheng, Yong and Naim, Iftekhar and Proleev, Lev and Vincent, Damien and Singh, Aayush and Krikun, Maxim and Krishnan, Dilip and Ghahramani, Zoubin and Atias, Aviel and Aggarwal, Rajeev and Kirov, Christo and Vytiniotis, Dimitrios and Koh, Christy and Chronopoulou, Alexandra and Dogra, Pawan and Ion, Vlad-Doru and Tyen, Gladys and Lee, Jason and Weissenberger, Felix and Strohman, Trevor and Balakrishna, Ashwin and Rae, Jack and Velic, Marko and Liedekerke, Raoul de and Elyada, Oded and Yuan, Wentao and Liu, Canoee and Shani, Lior and Kishchenko, Sergey and Alessio, Bea and Li, Yandong and Song, Richard and Kwei, Sam and Jankowski, Orion and Pappu, Aneesh and Namiki, Youhei and Ma, Yenai and Tripuraneni, Nilesh and Cherry, Colin and Ikonomidis, Marissa and Ling, Yu-Cheng and Ji, Colin and Westberg, Beka and Wright, Auriel and Yu, Da and Parkinson, David and Ramaswamy, Swaroop and Connor, Jerome and Yeganeh, Soheil Hassas and Grover, Snchit and Kenwright, George and Litchev, Lubo and Apps, Chris and Tomala, Alex and Halim, Felix and Castro-Ros, Alex and Li, Zefei and Boral, Anudhyan and Sho, Pauline and Yarom, Michal and Malmi, Eric and Klinghoffer, David and Lin, Rebecca and Ansell, Alan and S, Pradeep Kumar and Zhao, Shubin and Zuo, Siqi and Santoro, Adam and Cheng, Heng-Tze and Demmessie, Solomon and Liu, Yuchi and Brichtova, Nicole and Culp, Allie and Braun, Nathaniel and Graur, Dan and Ng, Will and Mehta, Nikhil and Phillips, Aaron and Sundberg, Patrik and Godbole, Varun and Liu, Fangyu and Katariya, Yash and Rim, David and Seyedhosseini, Mojtaba and Ammirati, Sean and Valfridsson, Jonas and Malihi, Mahan and Knight, Timothy and Toor, Andeep and Lampe, Thomas and Ittycheriah, Abe and Chiang, Lewis and Yeung, Chak and Fréchette, Alexandre and Rao, Jinmeng and Wang, Huisheng and Srivastava, Himanshu and Zhang, Richard and Rhodes, Rocky and Brand, Ariel and Weesner, Dean and Figotin, Ilya and Gimeno, Felix and Fellinger, Rachana and Marcenac, Pierre and Leal, José and Marcus, Eyal and Cotruta, Victor and Cabrera, Rodrigo and Luo, Sheryl and Garrette, Dan and Axelrod, Vera and Baltateanu, Sorin and Barker, David and Chen, Dongkai and Toma, Horia and Ingram, Ben and Riesa, Jason and Kulkarni, Chinmay and Zhang, Yujing and Liu, Hongbin and Wang, Chao and Polacek, Martin and Wu, Will and Hui, Kai and Reyes, Adrian N. and Su, Yi and Barnes, Megan and Malhi, Ishaan and Siddiqui, Anfal and Feng, Qixuan and Damaschin, Mihai and Pighin, Daniele and Steiner, Andreas and Yang, Samuel and Boppana, Ramya Sree and Ivanov, Simeon and Kandoor, Arun and Shah, Aditya and Mujika, Asier and Huang, Da and Choquette-Choo, Christopher A. and Patel, Mohak and Yu, Tianhe and Creswell, Toni and Jerry and Liu and Barros, Catarina and Razeghi, Yasaman and Roy, Aurko and Culliton, Phil and Xiong, Binbin and Pan, Jiaqi and Strohmann, Thomas and Powell, Tolly and Seal, Babi and DeCarlo, Doug and Shyam, Pranav and Katircioglu, Kaan and Wang, Xuezhi and Hardin, Cassidy and Odisho, Immanuel and Broder, Josef and Chang, Oscar and Nair, Arun and Shtefan, Artem and O'Brien, Maura and Agarwal, Manu and Potluri, Sahitya and Goyal, Siddharth and Jhindal, Amit and Thakur, Saksham and Stuken, Yury and Lyon, James and Toutanova, Kristina and Feng, Fangxiaoyu and Wu, Austin and Horn, Ben and Wang, Alek and Cullum, Alex and Taubman, Gabe and Shrivastava, Disha and Shi, Chongyang and Tomlinson, Hamish and Patel, Roma and Tu, Tao and Oflazer, Ada Maksutaj and Pongetti, Francesco and Yang, Mingyao and Taïga, Adrien Ali and Perot, Vincent and Pierse, Nuo Wang and Han, Feng and Drori, Yoel and Iturrate, Iñaki and Chakrabarti, Ayan and Yeung, Legg and Dopson, Dave and Chen, Yi-ting and Kulshreshtha, Apoorv and Guo, Tongfei and Pham, Philip and Schuster, Tal and Chen, Junquan and Polozov, Alex and Xing, Jinwei and Zhou, Huanjie and Kacham, Praneeth and Kukliansky, Doron and Miech, Antoine and Yaroshenko, Sergey and Chi, Ed and Douglas, Sholto and Fei, Hongliang and Blondel, Mathieu and Myla, Preethi and Madmoni, Lior and Wu, Xing and Keysers, Daniel and Kjems, Kristian and Albuquerque, Isabela and Yu, Lijun and D'sa, Joel and Plantan, Michelle and Ionescu, Vlad and Elias, Jaume Sanchez and Gupta, Abhirut and Vuyyuru, Manish Reddy and Alcober, Fred and Zhou, Tong and Ji, Kaiyang and Hartmann, Florian and Puttagunta, Subha and Song, Hugo and Amid, Ehsan and Stefanoiu, Anca and Lee, Andrew and Pucciarelli, Paul and Wang, Emma and Raul, Amit and Petrov, Slav and Tian, Isaac and Anklin, Valentin and Nti, Nana and Gomes, Victor and Schumacher, Max and Vesom, Grace and Panagopoulos, Alex and Bousmalis, Konstantinos and Andor, Daniel and Jacob, Josh and Zhang, Yuan and Rosgen, Bill and Kecman, Matija and Tung, Matthew and Belias, Alexandra and Goodman, Noah and Covington, Paul and Wieder, Brian and Saxena, Nikita and Davoodi, Elnaz and Huang, Muhuan and Maddineni, Sharath and Roulet, Vincent and Campbell-Ajala, Folawiyo and Sessa, Pier Giuseppe and Xintian and Wu and Lai, Guangda and Collins, Paul and Haig, Alex and Sakenas, Vytenis and Xu, Xiaowei and Giustina, Marissa and Shafey, Laurent El and Charoenpanit, Pichi and Garg, Shefali and Ainslie, Joshua and Severson, Boone and Arenas, Montse Gonzalez and Pathak, Shreya and Rajayogam, Sujee and Feng, Jie and Bakker, Michiel and Li, Sheng and Wichers, Nevan and Rogers, Jamie and Geng, Xinyang and Li, Yeqing and Jagerman, Rolf and Jia, Chao and Olmert, Nadav and Sharon, David and Mauger, Matthew and Mariserla, Sandeep and Ma, Hongxu and Mohabey, Megha and Kim, Kyuyeun and Andreev, Alek and Pollom, Scott and Love, Juliette and Jain, Vihan and Agrawal, Priyanka and Schroecker, Yannick and Fortin, Alisa and Warmuth, Manfred and Liu, Ji and Leach, Andrew and Blok, Irina and Girirajan, Ganesh Poomal and Aharoni, Roee and Uria, Benigno and Sozanschi, Andrei and Goldberg, Dan and Ionita, Lucian and Ribeiro, Marco Tulio and Zlocha, Martin and Birodkar, Vighnesh and Lachgar, Sami and Yuan, Liangzhe and Choudhury, Himadri and Ginsberg, Matt and Zheng, Fei and Dibb, Gregory and Graves, Emily and Lokhande, Swachhand and Rasskin, Gabriel and Muraru, George-Cristian and Quick, Corbin and Tata, Sandeep and Sermanet, Pierre and Chawla, Aditya and Karo, Itay and Wang, Yan and Zhang, Susan and Keller, Orgad and Dragan, Anca and Su, Guolong and Chou, Ian and Liu, Xi and Tao, Yiqing and Prabhakara, Shruthi and Wilson, Marc and Liu, Ruibo and Wang, Shibo and Evans, Georgie and Du, David and Castaño, Alfonso and Prasad, Gautam and Mahdy, Mona El and Gerlach, Sebastian and Reid, Machel and Kahn, Jarrod and Zait, Amir and Pillai, Thanumalayan Sankaranarayana and Ulrich, Thatcher and Wang, Guanyu and Wassenberg, Jan and Farkash, Efrat and Yalasangi, Kiran and Wang, Congchao and Bauza, Maria and Bucher, Simon and Liu, Ting and Yan, Jun and Leung, Gary and Sindhwani, Vikas and Barnes, Parker and Singh, Avi and Jurin, Ivan and Chang, Jichuan and Bhumihar, Niket Kumar and Eiger, Sivan and Citovsky, Gui and Withbroe, Ben and Li, Zhang and Xue, Siyang and Santo, Niccolò Dal and Stoyanov, Georgi and Raimond, Yves and Zheng, Steven and Gao, Yilin and Listík, Vít and Kwasiborski, Sławek and Saputro, Rachel and Ozturel, Adnan and Mallya, Ganesh and Majmundar, Kushal and West, Ross and Caron, Paul and Wei, Jinliang and Castrejon, Lluis and Vikram, Sharad and Ramachandran, Deepak and Dhawan, Nikhil and Park, Jiho and Smoot, Sara and Driessche, George van den and Blau, Yochai and Malik, Chase and Liang, Wei and Hirsch, Roy and Santos, Cicero Nogueira dos and Weinstein, Eugene and Oord, Aäron van den and Lall, Sid and FitzGerald, Nicholas and Jiang, Zixuan and Yang, Xuan and Webster, Dale and Elqursh, Ali and Pope, Aedan and Rotival, Georges and Raposo, David and Zhu, Wanzheng and Dean, Jeff and Alabed, Sami and Tran, Dustin and Gupta, Arushi and Gleicher, Zach and Austin, Jessica and Rosseel, Edouard and Umekar, Megh and Das, Dipanjan and Sun, Yinghao and Chen, Kai and Misiunas, Karolis and Zhou, Xiang and Di, Yixian and Loo, Alyssa and Newlan, Josh and Li, Bo and Ramasesh, Vinay and Xu, Ying and Chen, Alex and Gandhe, Sudeep and Soricut, Radu and Gupta, Nikita and Hu, Shuguang and El-Sayed, Seliem and Garcia, Xavier and Brusilovsky, Idan and Chen, Pu-Chin and Bolt, Andrew and Huang, Lu and Gurney, Alex and Zhang, Zhiying and Pritzel, Alexander and Wilkiewicz, Jarek and Seybold, Bryan and Shamanna, Bhargav Kanagal and Fischer, Felix and Dean, Josef and Gill, Karan and Mcilroy, Ross and Bhowmick, Abhishek and Selier, Jeremy and Yang, Antoine and Cheng, Derek and Magay, Vladimir and Tan, Jie and Varma, Dhriti and Walder, Christian and Kocisky, Tomas and Nakashima, Ryo and Natsev, Paul and Kwong, Mike and Gog, Ionel and Zhang, Chiyuan and Dieleman, Sander and Jimma, Thomas and Ryabtsev, Andrey and Brahma, Siddhartha and Steiner, David and Du, Dayou and Žužul, Ante and Žanić, Mislav and Raghavachari, Mukund and Gierke, Willi and Zheng, Zeyu and Petrova, Dessie and Dauphin, Yann and Liu, Yuchuan and Kessler, Ido and Hand, Steven and Duvarney, Chris and Kim, Seokhwan and Lee, Hyo and Hussenot, Léonard and Hui, Jeffrey and Smith, Josh and Jain, Deepali and Xia, Jiawei and Tomar, Gaurav Singh and Amiri, Keyvan and Phan, Du and Fuchs, Fabian and Weyand, Tobias and Tomasev, Nenad and Cordell, Alexandra and Liu, Xin and Mallinson, Jonathan and Joshi, Pankaj and Crawford, Andy and Suggala, Arun and Chien, Steve and Fernando, Nick and Sanchez-Vargas, Mariella and Williams, Duncan and Crone, Phil and Luo, Xiyang and Karpov, Igor and Shan, Jyn and Thurk, Terry and Strudel, Robin and Voigtlaender, Paul and Patil, Piyush and Dozat, Tim and Khodaei, Ali and Singla, Sahil and Ambroszczyk, Piotr and Wu, Qiyin and Chang, Yifan and Roark, Brian and Hegde, Chaitra and Ding, Tianli and Filos, Angelos and Wu, Zhongru and Pinto, André Susano and Liu, Shuang and Khanna, Saarthak and Pandey, Aditya and Mcloughlin, Siobhan and Li, Qiujia and Haves, Sam and Zhou, Allan and Buchatskaya, Elena and Leal, Isabel and Boursac, Peter de and Akazawa, Nami and Anderson, Nina and Chen, Terry and Somandepalli, Krishna and Liang, Chen and Goenka, Sheela and Winkler, Stephanie and Grushetsky, Alexander and Ding, Yifan and Smith, Jamie and Ye, Fan and Pont-Tuset, Jordi and Li, Eric and Li, Ruichao and Golany, Tomer and Wegner, Dawid and Jiang, Tao and Barak, Omer and Shangguan, Yuan and Vértes, Eszter and Wong, Renee and Bornschein, Jörg and Tudor, Alex and Bevilacqua, Michele and Schaul, Tom and Rawat, Ankit Singh and Zhao, Yang and Axiotis, Kyriakos and Meng, Lei and McLean, Cory and Lai, Jonathan and Beattie, Jennifer and Kushman, Nate and Liu, Yaxin and Kutzman, Blair and Lang, Fiona and Ye, Jingchen and Netrapalli, Praneeth and Mishra, Pushkar and Khan, Myriam and Goel, Megha and Willoughby, Rob and Tian, David and Zhuang, Honglei and Chen, J. D. and Tsai, Zak and Kementsietsidis, Tasos and Khare, Arjun and Keeling, James and Xu, Keyang and Waters, Nathan and Altché, Florent and Popat, Ashok and Mittal, Bhavishya and Saxton, David and Badawy, Dalia El and Mathieu, Michael and Zheng, Zheng and Zhou, Hao and Ranka, Nishant and Shin, Richard and Duan, Qingnan and Salimans, Tim and Mihailescu, Ioana and Shaham, Uri and Chang, Ming-Wei and Assael, Yannis and Dikkala, Nishanth and Izzard, Martin and Cohen-Addad, Vincent and Graves, Cat and Feinberg, Vlad and Chung, Grace and Strouse, D. J. and Karmon, Danny and Sharifzadeh, Sahand and Ashwood, Zoe and Pham, Khiem and Blanton, Jon and Vasiloff, Alex and Barber, Jarred and Geller, Mark and Zhou, Aurick and Zubach, Fedir and Huang, Tzu-Kuo and Zhang, Lei and Gupta, Himanshu and Young, Matt and Proskurnia, Julia and Votel, Ronny and Gabeur, Valentin and Barcik, Gabriel and Tripathi, Aditya and Yu, Hongkun and Yan, Geng and Changpinyo, Beer and Pavetić, Filip and Coyle, Amy and Fujii, Yasuhisa and Mendez, Jorge Gonzalez and Zhou, Tianhao and Rajamani, Harish and Hechtman, Blake and Cao, Eddie and Juan, Da-Cheng and Tan, Yi-Xuan and Dalibard, Valentin and Du, Yilun and Clay, Natalie and Yao, Kaisheng and Jia, Wenhao and Vijaykumar, Dimple and Zhou, Yuxiang and Bai, Xinyi and Hung, Wei-Chih and Pecht, Steven and Todorov, Georgi and Khadke, Nikhil and Gupta, Pramod and Lahoti, Preethi and Autef, Arnaud and Duddu, Karthik and Lee-Thorp, James and Bykovsky, Alexander and Misiunas, Tautvydas and Flennerhag, Sebastian and Thangaraj, Santhosh and McGiffin, Jed and Nado, Zack and Kunesch, Markus and Noever, Andreas and Hertz, Amir and Liang, Marco and Stone, Victor and Palmer, Evan and Daruki, Samira and Pramanik, Arijit and Põder, Siim and Kyker, Austin and Khan, Mina and Sluzhaev, Evgeny and Ritter, Marvin and Ruderman, Avraham and Zhou, Wenlei and Nagpal, Chirag and Vodrahalli, Kiran and Necula, George and Barham, Paul and Pavlick, Ellie and Hartford, Jay and Shafran, Izhak and Zhao, Long and Mikuła, Maciej and Eccles, Tom and Shimokawa, Hidetoshi and Garg, Kanav and Vilnis, Luke and Chen, Hanwen and Shumailov, Ilia and Lee, Kuang-Huei and Abdelhamed, Abdelrahman and Xie, Meiyan and Cohen, Vered and Hlavnova, Ester and Malkin, Dan and Sitawarin, Chawin and Lottes, James and Coquinot, Pauline and Yu, Tianli and Kumar, Sandeep and Zhang, Jingwei and Mahendru, Aroma and Ahmed, Zafarali and Martens, James and Chen, Tao and Boag, Aviel and Peng, Daiyi and Devin, Coline and Klimovskiy, Arseniy and Phuong, Mary and Vainstein, Danny and Xie, Jin and Ramabhadran, Bhuvana and Howard, Nathan and Yu, Xinxin and Goswami, Gitartha and Cui, Jingyu and Shleifer, Sam and Pinto, Mario and Yeh, Chih-Kuan and Yang, Ming-Hsuan and Javanmardi, Sara and Ethier, Dan and Lee, Chace and Orbay, Jordi and Kotecha, Suyog and Bromberg, Carla and Shaw, Pete and Thornton, James and Rosenthal, Adi Gerzi and Gu, Shane and Thomas, Matt and Gemp, Ian and Ayyar, Aditya and Ushio, Asahi and Selvan, Aarush and Wee, Joel and Liu, Chenxi and Majzoubi, Maryam and Yu, Weiren and Abernethy, Jake and Liechty, Tyler and Pan, Renke and Nguyen, Hoang and Qiong and Hu and Perrin, Sarah and Arora, Abhinav and Pitler, Emily and Wang, Weiyi and Shivakumar, Kaushik and Prost, Flavien and Limonchik, Ben and Wang, Jing and Gao, Yi and Cour, Timothee and Buch, Shyamal and Gui, Huan and Ivanova, Maria and Neubeck, Philipp and Chan, Kelvin and Kim, Lucy and Chen, Huizhong and Goyal, Naman and Chung, Da-Woon and Liu, Lu and Su, Yao and Petrushkina, Anastasia and Shen, Jiajun and Joulin, Armand and Xu, Yuanzhong and Lin, Stein Xudong and Kulizhskaya, Yana and Chelba, Ciprian and Vasudevan, Shobha and Collins, Eli and Bashlovkina, Vasilisa and Lu, Tony and Fritz, Doug and Park, Jongbin and Zhou, Yanqi and Su, Chen and Tanburn, Richard and Sushkov, Mikhail and Rasquinha, Mitchelle and Li, Jinning and Prendki, Jennifer and Li, Yiming and LV, Pallavi and Sharma, Shriya and Fitoussi, Hen and Huang, Hui and Dai, Andrew and Dao, Phuong and Burrows, Mike and Prior, Henry and Qin, Danfeng and Pundak, Golan and Sjoesund, Lars Lowe and Khurshudov, Art and Zhu, Zhenkai and Webson, Albert and Kemp, Elizabeth and Tan, Tat and Agrawal, Saurabh and Sargsyan, Susie and Cheng, Liqun and Stephan, Jim and Kwiatkowski, Tom and Reid, David and Byravan, Arunkumar and Michaely, Assaf Hurwitz and Heess, Nicolas and Zhou, Luowei and Goenka, Sonam and Carpenter, Viral and Levskaya, Anselm and Wang, Bo and Roberts, Reed and Leblond, Rémi and Chikkerur, Sharat and Ginzburg, Stav and Chang, Max and Riachi, Robert and Chuqiao and Xu and Borsos, Zalán and Pliskin, Michael and Pawar, Julia and Lustman, Morgane and Kirkwood, Hannah and Anand, Ankit and Chaudhary, Aditi and Kalb, Norbert and Milan, Kieran and Augenstein, Sean and Goldie, Anna and Prince, Laurel and Raman, Karthik and Sun, Yanhua and Xia, Vivian and Cohen, Aaron and Huo, Zhouyuan and Camp, Josh and Ellis, Seher and Zilka, Lukas and Torres, David Vilar and Patel, Lisa and Arora, Sho and Chan, Betty and Adler, Jonas and Ayoub, Kareem and Liang, Jacky and Jamil, Fayaz and Jiang, Jiepu and Baumgartner, Simon and Sun, Haitian and Karov, Yael and Akulov, Yaroslav and Zheng, Hui and Cai, Irene and Fantacci, Claudio and Rubin, James and Acha, Alex Rav and Wang, Mengchao and D'Souza, Nina and Sathyanarayana, Rohit and Dai, Shengyang and Rowe, Simon and Simanovsky, Andrey and Goldman, Omer and Kuang, Yuheng and Pan, Xiaoyue and Rosenberg, Andrew and Rojas-Esponda, Tania and Dutta, Praneet and Zeng, Amy and Jurenka, Irina and Farquhar, Greg and Bansal, Yamini and Iqbal, Shariq and Roelofs, Becca and Joung, Ga-Young and Beak, Parker and Ryu, Changwan and Poplin, Ryan and Wu, Yan and Alayrac, Jean-Baptiste and Buthpitiya, Senaka and Ronneberger, Olaf and Habtegebriel, Caleb and Li, Wei and Cavallaro, Paul and Wei, Aurora and Bensky, Guy and Denk, Timo and Ganapathy, Harish and Stanway, Jeff and Joshi, Pratik and Bertolini, Francesco and Lo, Jessica and Ma, Olivia and Charles, Zachary and Sampemane, Geta and Sahni, Himanshu and Chen, Xu and Askham, Harry and Gaddy, David and Young, Peter and Tan, Jiewen and Eyal, Matan and Bražinskas, Arthur and Zhong, Li and Wu, Zhichun and Epstein, Mark and Bailey, Kai and Hard, Andrew and Lee, Kamyu and Goldshtein, Sasha and Ruiz, Alex and Badawi, Mohammed and Lochbrunner, Matthias and Kearns, J. K. and Brown, Ashley and Pardo, Fabio and Weber, Theophane and Yang, Haichuan and Jiang, Pan-Pan and Akin, Berkin and Fu, Zhao and Wainwright, Marcus and Zou, Chi and Gaba, Meenu and Manzagol, Pierre-Antoine and Kan, Wendy and Song, Yang and Zainullina, Karina and Lin, Rui and Ko, Jeongwoo and Deshmukh, Salil and Jindal, Apoorv and Svensson, James and Tyam, Divya and Zhao, Heri and Kaeser-Chen, Christine and Baird, Scott and Moradi, Pooya and Hall, Jamie and Guo, Qiuchen and Tsang, Vincent and Liang, Bowen and Pereira, Fernando and Ganesh, Suhas and Korotkov, Ivan and Adamek, Jakub and Thiagarajan, Sridhar and Tran, Vinh and Chen, Charles and Tar, Chris and Jain, Sanil and Dasgupta, Ishita and Bilal, Taylan and Reitter, David and Zhao, Kai and Vezzani, Giulia and Gehman, Yasmin and Mehta, Pulkit and Beltrone, Lauren and Dotiwalla, Xerxes and Guadarrama, Sergio and Abbas, Zaheer and Karp, Stefani and Georgiev, Petko and Ferng, Chun-Sung and Brockschmidt, Marc and Peng, Liqian and Hirnschall, Christoph and Verma, Vikas and Bi, Yingying and Xiao, Ying and Dabush, Avigail and Xu, Kelvin and Wallis, Phil and Parker, Randall and Wang, Qifei and Xu, Yang and Safarli, Ilkin and Tewari, Dinesh and Zhang, Yin and Kim, Seungyeon and Gesmundo, Andrea and Thomas, Mackenzie and Levi, Sergey and Chowdhury, Ahmed and Rao, Kanishka and Garst, Peter and Conway-Rahman, Sam and Ran, Helen and McKinney, Kay and Xiao, Zhisheng and Yu, Wenhao and Agrawal, Rohan and Stjerngren, Axel and Ionescu, Catalin and Chen, Jingjing and Sharma, Vivek and Chiu, Justin and Liu, Fei and Franko, Ken and Sanford, Clayton and Cai, Xingyu and Michel, Paul and Ganapathy, Sanjay and Labanowski, Jane and Garrett, Zachary and Vargas, Ben and Sun, Sean and Gale, Bryan and Buschmann, Thomas and Desjardins, Guillaume and Ghelani, Nimesh and Jain, Palak and Verma, Mudit and Asawaroengchai, Chulayuth and Eisenschlos, Julian and Harlalka, Jitendra and Kazawa, Hideto and Metzler, Don and Howland, Joshua and Jian, Ying and Ades, Jake and Shah, Viral and Gangwani, Tynan and Lee, Seungji and Ring, Roman and Hernandez, Steven M. and Reich, Dean and Sinha, Amer and Sathe, Ashutosh and Kovac, Joe and Gill, Ashleah and Kannan, Ajay and D'olimpio, Andrea and Sevenich, Martin and Whang, Jay and Kim, Been and Sim, Khe Chai and Chen, Jilin and Zhang, Jiageng and Lall, Shuba and Matias, Yossi and Jia, Bill and Friesen, Abe and Nasso, Sara and Thapliyal, Ashish and Perozzi, Bryan and Yu, Ting and Shekhawat, Anna and Huda, Safeen and Grabowski, Peter and Wang, Eric and Sreevatsa, Ashwin and Dib, Hilal and Hassen, Mehadi and Schuh, Parker and Milutinovic, Vedrana and Welty, Chris and Quinn, Michael and Shah, Ali and Wang, Bangju and Barth-Maron, Gabe and Frye, Justin and Axelsson, Natalie and Zhu, Tao and Ma, Yukun and Giannoumis, Irene and Sedghi, Hanie and Ye, Chang and Luan, Yi and Aydin, Kevin and Chandra, Bilva and Sampathkumar, Vivek and Huang, Ronny and Lavrenko, Victor and Eleryan, Ahmed and Hong, Zhi and Hansen, Steven and Carthy, Sara Mc and Samanta, Bidisha and Ćevid, Domagoj and Wang, Xin and Li, Fangtao and Voznesensky, Michael and Hoffman, Matt and Terzis, Andreas and Sehwag, Vikash and Fidel, Gil and He, Luheng and Cai, Mu and He, Yanzhang and Feng, Alex and Nikoltchev, Martin and Phatale, Samrat and Chase, Jason and Lawton, Rory and Zhang, Ming and Ouyang, Tom and Tragut, Manuel and Manshadi, Mehdi Hafezi and Narayanan, Arjun and Shen, Jiaming and Gao, Xu and Bolukbasi, Tolga and Roy, Nick and Li, Xin and Golovin, Daniel and Panait, Liviu and Qin, Zhen and Han, Guangxing and Anthony, Thomas and Kudugunta, Sneha and Patraucean, Viorica and Ray, Aniket and Chen, Xinyun and Yang, Xiaochen and Bhatia, Tanuj and Talluri, Pranav and Morris, Alex and Ražnatović, Andrija and Brownfield, Bethanie and An, James and Peng, Sheng and Kane, Patrick and Zheng, Ce and Duduta, Nico and Kessinger, Joshua and Noraky, James and Liu, Siqi and Rong, Keran and Veličković, Petar and Rush, Keith and Goldin, Alex and Wei, Fanny and Garlapati, Shiva Mohan Reddy and Pantofaru, Caroline and Kwon, Okwan and Ni, Jianmo and Noland, Eric and Trapani, Julia Di and Beaufays, Françoise and Roy, Abhijit Guha and Chow, Yinlam and Turker, Aybuke and Cideron, Geoffrey and Mei, Lantao and Clark, Jon and Dou, Qingyun and Bošnjak, Matko and Leith, Ralph and Du, Yuqing and Yazdanbakhsh, Amir and Nasr, Milad and Kwak, Chester and Sheth, Suraj Satishkumar and Kaskasoli, Alex and Anand, Ankesh and Lakshminarayanan, Balaji and Jerome, Sammy and Bieber, David and Chu, Chun-Te and Senges, Alexandre and Shen, Tianxiao and Sridhar, Mukund and Ndebele, Ndaba and Beyret, Benjamin and Mohamed, Shakir and Chen, Mia and Freitag, Markus and Guo, Jiaxian and Liu, Luyang and Roit, Paul and Chen, Heng and Yan, Shen and Stone, Tom and Co-Reyes, J. D. and Cole, Jeremy and Scellato, Salvatore and Azizi, Shekoofeh and Hashemi, Hadi and Jin, Alicia and Iyer, Anand and Valentine, Marcella and György, András and Ahuja, Arun and Diaz, Daniel Hernandez and Lee, Chen-Yu and Clement, Nathan and Kong, Weize and Garmon, Drew and Watts, Ishaan and Bhatia, Kush and Gupta, Khyatti and Miecnikowski, Matt and Vallet, Hugo and Taly, Ankur and Loper, Edward and Joshi, Saket and Atwood, James and Chick, Jo and Collier, Mark and Iliopoulos, Fotis and Trostle, Ryan and Gunel, Beliz and Leal-Cavazos, Ramiro and Hrafnkelsson, Arnar Mar and Guzman, Michael and Ju, Xiaoen and Forbes, Andy and Emond, Jesse and Chauhan, Kushal and Caine, Ben and Xiao, Li and Zeng, Wenjun and Moufarek, Alexandre and Murphy, Daniel and Meng, Maya and Gupta, Nitish and Riedel, Felix and Das, Anil and Lawal, Elijah and Narayan, Shashi and Sosea, Tiberiu and Swirhun, James and Friso, Linda and Neyshabur, Behnam and Lu, Jing and Girgin, Sertan and Wunder, Michael and Yvinec, Edouard and Pyne, Aroonalok and Carbune, Victor and Rijhwani, Shruti and Guo, Yang and Doshi, Tulsee and Briukhov, Anton and Bain, Max and Hitron, Ayal and Wang, Xuanhui and Gupta, Ashish and Chen, Ke and Du, Cosmo and Zhang, Weiyang and Shah, Dhruv and Akula, Arjun and Dylla, Max and Kachra, Ashyana and Kuo, Weicheng and Zou, Tingting and Wang, Lily and Xu, Luyao and Zhu, Jifan and Snyder, Justin and Menon, Sachit and Firat, Orhan and Mordatch, Igor and Yuan, Yuan and Ponomareva, Natalia and Blevins, Rory and Moore, Lawrence and Wang, Weijun and Chen, Phil and Scholz, Martin and Dwornik, Artur and Lin, Jason and Li, Sicheng and Antognini, Diego and I, Te and Song, Xiaodan and Miller, Matt and Kalra, Uday and Raveret, Adam and Akerlund, Oscar and Wu, Felix and Nystrom, Andrew and Godbole, Namrata and Liu, Tianqi and DeBalsi, Hannah and Zhao, Jewel and Liu, Buhuang and Caciularu, Avi and Lax, Lauren and Khandelwal, Urvashi and Langston, Victoria and Bailey, Eric and Lattanzi, Silvio and Wang, Yufei and Kovelamudi, Neel and Mondal, Sneha and Guruganesh, Guru and Hua, Nan and Roval, Ofir and Wesołowski, Paweł and Ingale, Rishikesh and Halcrow, Jonathan and Sohn, Tim and Angermueller, Christof and Raad, Bahram and Stickgold, Eli and Lu, Eva and Kosik, Alec and Xie, Jing and Lillicrap, Timothy and Huang, Austin and Zhang, Lydia Lihui and Paulus, Dominik and Farabet, Clement and Wertheim, Alex and Wang, Bing and Joshi, Rishabh and Ko, Chu-ling and Wu, Yonghui and Agrawal, Shubham and Lin, Lily and Sheng, XiangHai and Sung, Peter and Breland-King, Tyler and Butterfield, Christina and Gawde, Swapnil and Singh, Sumeet and Zhang, Qiao and Apte, Raj and Shetty, Shilpa and Hutter, Adrian and Li, Tao and Salesky, Elizabeth and Lebron, Federico and Kanerva, Jonni and Paganini, Michela and Nguyen, Arthur and Vallu, Rohith and Peter, Jan-Thorsten and Velury, Sarmishta and Kao, David and Hoover, Jay and Bortsova, Anna and Bishop, Colton and Jakobovits, Shoshana and Agostini, Alessandro and Agarwal, Alekh and Liu, Chang and Kwong, Charles and Tavakkol, Sasan and Bica, Ioana and Greve, Alex and GP, Anirudh and Marcus, Jake and Hou, Le and Duerig, Tom and Moroshko, Rivka and Lacey, Dave and Davis, Andy and Amelot, Julien and Wang, Guohui and Kim, Frank and Strinopoulos, Theofilos and Wan, Hui and Lan, Charline Le and Krishnan, Shankar and Tang, Haotian and Humphreys, Peter and Bai, Junwen and Shtacher, Idan Heimlich and Machado, Diego and Pang, Chenxi and Burke, Ken and Liu, Dangyi and Aravamudhan, Renga and Song, Yue and Hirst, Ed and Singh, Abhimanyu and Jou, Brendan and Bai, Liang and Piccinno, Francesco and Fu, Chuyuan Kelly and Alazard, Robin and Meiri, Barak and Winter, Daniel and Chen, Charlie and Zhang, Mingda and Heitkaemper, Jens and Lambert, John and Lee, Jinhyuk and Frömmgen, Alexander and Rogulenko, Sergey and Nair, Pranav and Niemczyk, Paul and Bulyenov, Anton and Xu, Bibo and Shemtov, Hadar and Zadimoghaddam, Morteza and Toropov, Serge and Wirth, Mateo and Dai, Hanjun and Gollapudi, Sreenivas and Zheng, Daniel and Kurakin, Alex and Lee, Chansoo and Bullard, Kalesha and Serrano, Nicolas and Balazevic, Ivana and Li, Yang and Schalkwyk, Johan and Murphy, Mark and Zhang, Mingyang and Sequeira, Kevin and Datta, Romina and Agrawal, Nishant and Sutton, Charles and Attaluri, Nithya and Chiang, Mencher and Farhan, Wael and Thornton, Gregory and Lin, Kate and Choma, Travis and Nguyen, Hung and Dasgupta, Kingshuk and Robinson, Dirk and Comşa, Iulia and Riley, Michael and Pillai, Arjun and Mustafa, Basil and Golan, Ben and Zandieh, Amir and Lespiau, Jean-Baptiste and Porter, Billy and Ross, David and Rajayogam, Sujeevan and Agarwal, Mohit and Venugopalan, Subhashini and Shahriari, Bobak and Yan, Qiqi and Xu, Hao and Tobin, Taylor and Dubov, Pavel and Shi, Hongzhi and Recasens, Adrià and Kovsharov, Anton and Borgeaud, Sebastian and Dery, Lucio and Vasanth, Shanthal and Gribovskaya, Elena and Qiu, Linhai and Mahdieh, Mahdis and Skut, Wojtek and Nielsen, Elizabeth and Zheng, C. J. and Yu, Adams and Bostock, Carrie Grimes and Gupta, Shaleen and Archer, Aaron and Rawles, Chris and Davies, Elinor and Svyatkovskiy, Alexey and Tsai, Tomy and Halpern, Yoni and Reisswig, Christian and Wydrowski, Bartek and Chang, Bo and Puigcerver, Joan and Taege, Mor Hazan and Li, Jian and Schnider, Eva and Li, Xinjian and Dena, Dragos and Xu, Yunhan and Telang, Umesh and Shi, Tianze and Zen, Heiga and Kastner, Kyle and Ko, Yeongil and Subramaniam, Neesha and Kumar, Aviral and Blois, Pete and Dai, Zhuyun and Wieting, John and Lu, Yifeng and Zeldes, Yoel and Xie, Tian and Hauth, Anja and Ţifrea, Alexandru and Li, Yuqi and El-Husseini, Sam and Abolafia, Dan and Zhou, Howard and Ding, Wen and Ghalebikesabi, Sahra and Guía, Carlos and Maksai, Andrii and Weisz, Ágoston and Arik, Sercan and Sukhanov, Nick and Świetlik, Aga and Jia, Xuhui and Yu, Luo and Wang, Weiyue and Brand, Mark and Bloxwich, Dawn and Kirmani, Sean and Chen, Zhe and Go, Alec and Sprechmann, Pablo and Kannen, Nithish and Carin, Alen and Sandhu, Paramjit and Edkins, Isabel and Nooteboom, Leslie and Gupta, Jai and Maggiore, Loren and Azizi, Javad and Pritch, Yael and Yin, Pengcheng and Gupta, Mansi and Tarlow, Danny and Smith, Duncan and Ivanov, Desi and Babaeizadeh, Mohammad and Goel, Ankita and Kambala, Satish and Chu, Grace and Kastelic, Matej and Liu, Michelle and Soltau, Hagen and Stone, Austin and Agrawal, Shivani and Kim, Min and Soparkar, Kedar and Tadepalli, Srinivas and Bunyan, Oskar and Soh, Rachel and Kannan, Arvind and Kim, D. Y. and Chen, Blake JianHang and Halumi, Afief and Roy, Sudeshna and Wang, Yulong and Sercinoglu, Olcan and Gibson, Gena and Bhatnagar, Sijal and Sano, Motoki and Dincklage, Daniel von and Ren, Qingchun and Mitrevski, Blagoj and Olšák, Mirek and She, Jennifer and Doersch, Carl and Jilei and Wang and Liu, Bingyuan and Tan, Qijun and Yakar, Tamar and Warkentin, Tris and Ramirez, Alex and Lebsack, Carl and Dillon, Josh and Mathews, Rajiv and Cobley, Tom and Wu, Zelin and Chen, Zhuoyuan and Simon, Jon and Nath, Swaroop and Sainath, Tara and Bendebury, Alexei and Julian, Ryan and Mankalale, Bharath and Ćurko, Daria and Zacchello, Paulo and Brown, Adam R. and Sodhia, Kiranbir and Howard, Heidi and Caelles, Sergi and Gupta, Abhinav and Evans, Gareth and Bulanova, Anna and Katzen, Lesley and Goldenberg, Roman and Tsitsulin, Anton and Stanton, Joe and Schillings, Benoit and Kovalev, Vitaly and Fry, Corey and Shah, Rushin and Lin, Kuo and Upadhyay, Shyam and Li, Cheng and Radpour, Soroush and Maggioni, Marcello and Xiong, Jing and Haas, Lukas and Brennan, Jenny and Kamath, Aishwarya and Savinov, Nikolay and Nagrani, Arsha and Yacovone, Trevor and Kappedal, Ryan and Andriopoulos, Kostas and Lao, Li and Li, YaGuang and Rozhdestvenskiy, Grigory and Hashimoto, Kazuma and Audibert, Andrew and Austin, Sophia and Rodriguez, Daniel and Ruoss, Anian and Honke, Garrett and Karkhanis, Deep and Xiong, Xi and Wei, Qing and Huang, James and Leng, Zhaoqi and Premachandran, Vittal and Bileschi, Stan and Evangelopoulos, Georgios and Mensink, Thomas and Pavagadhi, Jay and Teplyashin, Denis and Chang, Paul and Xue, Linting and Tanzer, Garrett and Goldman, Sally and Patel, Kaushal and Li, Shixin and Wiesner, Jeremy and Zheng, Ivy and Stewart-Binks, Ian and Han, Jie and Li, Zhi and Luo, Liangchen and Lenc, Karel and Lučić, Mario and Xue, Fuzhao and Mullins, Ryan and Guseynov, Alexey and Chang, Chung-Ching and Galatzer-Levy, Isaac and Zhang, Adam and Bingham, Garrett and Hu, Grace and Hartman, Ale and Ma, Yue and Griffith, Jordan and Irpan, Alex and Radebaugh, Carey and Yue, Summer and Fan, Lijie and Ungureanu, Victor and Sorokin, Christina and Teufel, Hannah and Li, Peiran and Anil, Rohan and Paparas, Dimitris and Wang, Todd and Lin, Chu-Cheng and Peng, Hui and Shum, Megan and Petrovic, Goran and Brady, Demetra and Nguyen, Richard and Macherey, Klaus and Li, Zhihao and Singh, Harman and Yenugula, Madhavi and Iinuma, Mariko and Chen, Xinyi and Kopparapu, Kavya and Stern, Alexey and Dave, Shachi and Thekkath, Chandu and Perot, Florence and Kumar, Anurag and Li, Fangda and Xiao, Yang and Bilotti, Matthew and Bateni, Mohammad Hossein and Noble, Isaac and Lee, Lisa and Vázquez-Reina, Amelio and Salazar, Julian and Yang, Xiaomeng and Wang, Boyu and Gruzewska, Ela and Rao, Anand and Raghuram, Sindhu and Xu, Zheng and Ben-David, Eyal and Mei, Jieru and Dalmia, Sid and Zhang, Zhaoyi and Liu, Yuchen and Bansal, Gagan and Pankov, Helena and Schwarcz, Steven and Burns, Andrea and Chan, Christine and Sanghai, Sumit and Liang, Ricky and Liang, Ethan and He, Antoine and Stuart, Amy and Narayanan, Arun and Zhu, Yukun and Frank, Christian and Fatemi, Bahar and Sabne, Amit and Lang, Oran and Bhattacharya, Indro and Settle, Shane and Wang, Maria and McMahan, Brendan and Tacchetti, Andrea and Soares, Livio Baldini and Hadian, Majid and Cabi, Serkan and Chung, Timothy and Putikhin, Nikita and Li, Gang and Chen, Jeremy and Tarango, Austin and Michalewski, Henryk and Kazemi, Mehran and Masoom, Hussain and Sheftel, Hila and Shivanna, Rakesh and Vadali, Archita and Comanescu, Ramona and Reid, Doug and Moore, Joss and Neelakantan, Arvind and Sander, Michaël and Herzig, Jonathan and Rosenberg, Aviv and Dehghani, Mostafa and Choi, J. D. and Fink, Michael and Hayes, Reid and Ge, Eric and Weng, Shitao and Ho, Chia-Hua and Karro, John and Krishna, Kalpesh and Thiet, Lam Nguyen and Skerry-Ryan, Amy and Eppens, Daniel and Andreetto, Marco and Sarma, Navin and Bonacina, Silvano and Ayan, Burcu Karagol and Nawhal, Megha and Shan, Zhihao and Dusenberry, Mike and Thakoor, Shantanu and Gubbi, Sagar and Nguyen, Duc Dung and Tsarfaty, Reut and Albanie, Samuel and Mitrović, Jovana and Gandhi, Meet and Chen, Bo-Juen and Epasto, Alessandro and Stephanov, Georgi and Jin, Ye and Gehman, Samuel and Amini, Aida and Weber, Jack and Behbahani, Feryal and Xu, Shawn and Allamanis, Miltos and Chen, Xi and Ott, Myle and Sha, Claire and Jastrzebski, Michal and Qi, Hang and Greene, David and Wu, Xinyi and Toki, Abodunrinwa and Vlasic, Daniel and Shapiro, Jane and Kotikalapudi, Ragha and Shen, Zhe and Saeki, Takaaki and Xie, Sirui and Cassirer, Albin and Bharadwaj, Shikhar and Kiyono, Tatsuya and Bhojanapalli, Srinadh and Rosenfeld, Elan and Ritter, Sam and Mao, Jieming and Oliveira, João Gabriel and Egyed, Zoltan and Bandemer, Bernd and Parisotto, Emilio and Kinoshita, Keisuke and Pluto, Juliette and Maniatis, Petros and Li, Steve and Guo, Yaohui and Ghiasi, Golnaz and Tarbouriech, Jean and Chatterjee, Srimon and Jin, Julie and Katrina and Xu and Palomaki, Jennimaria and Arnold, Séb and Sewak, Madhavi and Piccinini, Federico and Sharma, Mohit and Albrecht, Ben and Purser-haskell, Sean and Vaswani, Ashwin and Chen, Chongyan and Wisniewski, Matheus and Cao, Qin and Aslanides, John and Phu, Nguyet Minh and Sieb, Maximilian and Agubuzu, Lauren and Zheng, Anne and Sohn, Daniel and Selvi, Marco and Andreassen, Anders and Subudhi, Krishan and Eruvbetine, Prem and Woodman, Oliver and Mery, Tomas and Krause, Sebastian and Ren, Xiaoqi and Ma, Xiao and Luo, Jincheng and Chen, Dawn and Fan, Wei and Griffiths, Henry and Schuler, Christian and Li, Alice and Zhang, Shujian and Sarr, Jean-Michel and Luo, Shixin and Patana, Riccardo and Watson, Matthew and Naboulsi, Dani and Collins, Michael and Sidhwani, Sailesh and Hoogeboom, Emiel and Silver, Sharon and Caveness, Emily and Zhao, Xiaokai and Rodriguez, Mikel and Deines, Maxine and Bai, Libin and Griffin, Patrick and Tagliasacchi, Marco and Xue, Emily and Babbula, Spandana Raj and Pang, Bo and Ding, Nan and Shen, Gloria and Peake, Elijah and Crocker, Remi and Raghvendra, Shubha Srinivas and Swisher, Danny and Han, Woohyun and Singh, Richa and Wu, Ling and Pchelin, Vladimir and Munkhdalai, Tsendsuren and Alon, Dana and Bacon, Geoff and Robles, Efren and Bulian, Jannis and Johnson, Melvin and Powell, George and Ferreira, Felipe Tiengo and Li, Yaoyiran and Benzing, Frederik and Velimirović, Mihajlo and Soyer, Hubert and Kong, William and Tony and Nguyên and Yang, Zhen and Liu, Jeremiah and Amersfoort, Joost van and Gillick, Daniel and Sun, Baochen and Rauschmayr, Nathalie and Zhang, Katie and Zhan, Serena and Zhou, Tao and Frolov, Alexey and Yang, Chengrun and Vnukov, Denis and Rouillard, Louis and Li, Hongji and Mandhane, Amol and Fallen, Nova and Venkataraman, Rajesh and Hu, Clara Huiyi and Brennan, Jennifer and Lee, Jenny and Chang, Jerry and Sundermeyer, Martin and Pan, Zhufeng and Ke, Rosemary and Tong, Simon and Fabrikant, Alex and Bono, William and Gu, Jindong and Foley, Ryan and Mao, Yiran and Delakis, Manolis and Bhaswar, Dhruva and Frostig, Roy and Li, Nick and Zipori, Avital and Hope, Cath and Kozlova, Olga and Mishra, Swaroop and Djolonga, Josip and Schiff, Craig and Merey, Majd Al and Briakou, Eleftheria and Morgan, Peter and Wan, Andy and Hassidim, Avinatan and Skerry-Ryan, R. J. and Sengupta, Kuntal and Jasarevic, Mary and Kallakuri, Praveen and Kunkle, Paige and Brennan, Hannah and Lieber, Tom and Mansoor, Hassan and Walker, Julian and Zhang, Bing and Xie, Annie and Žužić, Goran and Chukwuka, Adaeze and Druinsky, Alex and Cho, Donghyun and Yao, Rui and Naeem, Ferjad and Butt, Shiraz and Kim, Eunyoung and Jia, Zhipeng and Jordan, Mandy and Lelkes, Adam and Kurzeja, Mark and Wang, Sophie and Zhao, James and Over, Andrew and Chakladar, Abhishek and Prasetya, Marcel and Jha, Neha and Ganapathy, Sriram and Cong, Yale and Shroff, Prakash and Saroufim, Carl and Miryoosefi, Sobhan and Hammad, Mohamed and Nasir, Tajwar and Xi, Weijuan and Gao, Yang and Maeng, Young and Hora, Ben and Cheng, Chin-Yi and Haghani, Parisa and Lewenberg, Yoad and Lu, Caden and Matysiak, Martin and Raisinghani, Naina and Wang, Huiyu and Baugher, Lexi and Sukthankar, Rahul and Giang, Minh and Schultz, John and Fiedel, Noah and Chen, Minmin and Lee, Cheng-Chun and Dey, Tapomay and Zheng, Hao and Paul, Shachi and Smith, Celine and Ly, Andy and Wang, Yicheng and Bansal, Rishabh and Perz, Bartek and Ricco, Susanna and Blank, Stasha and Keshava, Vaishakh and Sharma, Deepak and Chow, Marvin and Lad, Kunal and Jalan, Komal and Osindero, Simon and Swanson, Craig and Scott, Jacob and Ilić, Anastasija and Li, Xiaowei and Jonnalagadda, Siddhartha Reddy and Soudagar, Afzal Shama and Xiong, Yan and Batsaikhan, Bat-Orgil and Jarrett, Daniel and Kumar, Naveen and Shah, Maulik and Lawlor, Matt and Waters, Austin and Graham, Mark and May, Rhys and Ramos, Sabela and Lefdal, Sandra and Cankara, Zeynep and Cano, Nacho and O'Donoghue, Brendan and Borovik, Jed and Liu, Frederick and Grimstad, Jordan and Alnahlawi, Mahmoud and Tsihlas, Katerina and Hudson, Tom and Grigorev, Nikolai and Jia, Yiling and Huang, Terry and Igwe, Tobenna Peter and Lebedev, Sergei and Tang, Xiaodan and Krivokon, Igor and Garcia, Frankie and Tan, Melissa and Jia, Eric and Stys, Peter and Vashishth, Shikhar and Liang, Yu and Venkatraman, Balaji and Gu, Chenjie and Kementsietsidis, Anastasios and Zhu, Chen and Jung, Junehyuk and Bai, Yunfei and Hosseini, Mohammad Javad and Ahmed, Faruk and Gupta, Aditya and Yuan, Xin and Ashraf, Shereen and Nigam, Shitij and Vasudevan, Gautam and Awasthi, Pranjal and Gilady, Adi Mayrav and Mariet, Zelda and Eskander, Ramy and Li, Haiguang and Hu, Hexiang and Garrido, Guillermo and Schlattner, Philippe and Zhang, George and Saxena, Rohun and Dević, Petar and Muralidharan, Kritika and Murthy, Ashwin and Zhou, Yiqian and Choi, Min and Wongpanich, Arissa and Wang, Zhengdong and Shah, Premal and Xu, Yuntao and Huang, Yiling and Spencer, Stephen and Chen, Alice and Cohan, James and Wang, Junjie and Tompson, Jonathan and Wu, Junru and Haroun, Ruba and Li, Haiqiong and Huergo, Blanca and Yang, Fan and Yin, Tongxin and Wendt, James and Bendersky, Michael and Chaabouni, Rahma and Snaider, Javier and Ferret, Johan and Jindal, Abhishek and Thompson, Tara and Xue, Andrew and Bishop, Will and Phal, Shubham Milind and Sharma, Archit and Sung, Yunhsuan and Radhakrishnan, Prabakar and Shomrat, Mo and Ingle, Reeve and Vij, Roopali and Gilmer, Justin and Istin, Mihai Dorin and Sobell, Sam and Lu, Yang and Nottage, Emily and Sadigh, Dorsa and Willcock, Jeremiah and Zhang, Tingnan and Xu, Steve and Brown, Sasha and Lee, Katherine and Wang, Gary and Zhu, Yun and Tay, Yi and Kim, Cheolmin and Gutierrez, Audrey and Sharma, Abhanshu and Xian, Yongqin and Seo, Sungyong and Cui, Claire and Pochernina, Elena and Baetu, Cip and Jastrzębski, Krzysztof and Ly, Mimi and Elhawaty, Mohamed and Suh, Dan and Sezener, Eren and Wang, Pidong and Yuen, Nancy and Tucker, George and Cai, Jiahao and Yang, Zuguang and Wang, Cindy and Muzio, Alex and Qian, Hai and Yoo, Jae and Lockhart, Derek and McKee, Kevin R. and Guo, Mandy and Mehrotra, Malika and Mendonça, Artur and Mehta, Sanket Vaibhav and Ben, Sherry and Tekur, Chetan and Mu, Jiaqi and Zhu, Muye and Krakovna, Victoria and Lee, Hongrae and Maschinot, A. J. and Cevey, Sébastien and Choe, HyunJeong and Bai, Aijun and Srinivasan, Hansa and Gasaway, Derek and Young, Nick and Siegler, Patrick and Holtmann-Rice, Dan and Piratla, Vihari and Baumli, Kate and Yogev, Roey and Hofer, Alex and Hasselt, Hado van and Grant, Svetlana and Chervonyi, Yuri and Silver, David and Hogue, Andrew and Agarwal, Ayushi and Wang, Kathie and Singh, Preeti and Flynn, Four and Lipschultz, Josh and David, Robert and Bellot, Lizzetth and Yang, Yao-Yuan and Le, Long and Graziano, Filippo and Olszewska, Kate and Hui, Kevin and Maurya, Akanksha and Parotsidis, Nikos and Chen, Weijie and Oguntebi, Tayo and Kelley, Joe and Baddepudi, Anirudh and Mauerer, Johannes and Shaw, Gregory and Siegman, Alex and Yang, Lin and Shetty, Shravya and Roy, Subhrajit and Song, Yunting and Stokowiec, Wojciech and Burnell, Ryan and Savant, Omkar and Busa-Fekete, Robert and Miao, Jin and Ghosh, Samrat and MacDermed, Liam and Lippe, Phillip and Dektiarev, Mikhail and Behrman, Zach and Mentzer, Fabian and Nguyen, Kelvin and Wei, Meng and Verma, Siddharth and Knutsen, Chris and Dasari, Sudeep and Yan, Zhipeng and Mitrichev, Petr and Wang, Xingyu and Shejwalkar, Virat and Austin, Jacob and Sunkara, Srinivas and Potti, Navneet and Virin, Yan and Wright, Christian and Liu, Gaël and Riva, Oriana and Pot, Etienne and Kochanski, Greg and Le, Quoc and Balasubramaniam, Gargi and Dhar, Arka and Liao, Yuguo and Bloniarz, Adam and Shukla, Divyansh and Cole, Elizabeth and Lee, Jong and Zhang, Sheng and Kafle, Sushant and Vashishtha, Siddharth and Mahmoudieh, Parsa and Chen, Grace and Hoffmann, Raphael and Srinivasan, Pranesh and Lago, Agustin Dal and Shalom, Yoav Ben and Wang, Zi and Elabd, Michael and Sharma, Anuj and Oh, Junhyuk and Kothawade, Suraj and Le, Maigo and Monteiro, Marianne and Yang, Shentao and Alarakyia, Kaiz and Geirhos, Robert and Mincu, Diana and Garnes, Håvard and Kobayashi, Hayato and Mariooryad, Soroosh and Krasowiak, Kacper and Zhixin and Lai and Mourad, Shibl and Wang, Mingqiu and Bu, Fan and Aharoni, Ophir and Chen, Guanjie and Goyal, Abhimanyu and Zubov, Vadim and Bapna, Ankur and Dabir, Elahe and Kothari, Nisarg and Lamerigts, Kay and Cao, Nicola De and Shar, Jeremy and Yew, Christopher and Kulkarni, Nitish and Mahaarachchi, Dre and Joshi, Mandar and Zhu, Zhenhai and Lichtarge, Jared and Zhou, Yichao and Muckenhirn, Hannah and Selo, Vittorio and Vinyals, Oriol and Chen, Peter and Brohan, Anthony and Mehta, Vaibhav and Cogan, Sarah and Wang, Ruth and Geri, Ty and Ko, Wei-Jen and Chen, Wei and Viola, Fabio and Shivam, Keshav and Wang, Lisa and Elish, Madeleine Clare and Popa, Raluca Ada and Pereira, Sébastien and Liu, Jianqiao and Koster, Raphael and Kim, Donnie and Zhang, Gufeng and Ebrahimi, Sayna and Talukdar, Partha and Zheng, Yanyan and Poklukar, Petra and Mikhalap, Ales and Johnson, Dale and Vijayakumar, Anitha and Omernick, Mark and Dibb, Matt and Dubey, Ayush and Hu, Qiong and Suman, Apurv and Aggarwal, Vaibhav and Kornakov, Ilya and Xia, Fei and Lowe, Wing and Kolganov, Alexey and Xiao, Ted and Nikolaev, Vitaly and Hemingray, Steven and Li, Bonnie and Iljazi, Joana and Rybiński, Mikołaj and Sandhu, Ballie and Lu, Peggy and Luong, Thang and Jenatton, Rodolphe and Govindaraj, Vineetha and Hui and Li and Dulac-Arnold, Gabriel and Park, Wonpyo and Wang, Henry and Modi, Abhinit and Pouget-Abadie, Jean and Greller, Kristina and Gupta, Rahul and Berry, Robert and Ramachandran, Prajit and Xie, Jinyu and McCafferty, Liam and Wang, Jianling and Gupta, Kilol and Lim, Hyeontaek and Bratanič, Blaž and Brock, Andy and Akolzin, Ilia and Sproch, Jim and Karliner, Dan and Kim, Duhyeon and Goedeckemeyer, Adrian and Shazeer, Noam and Schmid, Cordelia and Calandriello, Daniele and Bhatia, Parul and Choromanski, Krzysztof and Montgomery, Ceslee and Dua, Dheeru and Ramalho, Ana and King, Helen and Gao, Yue and Nguyen, Lynn and Lindner, David and Pitta, Divya and Johnson, Oleaser and Salama, Khalid and Ardila, Diego and Han, Michael and Farnese, Erin and Odoom, Seth and Wang, Ziyue and Ding, Xiangzhuo and Rink, Norman and Smith, Ray and Lehri, Harshal Tushar and Cohen, Eden and Vats, Neera and He, Tong and Gopavarapu, Parthasarathy and Paszke, Adam and Patel, Miteyan and Gansbeke, Wouter Van and Loher, Lucia and Castro, Luis and Voitovich, Maria and Glehn, Tamara von and George, Nelson and Niklaus, Simon and Eaton-Rosen, Zach and Rakićević, Nemanja and Jue, Erik and Perel, Sagi and Zhang, Carrie and Bahat, Yuval and Pouget, Angéline and Xing, Zhi and Huot, Fantine and Shenoy, Ashish and Bos, Taylor and Coriou, Vincent and Richter, Bryan and Noy, Natasha and Wang, Yaqing and Ontanon, Santiago and Qin, Siyang and Makarchuk, Gleb and Hassabis, Demis and Li, Zhuowan and Sharma, Mandar and Venkatesan, Kumaran and Kemaev, Iurii and Daniel, Roxanne and Huang, Shiyu and Shah, Saloni and Ponce, Octavio and Warren and Chen and Faruqui, Manaal and Wu, Jialin and Andačić, Slavica and Payrits, Szabolcs and McDuff, Daniel and Hume, Tom and Cao, Yuan and Tessler, M. H. and Wang, Qingze and Wang, Yinan and Rendulic, Ivor and Agustsson, Eirikur and Johnson, Matthew and Lando, Tanya and Howard, Andrew and Padmanabhan, Sri Gayatri Sundara and Daswani, Mayank and Banino, Andrea and Kilgore, Michael and Heek, Jonathan and Ji, Ziwei and Caceres, Alvaro and Li, Conglong and Kassner, Nora and Vlaskin, Alexey and Liu, Zeyu and Grills, Alex and Hou, Yanhan and Sukkerd, Roykrong and Cheon, Gowoon and Shetty, Nishita and Markeeva, Larisa and Stanczyk, Piotr and Iyer, Tejas and Gong, Yuan and Gao, Shawn and Gopalakrishnan, Keerthana and Blyth, Tim and Reynolds, Malcolm and Bhoopchand, Avishkar and Bilenko, Misha and Gharibian, Dero and Zayats, Vicky and Faust, Aleksandra and Singh, Abhinav and Ma, Min and Jiao, Hongyang and Vijayanarasimhan, Sudheendra and Aroyo, Lora and Yadav, Vikas and Chakera, Sarah and Kakarla, Ashwin and Meshram, Vilobh and Gregor, Karol and Botea, Gabriela and Senter, Evan and Jia, Dawei and Kovacs, Geza and Sharma, Neha and Baur, Sebastien and Kang, Kai and He, Yifan and Zhuo, Lin and Kostelac, Marija and Laish, Itay and Peng, Songyou and O'Bryan, Louis and Kasenberg, Daniel and Rao, Girish Ramchandra and Leurent, Edouard and Zhang, Biao and Stevens, Sage and Salazar, Ana and Zhang, Ye and Lobov, Ivan and Walker, Jake and Porter, Allen and Redshaw, Morgan and Ke, Han and Rao, Abhishek and Lee, Alex and Lam, Hoi and Moffitt, Michael and Kim, Jaeyoun and Qiao, Siyuan and Koo, Terry and Dadashi, Robert and Song, Xinying and Sundararajan, Mukund and Xu, Peng and Kawamoto, Chizu and Zhong, Yan and Barbu, Clara and Reddy, Apoorv and Verzetti, Mauro and Li, Leon and Papamakarios, George and Klimczak-Plucińska, Hanna and Cassin, Mary and Kavukcuoglu, Koray and Swavely, Rigel and Vaucher, Alain and Zhao, Jeffrey and Hemsley, Ross and Tschannen, Michael and Ge, Heming and Menghani, Gaurav and Yu, Yang and Ha, Natalie and He, Wei and Wu, Xiao and Song, Maggie and Sterneck, Rachel and Zinke, Stefan and Calian, Dan A. and Marsden, Annie and Ruiz, Alejandro Cruzado and Hessel, Matteo and Gueta, Almog and Lee, Benjamin and Farris, Brian and Gupta, Manish and Li, Yunjie and Saleh, Mohammad and Misra, Vedant and Xiao, Kefan and Mendolicchio, Piermaria and Buttimore, Gavin and Krayvanova, Varvara and Nayakanti, Nigamaa and Wiethoff, Matthew and Pande, Yash and Mirhoseini, Azalia and Lao, Ni and Liu, Jasmine and Hua, Yiqing and Chen, Angie and Malkov, Yury and Kalashnikov, Dmitry and Gupta, Shubham and Audhkhasi, Kartik and Zhai, Yuexiang and Kopalle, Sudhindra and Jain, Prateek and Ofek, Eran and Meyer, Clemens and Baatarsukh, Khuslen and Strejček, Hana and Qian, Jun and Freedman, James and Figueira, Ricardo and Sokolik, Michal and Bachem, Olivier and Lin, Raymond and Kharrat, Dia and Hidey, Chris and Xu, Pingmei and Duan, Dennis and Li, Yin and Ersoy, Muge and Everett, Richard and Cen, Kevin and Santamaria-Fernandez, Rebeca and Taubenfeld, Amir and Mackinnon, Ian and Deng, Linda and Zablotskaia, Polina and Viswanadha, Shashank and Goel, Shivanker and Yates, Damion and Deng, Yunxiao and Choy, Peter and Chen, Mingqing and Sinha, Abhishek and Mossin, Alex and Wang, Yiming and Szlam, Arthur and Hao, Susan and Rubenstein, Paul Kishan and Toksoz-Exley, Metin and Aperghis, Miranda and Zhong, Yin and Ahn, Junwhan and Isard, Michael and Lacombe, Olivier and Luisier, Florian and Anastasiou, Chrysovalantis and Kalley, Yogesh and Prabhu, Utsav and Dunleavy, Emma and Bijwadia, Shaan and Mao-Jones, Justin and Chen, Kelly and Pasumarthi, Rama and Wood, Emily and Dostmohamed, Adil and Hurley, Nate and Simsa, Jiri and Parrish, Alicia and Pajarskas, Mantas and Harvey, Matt and Skopek, Ondrej and Kochinski, Yony and Rey, Javier and Rieser, Verena and Zhou, Denny and Lee, Sun Jae and Acharya, Trilok and Li, Guowang and Jiang, Joe and Zhang, Xiaofan and Gipson, Bryant and Mahintorabi, Ethan and Gelmi, Marco and Khajehnouri, Nima and Yeh, Angel and Lee, Kayi and Matthey, Loic and Baker, Leslie and Pham, Trang and Fu, Han and Pak, Alex and Gupta, Prakhar and Vasconcelos, Cristina and Sadovsky, Adam and Walker, Brian and Hsiao, Sissie and Zochbauer, Patrik and Marzoca, Andreea and Velan, Noam and Zeng, Junhao and Baechler, Gilles and Driess, Danny and Jain, Divya and Huang, Yanping and Tao, Lizzie and Maggs, John and Levine, Nir and Schneider, Jon and Gemzer, Erika and Petit, Samuel and Han, Shan and Fisher, Zach and Zelle, Dustin and Biles, Courtney and Ie, Eugene and Fadeeva, Asya and Liu, Casper and Franco, Juliana Vicente and Collister, Adrian and Zhang, Hao and Wang, Renshen and Zhao, Ruizhe and Kieliger, Leandro and Shuster, Kurt and Zhu, Rui and Gong, Boqing and Chan, Lawrence and Sun, Ruoxi and Basu, Sujoy and Zimmermann, Roland and Hayes, Jamie and Bapna, Abhishek and Snoek, Jasper and Yang, Weel and Datta, Puranjay and Abdallah, Jad Al and Kilgour, Kevin and Li, Lu and Mah, S. Q. and Jun, Yennie and Rivière, Morgane and Karmarkar, Abhijit and Spalink, Tammo and Huang, Tao and Gonzalez, Lucas and Tran, Duc-Hieu and Nowak, Averi and Palowitch, John and Chadwick, Martin and Talius, Ellie and Mehta, Harsh and Sellam, Thibault and Fränken, Philipp and Nicosia, Massimo and He, Kyle and Kini, Aditya and Amos, David and Basu, Sugato and Jobe, Harrison and Shaw, Eleni and Xu, Qiantong and Evans, Colin and Ikeda, Daisuke and Yan, Chaochao and Jin, Larry and Wang, Lun and Yadav, Sachin and Labzovsky, Ilia and Sampath, Ramesh and Ma, Ada and Schumann, Candice and Siddhant, Aditya and Shah, Rohin and Youssef, John and Agarwal, Rishabh and Dabney, Natalie and Tonioni, Alessio and Ambar, Moran and Li, Jing and Guyon, Isabelle and Li, Benny and Soergel, David and Fang, Boya and Karadzhov, Georgi and Udrescu, Cristian and Trinh, Trieu and Raunak, Vikas and Noury, Seb and Guo, Dee and Gupta, Sonal and Finkelstein, Mara and Petek, Denis and Liang, Lihao and Billock, Greg and Sun, Pei and Wood, David and Song, Yiwen and Yu, Xiaobin and Matejovicova, Tatiana and Cohen, Regev and Andra, Kalyan and D'Ambrosio, David and Deng, Zhiwei and Nallatamby, Vincent and Songhori, Ebrahim and Dangovski, Rumen and Lampinen, Andrew and Botadra, Pankil and Hillier, Adam and Cao, Jiawei and Baddi, Nagabhushan and Kuncoro, Adhi and Yoshino, Toshihiro and Bhagatwala, Ankit and Ranzato, Marcáurelio and Schaeffer, Rylan and Liu, Tianlin and Ye, Shuai and Sarvana, Obaid and Nham, John and Kuang, Chenkai and Gao, Isabel and Baek, Jinoo and Mittal, Shubham and Wahid, Ayzaan and Gergely, Anita and Ni, Bin and Feldman, Josh and Muir, Carrie and Lamblin, Pascal and Macherey, Wolfgang and Dyer, Ethan and Kilpatrick, Logan and Campos, Víctor and Bhutani, Mukul and Fort, Stanislav and Ahmad, Yanif and Severyn, Aliaksei and Chatziprimou, Kleopatra and Ferludin, Oleksandr and Dimarco, Mason and Kusupati, Aditya and Heyward, Joe and Bahir, Dan and Villela, Kevin and Millican, Katie and Marcus, Dror and Bahargam, Sanaz and Unlu, Caglar and Roth, Nicholas and Wei, Zichuan and Gopal, Siddharth and Ghoshal, Deepanway and Lee, Edward and Lin, Sharon and Lees, Jennie and Lee, Dayeong and Hosseini, Anahita and Fan, Connie and Neel, Seth and Wu, Marcus and Altun, Yasemin and Cai, Honglong and Piqueras, Enrique and Woodward, Josh and Bissacco, Alessandro and Haykal, Salem and Bordbar, Mahyar and Sundaram, Prasha and Hodkinson, Sarah and Toyama, Daniel and Polovets, George and Myers, Austin and Sinha, Anu and Levinboim, Tomer and Krishnakumar, Kashyap and Chhaparia, Rachita and Sholokhova, Tatiana and Gundavarapu, Nitesh Bharadwaj and Jawahar, Ganesh and Qureshi, Haroon and Hu, Jieru and Momchev, Nikola and Rahtz, Matthew and Wu, Renjie and S, Aishwarya P. and Dhamdhere, Kedar and Guo, Meiqi and Gupta, Umang and Eslami, Ali and Schain, Mariano and Blokzijl, Michiel and Welling, David and Orr, Dave and Bolelli, Levent and Perez-Nieves, Nicolas and Sirotenko, Mikhail and Prasad, Aman and Kar, Arjun and Pigem, Borja De Balle and Terzi, Tayfun and Weisz, Gellért and Ghosh, Dipankar and Mavalankar, Aditi and Madeka, Dhruv and Daugaard, Kaspar and Adam, Hartwig and Shah, Viraj and Berman, Dana and Tran, Maggie and Baker, Steven and Andrejczuk, Ewa and Chole, Grishma and Raboshchuk, Ganna and Mirzazadeh, Mahdi and Kagohara, Thais and Wu, Shimu and Schallhart, Christian and Orlando, Bernett and Wang, Chen and Rrustemi, Alban and Xiong, Hao and Liu, Hao and Vezer, Arpi and Ramsden, Nolan and Chang, Shuo-yiin and Mudgal, Sidharth and Li, Yan and Vieillard, Nino and Hoshen, Yedid and Ahmad, Farooq and Slone, Ambrose and Hua, Amy and Potikha, Natan and Rossini, Mirko and Stritar, Jon and Prakash, Sushant and Wang, Zifeng and Dong, Xuanyi and Nazari, Alireza and Nehoran, Efrat and Tekelioglu, Kaan and Li, Yinxiao and Badola, Kartikeya and Funkhouser, Tom and Li, Yuanzhen and Yerram, Varun and Ganeshan, Ramya and Formoso, Daniel and Langner, Karol and Shi, Tian and Li, Huijian and Yamamori, Yumeya and Panda, Amayika and Saade, Alaa and Scarpati, Angelo Scorza and Breaux, Chris and Carey, C. J. and Zhou, Zongwei and Hsieh, Cho-Jui and Bridgers, Sophie and Butryna, Alena and Gupta, Nishesh and Tulsyan, Vaibhav and Woo, Sanghyun and Eltyshev, Evgenii and Grathwohl, Will and Parks, Chanel and Benjamin, Seth and Panigrahy, Rina and Dodhia, Shenil and Freitas, Daniel De and Sauer, Chris and Song, Will and Alet, Ferran and Tolins, Jackson and Paduraru, Cosmin and Zhou, Xingyi and Albert, Brian and Zhang, Zizhao and Shu, Lei and Bansal, Mudit and Nguyen, Sarah and Globerson, Amir and Xiao, Owen and Manyika, James and Hennigan, Tom and Rong, Rong and Matak, Josip and Bakalov, Anton and Sharma, Ankur and Sinopalnikov, Danila and Pierson, Andrew and Roller, Stephen and Brown, Geoff and Gao, Mingcen and Fukuzawa, Toshiyuki and Ghafouri, Amin and Vassigh, Kenny and Barr, Iain and Wang, Zhicheng and Korsun, Anna and Jayaram, Rajesh and Ren, Lijie and Zaman, Tim and Khan, Samira and Lunts, Yana and Deutsch, Dan and Uthus, Dave and Katz, Nitzan and Samsikova, Masha and Khalifa, Amr and Sethi, Nikhil and Sun, Jiao and Tang, Luming and Alon, Uri and Luo, Xianghong and Yu, Dian and Nayyar, Abhishek and Petrini, Bryce and Truong, Will and Hellendoorn, Vincent and Chinaev, Nikolai and Alberti, Chris and Wang, Wei and Hu, Jingcao and Mirrokni, Vahab and Balashankar, Ananth and Aharon, Avia and Mehta, Aahil and Iscen, Ahmet and Kready, Joseph and Manning, Lucas and Mohananey, Anhad and Chen, Yuankai and Tripathi, Anshuman and Wu, Allen and Petrovski, Igor and Hwang, Dawsen and Baeuml, Martin and Chandrakaladharan, Shreyas and Liu, Yuan and Coaguila, Rey and Chen, Maxwell and Ma, Sally and Tafti, Pouya and Tatineni, Susheel and Spitz, Terry and Ye, Jiayu and Vicol, Paul and Rosca, Mihaela and Puigdomènech, Adrià and Yahav, Zohar and Ghemawat, Sanjay and Lin, Hanzhao and Kirk, Phoebe and Nabulsi, Zaid and Brin, Sergey and Bohnet, Bernd and Caluwaerts, Ken and Veerubhotla, Aditya Srikanth and Zheng, Dan and Dai, Zihang and Petrov, Petre and Xu, Yichong and Mehran, Ramin and Xu, Zhuo and Zintgraf, Luisa and Choi, Jiho and Hombaiah, Spurthi Amba and Thoppilan, Romal and Reddi, Sashank and Lew, Lukasz and Li, Li and Webster, Kellie and Sawhney, K. P. and Lamprou, Lampros and Shakeri, Siamak and Lunayach, Mayank and Chen, Jianmin and Bagri, Sumit and Salcianu, Alex and Chen, Ying and Donchev, Yani and Magister, Charlotte and Nørly, Signe and Rodrigues, Vitor and Izo, Tomas and Noga, Hila and Zou, Joe and Köppe, Thomas and Zhou, Wenxuan and Lee, Kenton and Long, Xiangzhu and Eisenbud, Danielle and Chen, Anthony and Schenck, Connor and To, Chi Ming and Zhong, Peilin and Taropa, Emanuel and Truong, Minh and Levy, Omer and Martins, Danilo and Zhang, Zhiyuan and Semturs, Christopher and Zhang, Kelvin and Yakubovich, Alex and Moreno, Pol and McConnaughey, Lara and Lu, Di and Redmond, Sam and Weerts, Lotte and Bitton, Yonatan and Refice, Tiziana and Lacasse, Nicolas and Conmy, Arthur and Tallec, Corentin and Odell, Julian and Forbes-Pollard, Hannah and Socala, Arkadiusz and Hoech, Jonathan and Kohli, Pushmeet and Walton, Alanna and Wang, Rui and Sazanovich, Mikita and Zhu, Kexin and Kapishnikov, Andrei and Galt, Rich and Denton, Matthew and Murdoch, Ben and Sikora, Caitlin and Mohamed, Kareem and Wei, Wei and First, Uri and McConnell, Tim and Cobo, Luis C. and Qin, James and Avrahami, Thi and Balle, Daniel and Watanabe, Yu and Louis, Annie and Kraft, Adam and Ariafar, Setareh and Gu, Yiming and Rives, Eugénie and Yoon, Charles and Rusu, Andrei and Cobon-Kerr, James and Hahn, Chris and Luo, Jiaming and Yuvein and Zhu and Ahuja, Niharika and Benenson, Rodrigo and Kaufman, Raphaël Lopez and Yu, Honglin and Hightower, Lloyd and Zhang, Junlin and Ni, Darren and Hendricks, Lisa Anne and Wang, Gabby and Yona, Gal and Jain, Lalit and Barrio, Pablo and Bhupatiraju, Surya and Velusamy, Siva and Dafoe, Allan and Riedel, Sebastian and Thomas, Tara and Yuan, Zhe and Bellaiche, Mathias and Panthaplackel, Sheena and Kloboves, Klemen and Jauhari, Sarthak and Akbulut, Canfer and Davchev, Todor and Gladchenko, Evgeny and Madras, David and Chuklin, Aleksandr and Hill, Tyrone and Yuan, Quan and Madhavan, Mukundan and Leonhard, Luke and Scandinaro, Dylan and Chen, Qihang and Niu, Ning and Douillard, Arthur and Damoc, Bogdan and Onoe, Yasumasa and Pedregosa, Fabian and Bertsch, Fred and Leichner, Chas and Pagadora, Joseph and Malmaud, Jonathan and Ponda, Sameera and Twigg, Andy and Duzhyi, Oleksii and Shen, Jingwei and Wang, Miaosen and Garg, Roopal and Chen, Jing and Evci, Utku and Lee, Jonathan and Liu, Leon and Kojima, Koji and Yamaguchi, Masa and Rajendran, Arunkumar and Piergiovanni, A. J. and Rajendran, Vinodh Kumar and Fornoni, Marco and Ibagon, Gabriel and Ragan, Harry and Khan, Sadh MNM and Blitzer, John and Bunner, Andrew and Sun, Guan and Kosakai, Takahiro and Lundberg, Scott and Elue, Ndidi and Guu, Kelvin and Park, S. K. and Park, Jane and Narayanaswamy, Arunachalam and Wu, Chengda and Mudigonda, Jayaram and Cohn, Trevor and Mu, Hairong and Kumar, Ravi and Graesser, Laura and Zhang, Yichi and Killam, Richard and Zhuang, Vincent and Giménez, Mai and Jishi, Wael Al and Ley-Wild, Ruy and Zhai, Alex and Osawa, Kazuki and Cedillo, Diego and Liu, Jialu and Upadhyay, Mayank and Sieniek, Marcin and Sharma, Roshan and Paine, Tom and Angelova, Anelia and Addepalli, Sravanti and Parada, Carolina and Majumder, Kingshuk and Lamp, Avery and Kumar, Sanjiv and Deng, Xiang and Myaskovsky, Artiom and Sabolić, Tea and Dudek, Jeffrey and York, Sarah and Quitry, Félix de Chaumont and Nie, Jiazhong and Cattle, Dee and Gunjan, Alok and Piot, Bilal and Khawaja, Waleed and Bang, Seojin and Wang, Simon and Khodadadeh, Siavash and R, Raghavender and Rawlani, Praynaa and Powell, Richard and Lee, Kevin and Griesser, Johannes and Oh, G. S. and Magalhaes, Cesar and Li, Yujia and Tokumine, Simon and Vogel, Hadas Natalie and Hsu, Dennis and BC, Arturo and Jindal, Disha and Cohen, Matan and Yang, Zi and Yuan, Junwei and Cesare, Dario de and Bruguier, Tony and Xu, Jun and Roy, Monica and Jacovi, Alon and Belov, Dan and Arya, Rahul and Meadowlark, Phoenix and Cohen-Ganor, Shlomi and Ye, Wenting and Morris-Suzuki, Patrick and Banzal, Praseem and Song, Gan and Ponnuramu, Pranavaraj and Zhang, Fred and Scrivener, George and Zaiem, Salah and Rochman, Alif Raditya and Han, Kehang and Ghazi, Badih and Lee, Kate and Drath, Shahar and Suo, Daniel and Girgis, Antonious and Shenoy, Pradeep and Nguyen, Duy and Eck, Douglas and Gupta, Somit and Yan, Le and Carreira, Joao and Gulati, Anmol and Sang, Ruoxin and Mirylenka, Daniil and Cooney, Emma and Chou, Edward and Ling, Mingyang and Fan, Cindy and Coleman, Ben and Tubone, Guilherme and Kumar, Ravin and Baldridge, Jason and Hernandez-Campos, Felix and Lazaridou, Angeliki and Besley, James and Yona, Itay and Bulut, Neslihan and Wellens, Quentin and Pierigiovanni, A. J. and George, Jasmine and Green, Richard and Han, Pu and Tao, Connie and Clark, Geoff and You, Chong and Abdolmaleki, Abbas and Fu, Justin and Chen, Tongzhou and Chaugule, Ashwin and Chandorkar, Angad and Rahman, Altaf and Thompson, Will and Koanantakool, Penporn and Bernico, Mike and Ren, Jie and Vlasov, Andrey and Vassilvitskii, Sergei and Kula, Maciej and Liang, Yizhong and Kim, Dahun and Huang, Yangsibo and Ye, Chengxi and Lepikhin, Dmitry and Helmholz, Wesley},
	month = oct,
	year = {2025},
	note = {arXiv:2507.06261 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	doi = {10.48550/arXiv.2403.05530},
	abstract = {In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textgreater}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and Mariooryad, Soroosh and Ding, Yifan and Geng, Xinyang and Alcober, Fred and Frostig, Roy and Omernick, Mark and Walker, Lexi and Paduraru, Cosmin and Sorokin, Christina and Tacchetti, Andrea and Gaffney, Colin and Daruki, Samira and Sercinoglu, Olcan and Gleicher, Zach and Love, Juliette and Voigtlaender, Paul and Jain, Rohan and Surita, Gabriela and Mohamed, Kareem and Blevins, Rory and Ahn, Junwhan and Zhu, Tao and Kawintiranon, Kornraphop and Firat, Orhan and Gu, Yiming and Zhang, Yujing and Rahtz, Matthew and Faruqui, Manaal and Clay, Natalie and Gilmer, Justin and Co-Reyes, J. D. and Penchev, Ivo and Zhu, Rui and Morioka, Nobuyuki and Hui, Kevin and Haridasan, Krishna and Campos, Victor and Mahdieh, Mahdis and Guo, Mandy and Hassan, Samer and Kilgour, Kevin and Vezer, Arpi and Cheng, Heng-Tze and Liedekerke, Raoul de and Goyal, Siddharth and Barham, Paul and Strouse, D. J. and Noury, Seb and Adler, Jonas and Sundararajan, Mukund and Vikram, Sharad and Lepikhin, Dmitry and Paganini, Michela and Garcia, Xavier and Yang, Fan and Valter, Dasha and Trebacz, Maja and Vodrahalli, Kiran and Asawaroengchai, Chulayuth and Ring, Roman and Kalb, Norbert and Soares, Livio Baldini and Brahma, Siddhartha and Steiner, David and Yu, Tianhe and Mentzer, Fabian and He, Antoine and Gonzalez, Lucas and Xu, Bibo and Kaufman, Raphael Lopez and Shafey, Laurent El and Oh, Junhyuk and Hennigan, Tom and Driessche, George van den and Odoom, Seth and Lucic, Mario and Roelofs, Becca and Lall, Sid and Marathe, Amit and Chan, Betty and Ontanon, Santiago and He, Luheng and Teplyashin, Denis and Lai, Jonathan and Crone, Phil and Damoc, Bogdan and Ho, Lewis and Riedel, Sebastian and Lenc, Karel and Yeh, Chih-Kuan and Chowdhery, Aakanksha and Xu, Yang and Kazemi, Mehran and Amid, Ehsan and Petrushkina, Anastasia and Swersky, Kevin and Khodaei, Ali and Chen, Gowoon and Larkin, Chris and Pinto, Mario and Yan, Geng and Badia, Adria Puigdomenech and Patil, Piyush and Hansen, Steven and Orr, Dave and Arnold, Sebastien M. R. and Grimstad, Jordan and Dai, Andrew and Douglas, Sholto and Sinha, Rishika and Yadav, Vikas and Chen, Xi and Gribovskaya, Elena and Austin, Jacob and Zhao, Jeffrey and Patel, Kaushal and Komarek, Paul and Austin, Sophia and Borgeaud, Sebastian and Friso, Linda and Goyal, Abhimanyu and Caine, Ben and Cao, Kris and Chung, Da-Woon and Lamm, Matthew and Barth-Maron, Gabe and Kagohara, Thais and Olszewska, Kate and Chen, Mia and Shivakumar, Kaushik and Agarwal, Rishabh and Godhia, Harshal and Rajwar, Ravi and Snaider, Javier and Dotiwalla, Xerxes and Liu, Yuan and Barua, Aditya and Ungureanu, Victor and Zhang, Yuan and Batsaikhan, Bat-Orgil and Wirth, Mateo and Qin, James and Danihelka, Ivo and Doshi, Tulsee and Chadwick, Martin and Chen, Jilin and Jain, Sanil and Le, Quoc and Kar, Arjun and Gurumurthy, Madhu and Li, Cheng and Sang, Ruoxin and Liu, Fangyu and Lamprou, Lampros and Munoz, Rich and Lintz, Nathan and Mehta, Harsh and Howard, Heidi and Reynolds, Malcolm and Aroyo, Lora and Wang, Quan and Blanco, Lorenzo and Cassirer, Albin and Griffith, Jordan and Das, Dipanjan and Lee, Stephan and Sygnowski, Jakub and Fisher, Zach and Besley, James and Powell, Richard and Ahmed, Zafarali and Paulus, Dominik and Reitter, David and Borsos, Zalan and Joshi, Rishabh and Pope, Aedan and Hand, Steven and Selo, Vittorio and Jain, Vihan and Sethi, Nikhil and Goel, Megha and Makino, Takaki and May, Rhys and Yang, Zhen and Schalkwyk, Johan and Butterfield, Christina and Hauth, Anja and Goldin, Alex and Hawkins, Will and Senter, Evan and Brin, Sergey and Woodman, Oliver and Ritter, Marvin and Noland, Eric and Giang, Minh and Bolina, Vijay and Lee, Lisa and Blyth, Tim and Mackinnon, Ian and Reid, Machel and Sarvana, Obaid and Silver, David and Chen, Alexander and Wang, Lily and Maggiore, Loren and Chang, Oscar and Attaluri, Nithya and Thornton, Gregory and Chiu, Chung-Cheng and Bunyan, Oskar and Levine, Nir and Chung, Timothy and Eltyshev, Evgenii and Si, Xiance and Lillicrap, Timothy and Brady, Demetra and Aggarwal, Vaibhav and Wu, Boxi and Xu, Yuanzhong and McIlroy, Ross and Badola, Kartikeya and Sandhu, Paramjit and Moreira, Erica and Stokowiec, Wojciech and Hemsley, Ross and Li, Dong and Tudor, Alex and Shyam, Pranav and Rahimtoroghi, Elahe and Haykal, Salem and Sprechmann, Pablo and Zhou, Xiang and Mincu, Diana and Li, Yujia and Addanki, Ravi and Krishna, Kalpesh and Wu, Xiao and Frechette, Alexandre and Eyal, Matan and Dafoe, Allan and Lacey, Dave and Whang, Jay and Avrahami, Thi and Zhang, Ye and Taropa, Emanuel and Lin, Hanzhao and Toyama, Daniel and Rutherford, Eliza and Sano, Motoki and Choe, HyunJeong and Tomala, Alex and Safranek-Shrader, Chalence and Kassner, Nora and Pajarskas, Mantas and Harvey, Matt and Sechrist, Sean and Fortunato, Meire and Lyu, Christina and Elsayed, Gamaleldin and Kuang, Chenkai and Lottes, James and Chu, Eric and Jia, Chao and Chen, Chih-Wei and Humphreys, Peter and Baumli, Kate and Tao, Connie and Samuel, Rajkumar and Santos, Cicero Nogueira dos and Andreassen, Anders and Rakićević, Nemanja and Grewe, Dominik and Kumar, Aviral and Winkler, Stephanie and Caton, Jonathan and Brock, Andrew and Dalmia, Sid and Sheahan, Hannah and Barr, Iain and Miao, Yingjie and Natsev, Paul and Devlin, Jacob and Behbahani, Feryal and Prost, Flavien and Sun, Yanhua and Myaskovsky, Artiom and Pillai, Thanumalayan Sankaranarayana and Hurt, Dan and Lazaridou, Angeliki and Xiong, Xi and Zheng, Ce and Pardo, Fabio and Li, Xiaowei and Horgan, Dan and Stanton, Joe and Ambar, Moran and Xia, Fei and Lince, Alejandro and Wang, Mingqiu and Mustafa, Basil and Webson, Albert and Lee, Hyo and Anil, Rohan and Wicke, Martin and Dozat, Timothy and Sinha, Abhishek and Piqueras, Enrique and Dabir, Elahe and Upadhyay, Shyam and Boral, Anudhyan and Hendricks, Lisa Anne and Fry, Corey and Djolonga, Josip and Su, Yi and Walker, Jake and Labanowski, Jane and Huang, Ronny and Misra, Vedant and Chen, Jeremy and Skerry-Ryan, R. J. and Singh, Avi and Rijhwani, Shruti and Yu, Dian and Castro-Ros, Alex and Changpinyo, Beer and Datta, Romina and Bagri, Sumit and Hrafnkelsson, Arnar Mar and Maggioni, Marcello and Zheng, Daniel and Sulsky, Yury and Hou, Shaobo and Paine, Tom Le and Yang, Antoine and Riesa, Jason and Rogozinska, Dominika and Marcus, Dror and Badawy, Dalia El and Zhang, Qiao and Wang, Luyu and Miller, Helen and Greer, Jeremy and Sjos, Lars Lowe and Nova, Azade and Zen, Heiga and Chaabouni, Rahma and Rosca, Mihaela and Jiang, Jiepu and Chen, Charlie and Liu, Ruibo and Sainath, Tara and Krikun, Maxim and Polozov, Alex and Lespiau, Jean-Baptiste and Newlan, Josh and Cankara, Zeyncep and Kwak, Soo and Xu, Yunhan and Chen, Phil and Coenen, Andy and Meyer, Clemens and Tsihlas, Katerina and Ma, Ada and Gottweis, Juraj and Xing, Jinwei and Gu, Chenjie and Miao, Jin and Frank, Christian and Cankara, Zeynep and Ganapathy, Sanjay and Dasgupta, Ishita and Hughes-Fitt, Steph and Chen, Heng and Reid, David and Rong, Keran and Fan, Hongmin and Amersfoort, Joost van and Zhuang, Vincent and Cohen, Aaron and Gu, Shixiang Shane and Mohananey, Anhad and Ilic, Anastasija and Tobin, Taylor and Wieting, John and Bortsova, Anna and Thacker, Phoebe and Wang, Emma and Caveness, Emily and Chiu, Justin and Sezener, Eren and Kaskasoli, Alex and Baker, Steven and Millican, Katie and Elhawaty, Mohamed and Aisopos, Kostas and Lebsack, Carl and Byrd, Nathan and Dai, Hanjun and Jia, Wenhao and Wiethoff, Matthew and Davoodi, Elnaz and Weston, Albert and Yagati, Lakshman and Ahuja, Arun and Gao, Isabel and Pundak, Golan and Zhang, Susan and Azzam, Michael and Sim, Khe Chai and Caelles, Sergi and Keeling, James and Sharma, Abhanshu and Swing, Andy and Li, YaGuang and Liu, Chenxi and Bostock, Carrie Grimes and Bansal, Yamini and Nado, Zachary and Anand, Ankesh and Lipschultz, Josh and Karmarkar, Abhijit and Proleev, Lev and Ittycheriah, Abe and Yeganeh, Soheil Hassas and Polovets, George and Faust, Aleksandra and Sun, Jiao and Rrustemi, Alban and Li, Pen and Shivanna, Rakesh and Liu, Jeremiah and Welty, Chris and Lebron, Federico and Baddepudi, Anirudh and Krause, Sebastian and Parisotto, Emilio and Soricut, Radu and Xu, Zheng and Bloxwich, Dawn and Johnson, Melvin and Neyshabur, Behnam and Mao-Jones, Justin and Wang, Renshen and Ramasesh, Vinay and Abbas, Zaheer and Guez, Arthur and Segal, Constant and Nguyen, Duc Dung and Svensson, James and Hou, Le and York, Sarah and Milan, Kieran and Bridgers, Sophie and Gworek, Wiktor and Tagliasacchi, Marco and Lee-Thorp, James and Chang, Michael and Guseynov, Alexey and Hartman, Ale Jakse and Kwong, Michael and Zhao, Ruizhe and Kashem, Sheleem and Cole, Elizabeth and Miech, Antoine and Tanburn, Richard and Phuong, Mary and Pavetic, Filip and Cevey, Sebastien and Comanescu, Ramona and Ives, Richard and Yang, Sherry and Du, Cosmo and Li, Bo and Zhang, Zizhao and Iinuma, Mariko and Hu, Clara Huiyi and Roy, Aurko and Bijwadia, Shaan and Zhu, Zhenkai and Martins, Danilo and Saputro, Rachel and Gergely, Anita and Zheng, Steven and Jia, Dawei and Antonoglou, Ioannis and Sadovsky, Adam and Gu, Shane and Bi, Yingying and Andreev, Alek and Samangooei, Sina and Khan, Mina and Kocisky, Tomas and Filos, Angelos and Kumar, Chintu and Bishop, Colton and Yu, Adams and Hodkinson, Sarah and Mittal, Sid and Shah, Premal and Moufarek, Alexandre and Cheng, Yong and Bloniarz, Adam and Lee, Jaehoon and Pejman, Pedram and Michel, Paul and Spencer, Stephen and Feinberg, Vladimir and Xiong, Xuehan and Savinov, Nikolay and Smith, Charlotte and Shakeri, Siamak and Tran, Dustin and Chesus, Mary and Bohnet, Bernd and Tucker, George and Glehn, Tamara von and Muir, Carrie and Mao, Yiran and Kazawa, Hideto and Slone, Ambrose and Soparkar, Kedar and Shrivastava, Disha and Cobon-Kerr, James and Sharman, Michael and Pavagadhi, Jay and Araya, Carlos and Misiunas, Karolis and Ghelani, Nimesh and Laskin, Michael and Barker, David and Li, Qiujia and Briukhov, Anton and Houlsby, Neil and Glaese, Mia and Lakshminarayanan, Balaji and Schucher, Nathan and Tang, Yunhao and Collins, Eli and Lim, Hyeontaek and Feng, Fangxiaoyu and Recasens, Adria and Lai, Guangda and Magni, Alberto and Cao, Nicola De and Siddhant, Aditya and Ashwood, Zoe and Orbay, Jordi and Dehghani, Mostafa and Brennan, Jenny and He, Yifan and Xu, Kelvin and Gao, Yang and Saroufim, Carl and Molloy, James and Wu, Xinyi and Arnold, Seb and Chang, Solomon and Schrittwieser, Julian and Buchatskaya, Elena and Radpour, Soroush and Polacek, Martin and Giordano, Skye and Bapna, Ankur and Tokumine, Simon and Hellendoorn, Vincent and Sottiaux, Thibault and Cogan, Sarah and Severyn, Aliaksei and Saleh, Mohammad and Thakoor, Shantanu and Shefey, Laurent and Qiao, Siyuan and Gaba, Meenu and Chang, Shuo-yiin and Swanson, Craig and Zhang, Biao and Lee, Benjamin and Rubenstein, Paul Kishan and Song, Gan and Kwiatkowski, Tom and Koop, Anna and Kannan, Ajay and Kao, David and Schuh, Parker and Stjerngren, Axel and Ghiasi, Golnaz and Gibson, Gena and Vilnis, Luke and Yuan, Ye and Ferreira, Felipe Tiengo and Kamath, Aishwarya and Klimenko, Ted and Franko, Ken and Xiao, Kefan and Bhattacharya, Indro and Patel, Miteyan and Wang, Rui and Morris, Alex and Strudel, Robin and Sharma, Vivek and Choy, Peter and Hashemi, Sayed Hadi and Landon, Jessica and Finkelstein, Mara and Jhakra, Priya and Frye, Justin and Barnes, Megan and Mauger, Matthew and Daun, Dennis and Baatarsukh, Khuslen and Tung, Matthew and Farhan, Wael and Michalewski, Henryk and Viola, Fabio and Quitry, Felix de Chaumont and Lan, Charline Le and Hudson, Tom and Wang, Qingze and Fischer, Felix and Zheng, Ivy and White, Elspeth and Dragan, Anca and Alayrac, Jean-baptiste and Ni, Eric and Pritzel, Alexander and Iwanicki, Adam and Isard, Michael and Bulanova, Anna and Zilka, Lukas and Dyer, Ethan and Sachan, Devendra and Srinivasan, Srivatsan and Muckenhirn, Hannah and Cai, Honglong and Mandhane, Amol and Tariq, Mukarram and Rae, Jack W. and Wang, Gary and Ayoub, Kareem and FitzGerald, Nicholas and Zhao, Yao and Han, Woohyun and Alberti, Chris and Garrette, Dan and Krishnakumar, Kashyap and Gimenez, Mai and Levskaya, Anselm and Sohn, Daniel and Matak, Josip and Iturrate, Inaki and Chang, Michael B. and Xiang, Jackie and Cao, Yuan and Ranka, Nishant and Brown, Geoff and Hutter, Adrian and Mirrokni, Vahab and Chen, Nanxin and Yao, Kaisheng and Egyed, Zoltan and Galilee, Francois and Liechty, Tyler and Kallakuri, Praveen and Palmer, Evan and Ghemawat, Sanjay and Liu, Jasmine and Tao, David and Thornton, Chloe and Green, Tim and Jasarevic, Mimi and Lin, Sharon and Cotruta, Victor and Tan, Yi-Xuan and Fiedel, Noah and Yu, Hongkun and Chi, Ed and Neitz, Alexander and Heitkaemper, Jens and Sinha, Anu and Zhou, Denny and Sun, Yi and Kaed, Charbel and Hulse, Brice and Mishra, Swaroop and Georgaki, Maria and Kudugunta, Sneha and Farabet, Clement and Shafran, Izhak and Vlasic, Daniel and Tsitsulin, Anton and Ananthanarayanan, Rajagopal and Carin, Alen and Su, Guolong and Sun, Pei and V, Shashank and Carvajal, Gabriel and Broder, Josef and Comsa, Iulia and Repina, Alena and Wong, William and Chen, Warren Weilun and Hawkins, Peter and Filonov, Egor and Loher, Lucia and Hirnschall, Christoph and Wang, Weiyi and Ye, Jingchen and Burns, Andrea and Cate, Hardie and Wright, Diana Gage and Piccinini, Federico and Zhang, Lei and Lin, Chu-Cheng and Gog, Ionel and Kulizhskaya, Yana and Sreevatsa, Ashwin and Song, Shuang and Cobo, Luis C. and Iyer, Anand and Tekur, Chetan and Garrido, Guillermo and Xiao, Zhuyun and Kemp, Rupert and Zheng, Huaixiu Steven and Li, Hui and Agarwal, Ananth and Ngani, Christel and Goshvadi, Kati and Santamaria-Fernandez, Rebeca and Fica, Wojciech and Chen, Xinyun and Gorgolewski, Chris and Sun, Sean and Garg, Roopal and Ye, Xinyu and Eslami, S. M. Ali and Hua, Nan and Simon, Jon and Joshi, Pratik and Kim, Yelin and Tenney, Ian and Potluri, Sahitya and Thiet, Lam Nguyen and Yuan, Quan and Luisier, Florian and Chronopoulou, Alexandra and Scellato, Salvatore and Srinivasan, Praveen and Chen, Minmin and Koverkathu, Vinod and Dalibard, Valentin and Xu, Yaming and Saeta, Brennan and Anderson, Keith and Sellam, Thibault and Fernando, Nick and Huot, Fantine and Jung, Junehyuk and Varadarajan, Mani and Quinn, Michael and Raul, Amit and Le, Maigo and Habalov, Ruslan and Clark, Jon and Jalan, Komal and Bullard, Kalesha and Singhal, Achintya and Luong, Thang and Wang, Boyu and Rajayogam, Sujeevan and Eisenschlos, Julian and Jia, Johnson and Finchelstein, Daniel and Yakubovich, Alex and Balle, Daniel and Fink, Michael and Agarwal, Sameer and Li, Jing and Dvijotham, Dj and Pal, Shalini and Kang, Kai and Konzelmann, Jaclyn and Beattie, Jennifer and Dousse, Olivier and Wu, Diane and Crocker, Remi and Elkind, Chen and Jonnalagadda, Siddhartha Reddy and Lee, Jong and Holtmann-Rice, Dan and Kallarackal, Krystal and Liu, Rosanne and Vnukov, Denis and Vats, Neera and Invernizzi, Luca and Jafari, Mohsen and Zhou, Huanjie and Taylor, Lilly and Prendki, Jennifer and Wu, Marcus and Eccles, Tom and Liu, Tianqi and Kopparapu, Kavya and Beaufays, Francoise and Angermueller, Christof and Marzoca, Andreea and Sarcar, Shourya and Dib, Hilal and Stanway, Jeff and Perbet, Frank and Trdin, Nejc and Sterneck, Rachel and Khorlin, Andrey and Li, Dinghua and Wu, Xihui and Goenka, Sonam and Madras, David and Goldshtein, Sasha and Gierke, Willi and Zhou, Tong and Liu, Yaxin and Liang, Yannie and White, Anais and Li, Yunjie and Singh, Shreya and Bahargam, Sanaz and Epstein, Mark and Basu, Sujoy and Lao, Li and Ozturel, Adnan and Crous, Carl and Zhai, Alex and Lu, Han and Tung, Zora and Gaur, Neeraj and Walton, Alanna and Dixon, Lucas and Zhang, Ming and Globerson, Amir and Uy, Grant and Bolt, Andrew and Wiles, Olivia and Nasr, Milad and Shumailov, Ilia and Selvi, Marco and Piccinno, Francesco and Aguilar, Ricardo and McCarthy, Sara and Khalman, Misha and Shukla, Mrinal and Galic, Vlado and Carpenter, John and Villela, Kevin and Zhang, Haibin and Richardson, Harry and Martens, James and Bosnjak, Matko and Belle, Shreyas Rammohan and Seibert, Jeff and Alnahlawi, Mahmoud and McWilliams, Brian and Singh, Sankalp and Louis, Annie and Ding, Wen and Popovici, Dan and Simicich, Lenin and Knight, Laura and Mehta, Pulkit and Gupta, Nishesh and Shi, Chongyang and Fatehi, Saaber and Mitrovic, Jovana and Grills, Alex and Pagadora, Joseph and Munkhdalai, Tsendsuren and Petrova, Dessie and Eisenbud, Danielle and Zhang, Zhishuai and Yates, Damion and Mittal, Bhavishya and Tripuraneni, Nilesh and Assael, Yannis and Brovelli, Thomas and Jain, Prateek and Velimirovic, Mihajlo and Akbulut, Canfer and Mu, Jiaqi and Macherey, Wolfgang and Kumar, Ravin and Xu, Jun and Qureshi, Haroon and Comanici, Gheorghe and Wiesner, Jeremy and Gong, Zhitao and Ruddock, Anton and Bauer, Matthias and Felt, Nick and GP, Anirudh and Arnab, Anurag and Zelle, Dustin and Rothfuss, Jonas and Rosgen, Bill and Shenoy, Ashish and Seybold, Bryan and Li, Xinjian and Mudigonda, Jayaram and Erdogan, Goker and Xia, Jiawei and Simsa, Jiri and Michi, Andrea and Yao, Yi and Yew, Christopher and Kan, Steven and Caswell, Isaac and Radebaugh, Carey and Elisseeff, Andre and Valenzuela, Pedro and McKinney, Kay and Paterson, Kim and Cui, Albert and Latorre-Chimoto, Eri and Kim, Solomon and Zeng, William and Durden, Ken and Ponnapalli, Priya and Sosea, Tiberiu and Choquette-Choo, Christopher A. and Manyika, James and Robenek, Brona and Vashisht, Harsha and Pereira, Sebastien and Lam, Hoi and Velic, Marko and Owusu-Afriyie, Denese and Lee, Katherine and Bolukbasi, Tolga and Parrish, Alicia and Lu, Shawn and Park, Jane and Venkatraman, Balaji and Talbert, Alice and Rosique, Lambert and Cheng, Yuchung and Sozanschi, Andrei and Paszke, Adam and Kumar, Praveen and Austin, Jessica and Li, Lu and Salama, Khalid and Perz, Bartek and Kim, Wooyeol and Dukkipati, Nandita and Baryshnikov, Anthony and Kaplanis, Christos and Sheng, XiangHai and Chervonyi, Yuri and Unlu, Caglar and Casas, Diego de Las and Askham, Harry and Tunyasuvunakool, Kathryn and Gimeno, Felix and Poder, Siim and Kwak, Chester and Miecnikowski, Matt and Mirrokni, Vahab and Dimitriev, Alek and Parisi, Aaron and Liu, Dangyi and Tsai, Tomy and Shevlane, Toby and Kouridi, Christina and Garmon, Drew and Goedeckemeyer, Adrian and Brown, Adam R. and Vijayakumar, Anitha and Elqursh, Ali and Jazayeri, Sadegh and Huang, Jin and Carthy, Sara Mc and Hoover, Jay and Kim, Lucy and Kumar, Sandeep and Chen, Wei and Biles, Courtney and Bingham, Garrett and Rosen, Evan and Wang, Lisa and Tan, Qijun and Engel, David and Pongetti, Francesco and Cesare, Dario de and Hwang, Dongseong and Yu, Lily and Pullman, Jennifer and Narayanan, Srini and Levin, Kyle and Gopal, Siddharth and Li, Megan and Aharoni, Asaf and Trinh, Trieu and Lo, Jessica and Casagrande, Norman and Vij, Roopali and Matthey, Loic and Ramadhana, Bramandia and Matthews, Austin and Carey, C. J. and Johnson, Matthew and Goranova, Kremena and Shah, Rohin and Ashraf, Shereen and Dasgupta, Kingshuk and Larsen, Rasmus and Wang, Yicheng and Vuyyuru, Manish Reddy and Jiang, Chong and Ijazi, Joana and Osawa, Kazuki and Smith, Celine and Boppana, Ramya Sree and Bilal, Taylan and Koizumi, Yuma and Xu, Ying and Altun, Yasemin and Shabat, Nir and Bariach, Ben and Korchemniy, Alex and Choo, Kiam and Ronneberger, Olaf and Iwuanyanwu, Chimezie and Zhao, Shubin and Soergel, David and Hsieh, Cho-Jui and Cai, Irene and Iqbal, Shariq and Sundermeyer, Martin and Chen, Zhe and Bursztein, Elie and Malaviya, Chaitanya and Biadsy, Fadi and Shroff, Prakash and Dhillon, Inderjit and Latkar, Tejasi and Dyer, Chris and Forbes, Hannah and Nicosia, Massimo and Nikolaev, Vitaly and Greene, Somer and Georgiev, Marin and Wang, Pidong and Martin, Nina and Sedghi, Hanie and Zhang, John and Banzal, Praseem and Fritz, Doug and Rao, Vikram and Wang, Xuezhi and Zhang, Jiageng and Patraucean, Viorica and Du, Dayou and Mordatch, Igor and Jurin, Ivan and Liu, Lewis and Dubey, Ayush and Mohan, Abhi and Nowakowski, Janek and Ion, Vlad-Doru and Wei, Nan and Tojo, Reiko and Raad, Maria Abi and Hudson, Drew A. and Keshava, Vaishakh and Agrawal, Shubham and Ramirez, Kevin and Wu, Zhichun and Nguyen, Hoang and Liu, Ji and Sewak, Madhavi and Petrini, Bryce and Choi, DongHyun and Philips, Ivan and Wang, Ziyue and Bica, Ioana and Garg, Ankush and Wilkiewicz, Jarek and Agrawal, Priyanka and Li, Xiaowei and Guo, Danhao and Xue, Emily and Shaik, Naseer and Leach, Andrew and Khan, Sadh MNM and Wiesinger, Julia and Jerome, Sammy and Chakladar, Abhishek and Wang, Alek Wenjiao and Ornduff, Tina and Abu, Folake and Ghaffarkhah, Alireza and Wainwright, Marcus and Cortes, Mario and Liu, Frederick and Maynez, Joshua and Terzis, Andreas and Samangouei, Pouya and Mansour, Riham and Kępa, Tomasz and Aubet, François-Xavier and Algymr, Anton and Banica, Dan and Weisz, Agoston and Orban, Andras and Senges, Alexandre and Andrejczuk, Ewa and Geller, Mark and Santo, Niccolo Dal and Anklin, Valentin and Merey, Majd Al and Baeuml, Martin and Strohman, Trevor and Bai, Junwen and Petrov, Slav and Wu, Yonghui and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeff and Vinyals, Oriol},
	month = dec,
	year = {2024},
	note = {arXiv:2403.05530 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o {System} {Card}},
	url = {http://arxiv.org/abs/2410.21276},
	doi = {10.48550/arXiv.2410.21276},
	abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {OpenAI and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and Mądry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and Lohmann, Fred von and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and Gu-Lemberg, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and Castro, Miguel Oom Temudo de and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and Campbell-Moore, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and Shuaiqi and Xia and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wei_chatie_2024,
	title = {{ChatIE}: {Zero}-{Shot} {Information} {Extraction} via {Chatting} with {ChatGPT}},
	shorttitle = {{ChatIE}},
	url = {http://arxiv.org/abs/2302.10205},
	doi = {10.48550/arXiv.2302.10205},
	abstract = {Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
	month = may,
	year = {2024},
	note = {arXiv:2302.10205 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{su_roformer_2023,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {http://arxiv.org/abs/2104.09864},
	doi = {10.48550/arXiv.2104.09864},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = nov,
	year = {2023},
	note = {arXiv:2104.09864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_extending_2023,
	title = {Extending {Context} {Window} of {Large} {Language} {Models} via {Positional} {Interpolation}},
	url = {http://arxiv.org/abs/2306.15595},
	doi = {10.48550/arXiv.2306.15595},
	abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2025-11-22},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
}
