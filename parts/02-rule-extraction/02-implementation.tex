% !TeX root = ../../main.tex
% !TeX encoding = utf8

\chapter{Implementation}\label{ch:implementation}

This chapter describes the iterative development process and various approaches explored for automated rule extraction.

\section{Methodology overview}

The implementation follows an iterative approach, where each iteration builds upon lessons learned from previous attempts. The general pipeline consists of:

\begin{enumerate}
    \item Document ingestion and preprocessing
    \item Information extraction from multiple sources
    \item Rule generation using LLMs
    \item Validation and refinement
    \item Human-in-the-loop feedback integration
\end{enumerate}

\section{First approach: Zero-shot prompting}

The initial approach used zero-shot prompting with a general-purpose LLM, providing sensor descriptions and P\&ID information directly in the prompt. While this approach could generate plausible rules, it suffered from:

\begin{itemize}
    \item Limited context window for large documentation sets
    \item Frequent hallucinations
    \item Inconsistent rule formats
    \item Poor performance on domain-specific terminology
\end{itemize}

\section{Fine-tuning}

To improve domain adaptation, we explored fine-tuning approaches using examples of correct rules derived from existing monitoring systems. This section describes:

\begin{itemize}
    \item Training data collection and annotation
    \item Model selection and fine-tuning procedures
    \item Evaluation metrics
    \item Comparison with zero-shot baseline
\end{itemize}

\section{Retrieval Augmented Generation (RAG)}

RAG combines retrieval of relevant documentation with generation, allowing the model to access a larger knowledge base without exceeding context limits. The implementation includes:

\begin{itemize}
    \item Document chunking and embedding strategies
    \item Vector database selection and indexing
    \item Retrieval strategies (dense, sparse, hybrid)
    \item Integration with the generation pipeline
\end{itemize}

\section{Extracting methods instead of rules}

Rather than generating rules directly, this approach extracts intermediate representations (methods or procedures) that can be compiled into rules. Benefits include:

\begin{itemize}
    \item More structured and verifiable outputs
    \item Easier composition of complex multi-step conditions
    \item Better alignment with how experts describe monitoring logic
\end{itemize}

\section{Direct Preference Optimization (DPO)}

DPO enables the model to learn from human preferences without requiring explicit reward modeling. The implementation includes:

\begin{itemize}
    \item Collection of rule pairs with preference annotations
    \item DPO training procedure
    \item Evaluation of preference alignment
    \item Comparison with RLHF approaches
\end{itemize}

\section{Improved diagram parsing}

Enhanced P\&ID parsing capabilities through:

\begin{itemize}
    \item Multi-modal models that combine vision and language understanding
    \item Symbol recognition and classification
    \item Relationship extraction between diagram elements
    \item Integration of diagram information with textual documentation
\end{itemize}

