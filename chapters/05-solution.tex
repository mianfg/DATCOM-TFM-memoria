% =============================================================================
% CHAPTER 5: SOLUTION DESIGN AND IMPLEMENTATION
% =============================================================================

\chapter{Solution Design and Implementation}\label{ch:5-solution-design-and-implementation}

In this chapter, we will incrementally build the main object of this thesis: the \textbf{rule extractor}. As detailed in \Cref{methodology}, we will follow an incremental approach, iterating through different versions, always guided by research and results.

\begin{enumerate}[]
    \setcounter{enumi}{-1}
    \item \hyperref[sec:implementation-0]{\textbf{Baseline: Naive Single-Document Extraction:}} extracting rules in natural language providing the document content directly as context, in a ``zero-shot'' fashion.
    \item \hyperref[sec:implementation-1]{\textbf{Chunking:}} chunking the provided documents and execute the ``zero-shot'' approach with each of the chunks.
    \item \hyperref[sec:implementation-2-structured]{\textbf{Structured Output:}} forcing rules to be output with a certain schema.
    \item \hyperref[sec:implementation-3-rag]{\textbf{Context Gathering via RAG:}} increasing the context of each of the chunks using a RAG approach with similarity search from a vector database.
    \item \hyperref[sec:implementation-4-python-functions]{\textbf{Executable Python Rules:}} breaking free from a schema-oriented output into executable Python functions.
    \item \hyperref[sec:implementation-5-sensors]{\textbf{Sensor Resolution:}} parsing sensor information from the generated Python rules using an agent.
    \item \hyperref[sec:implementation-6-time]{\textbf{Time Expression Resolution:}} parsing time expressions from the generated Python rules using an agent.
    \item \hyperref[sec:implementation-7-grounding]{\textbf{External Knowledge Grounding:}} connecting to the external world to gather additional data for rule generation.
    \item \hyperref[sec:implementation-8-verification]{\textbf{Rule Verification:}} verifying correctness of the generated rules.
    \item \hyperref[sec:implementation-9-consolidation]{\textbf{Rule Consolidation:}} unifying rules generated across different chunks, removing redundancies and inconsistencies.
\end{enumerate}

Finally, in \Cref{llm-orchestration-and-workflows} we put everything together and talk about some relevant implementation details for the final workflow, focusing on traceability and observability.

\newpage

\setcounter{section}{-1}

\section{Baseline: Naive Single-Document Extraction}\label{sec:implementation-0}

\begin{figure}[ht]
    \centering
    \includegraphics{diagram_00.pdf}
    \caption{Naive Single-Document Extraction}
    \label{fig:diagram-0}
\end{figure}

This baseline approach is grounded in the hypothesis that a zero-shot prompting strategy, devoid of external retrieval mechanisms, should be capable of extracting and synthesizing operational monitoring rules directly from raw text. This premise rests on three technical pillars.

First, \textbf{contextual capacity} has been dramatically expanded in recent foundational model architectures. Specifically, the scaling of RoPE (\textit{Rotary Positional Embeddings}) \cite{su_roformer_2024, chen_extending_2023}, has significantly extended context windows (see \Cref{tab:context_windows}). Theoretically, these models now possess the capacity to ingest entire industrial manuals--typically ranging from 50 to 200 pages--in a single inference pass without requiring truncation.

Additionally, LLMs have demonstrated ``emergent'' capabilities in \textbf{zero-shot IE}\cite{wei_chatie_2024}. Current literature suggests that for stand-alone documents, modern LLMs can perform relation extraction (e.g., mapping a rule to a sensor) without task-specific fine-tuning, relying solely on their ability to follow complex instructions \cite{kojima_large_2022}.

Most importantly, this approach closely explores the \textbf{``death of RAG'' assumption}, which posits that massive context windows may render RAG obsolete for certain tasks. This argument highlights that self-attention is \textit{lossless}--the model attends to the actual tokens--whereas vector retrieval is \textit{lossy}, as it relies on a compressed numerical shadow of the text \cite{kuratov_cramming_2025}. Furthermore, RAG introduces boundary issues where answers spanning multiple chunks are often missed. In contrast, long-context processing allows the model to reason across the entire text simultaneously, comparing multiple sections without arbitrary boundaries \cite{li_long_2024}.

\vspace*{2em}

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l}
        \toprule
        & \textbf{Release} & \textbf{Max. Input} & \textbf{Max. Output} \\
        \textbf{Model} & \textbf{Date} & \textbf{Tokens (Context)} & \textbf{Tokens} \\
        \midrule
        OpenAI GPT-4o & 2024-05-13 \hspace{2em} & 128k & 16k \\
        Anthropic Claude 3.5 Sonnet \hspace{2em} & 2024-06-20 & 200k & 8k \\
        DeepSeek-R1 & 2025-01-20 & 128k & 64k \\
        Anthropic Claude 3.7 Sonnet & 2025-02-24 & 200k & 128k \\
        Google Gemini 2.5 Pro & 2025-03-25 & 1M+ & 65k \\
        OpenAI GPT-5 & 2025-08-07 & 272k & 128k \\
        Anthropic Claude Opus 4.5 & 2025-11-24 & 200k & 64k \\
        Google Gemini 3.0 Pro & 2025-11-18 & 1M+ & 65k \\
        \bottomrule
    \end{tabular}
    \footnotesize
    \caption{Comparison of Context Windows and Output Sizes for Leading LLMs}
    \label{tab:context_windows}
\end{table}
\subsection*{Implementation}

Two primary implementation strategies were considered for this baseline.

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Zero-shot full-context extraction:} loading the entire document string into the context window and requesting extraction in a single prompt.
\item \textbf{Summarization-then-extraction:} Asking the model to summarize the document first, then extract rules from the summary.
\end{enumerate}

The second option was scrapped because summarization processes tend to lose specific numeric thresholds (e.g., distinct temperature or pressure values) which are critical for accurate industrial rules. Consequently, the full-context extraction method (option A) was selected (see \Cref{fig:diagram-0}).

The \textit{DeepSeek-R1} model was selected due to its status as a state-of-the-art foundation model with a significant context window and the ability to run locally via \textit{Ollama}. The input data consisted of technical documentation converted into plain text.

The prompting strategy utilized a zero-shot approach. We just specify the formatting a bit, by eliciting the model to \textit{``Return them [the rules] as a list.''} The document is injected directly into the prompt. The full prompt is shown in \Cref{thread:0-baseline}.

This is the simplest, most straight-forward way to use LLMs when extracting rules, and we establish it as a baseline for future iterations.


\begin{figure}[htbp]

\begin{UserMessage}
You are an expert industrial engineer.\\
Read the following document and extract ALL operational and safety rules.\\
Return them as a list.\\[6pt]
DOCUMENT:\\
\textcolor{purple}{\{document\}}
\end{UserMessage}


\begin{AssistantMessage}
Based on a thorough review of the provided C3/C4 Splitter document, the following is a comprehensive list of extracted operational and safety rules.

\subsection*{Operational Rules}

\textbf{General Operation:}

1.  Maintain the tray 7 (or sensitive tray) temperature around 80.2°C as the primary control for separation.

2.  Maintain reflux flow around 440 units to ensure proper overhead product quality.

3.  Monitor and utilize inferential analyzer readings (e.g., \texttt{14A3003I}, \texttt{14\_C4\_I}) for real-time product quality control.

...\\[6pt]

\textbf{Quality Control:}

8.  The primary quality objective is to maintain the C3 content in the bottom (C4) product below target specification.

9.  The secondary quality objective is to maintain a minimum reflux ratio to ensure overhead (C3) product composition.

...

\subsection*{Safety Rules}

\textbf{Pressure \& Relief Protection}

24. Maintain column pressure within safe operating limits as a primary safety objective.

25. The system is protected by pressure safety valves (PSVs): PSV8 set at 17.3 kg/cm² and PSV12 set at 17.8 kg/cm².

...\\[6pt]

\textbf{Emergency Response:}

31. \textbf{In case of loss of pressure control:} Reduce or stop the feed and ensure cooling water flow to condensers.

32. \textbf{In case of loss of level control}: Monitor pump operation and adjust manual bypasses as needed.

...
\end{AssistantMessage}

\caption{Message thread showcasing naive single-document extraction}
\label{thread:0-baseline}
\end{figure}

\subsection*{Results}

Implementing this basic approach yielded mixed results. An example output is shown in \Cref{thread:0-baseline}. Most importantly, the extracted rules \textbf{lacked consistency and utility}, with most of the rules being general descriptions and not actual rules (`\textit{`Monitor and utilize [...] readings [...] for real-time product quality control''}, \textit{``The primary quality objective is to maintain the C3 content [...] below target specification.''}). The rules that were useful were directly extracted from the text. Also, some of them were \textbf{hallucinations}, which are difficult to track back to the document (of course, we provide \textbf{no traceability or explainability} this way).

It is also important to note that, even if these rules are correct, they cannot be used immediately in the industrial context: the \textbf{lack of structure} makes these only useful for reading, but not for actual automated system.

Regarding LLM processing, we can also observe a \textbf{lost in the middle} phenomenon, where the LLM prioritized the information at the very beginning or end of the document, often ignoring content located in the middle sections. In the case of larger documents, the model suffered from \textbf{context overflow}, where the token count exceeded the model's efficient processing limit, leading to truncation, excessive latencies or outright refusal to process.

There are mutliple limitations to address from this baseline. We will prioritize in \Cref{sec:implementation-1} a \textbf{chunking strategy} to break the document into manageable segments, ensuring the LLM focuses on specific details and returns a more consistent set of rules. However, we must not forget about the utility and parsing issues, which we will resolve later with \textbf{structured output} in \Cref{sec:implementation-2-structured}.

\newpage

\section{Document Chunking}\label{sec:implementation-1}

\begin{figure}[ht]
    \centering
    \includegraphics{diagram_01.pdf}
    \caption{Naive Single-Document Extraction with Chunking}
    \label{fig:diagram-1}
\end{figure}

As a first iteration, we will explore the hypothesis that by decomposing the document into smaller segments (\textbf{\textit{``chunks''}}), the LLM's attention mechanism can focus on local details without the degradation observed in long-context windows, thereby improving the model's efficiency and scalability.

This approach addresses several critical limitations of full-context processing. First, regarding \textbf{scalability}, industrial corpora may exceed any state-of-the-art context window size, making segmentation a practical necessity. Second, it mitigates \textbf{attention dilution and positional bias}. Transformer performance is not uniform across the context window, as research demonstrates that LLMs are highly sensitive to irrelevant context (\textit{noise}), which significantly degrades reasoning accuracy \cite{shi_large_2023}. Furthermore, the \textbf{\textit{``lost in the middle''}} phenomenon has been widely studied and identified, where models fail to attend to details buried in the center of long inputs \cite{liu_lost_2024}.

Additionally, this strategy alleviates the \textbf{generation bottleneck}. While modern LLMs support massive input contexts (e.g., 128k tokens), their output generation is often severely limited (see \Cref{tab:context_windows}). Attempting to generate hundreds of rules in a single pass frequently results in output truncation. Finally, \textbf{segmentation} has been shown to be incredibly efficient, as processing smaller, focused segments (chunk-based retrieval) consistently yields higher recall for specific details compared to ingesting full long-context documents \cite{xu_retrieval_2024}.

\subsection*{Implementation}

There are many ways to perform document chunking. We can highlight three:

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Fixed size fragmentation:} divide the documents into even chunks.
\item \textbf{Divide document by hierarchy:} parse the sections and/or subsections of the document, and chunk the document into those divisions.
\item \textbf{Recursive character splitting with overlap:} splitting by characters while respecting sentence boundaries, with a rolling overlap.
\end{enumerate}

Among these options, the latter was selected, as it is robust against formatting errors and ensures boundary continuity, preventing sentences from being severed mid-thought.

We use LangChain's \texttt{RecursiveCharacterTextSplitter}, a text splitting algorithm designed to keep semantically related text together. It operates recursively using a specific hierarchy of separators (e.g., paragraphs, newlines, spaces), attempting to split text using the ``largest'' separator first. If the resulting chunk is still too large, it moves to the next separator in the hierarchy until the chunk satisfies some constraints:

\begin{itemize}
    \item \textbf{\texttt{chunk\_size}:} the splitter ensures that for every generated chunk $c_i$ in the output set $C$, $\text{length}(c_i)\leq L$, with $L=\text{\texttt{chunk\_size}}$. The algorithm creates a new chunk $c_i$ and populates it until $\text{length}(c_i)$ approaches $L$. If adding the next piece of text exceeds $L$, the chunk is closed, and a new chunk $c_{i+1}$ begins.
    \item \textbf{\texttt{chunk\_overlap}:} the relationship between two consecutive chunks $c_i$ and $c_{i+1}$ is defined such that the end of $c_i$ is identical to the beginning of $c_{i+1}$ for length $O =\text{\texttt{chunk\_overlap}}$:$$c_{i+1}[\text{start} : O] = c_i[\text{end} - O : \text{end}]$$ This preserves context between splits. Without overlap, a split might occur in the middle of a sentence or idea, severing the semantic link.
\end{itemize}

We enforce \texttt{chunk\_size = 1000}, \texttt{chunk\_overlap = 200}. This works well because it separates documents into a manageable amount of chunks. Each of the chunk will generate rules: having too low chunks considerably lowers throughput, while a big chunk size comes back to the approach of \Cref{sec:implementation-0} and its limitations.

\subsection*{Results}

Thanks to this chunking mechanism, we achieve \textbf{higher throughput}. In our experiments, the number of generated rules increased by approximately 6 times. This, however, is a double-edged sword: generating independent rules per chunk introduces \textbf{redundancies} and unnecessary rules, with rules that could collide with each other. We will explore synthesizing these rules later on, in \Cref{sec:implementation-9-consolidation}.

However, while \textbf{recall} improved, detecting more nuances and concrete rules, \textbf{context} was lost. Some rules might need information from other parts of the document, or the chunk split might cause parts of the ideas to be spread, effectively avoiding those rules not to be created. We will address this limitation in \Cref{sec:implementation-3-rag}.

It's also important to note that the \textbf{parsing} problem still persists, and is more visible when chunking. Some of the chunks returned rules in one format, and some in the other (see \Cref{thread:1-chunking}).


\begin{figure}[htbp]

\begin{AssistantMessage}
Based on the provided document, here is a list of extracted operational and safety rules:\\[6pt]

\textbf{Operational Rules:}

1.  Maintain the Steam System to provide backup heating for the reboiler.

2.  Operate the Cooling Water System to service condensers and product coolers.

3.  Ensure the Instrument Air System is functional to power pneumatic controls and instruments.

...\\[6pt]

\textbf{Safety Rules:}

1.  Maintain the Flare System for emergency pressure relief.

...
\end{AssistantMessage}


\begin{AssistantMessage}
Based on the provided document, here are the extracted operational and safety rules, organized by their source sections.

\subsection*{Operational Rules}

\textbf{From Section 3: Quality Control}
\begin{enumerate}
    \item If there is high C3 (Propane/Propylene) concentration in the bottom product, \textbf{increase the reboiler duty} and/or \textbf{increase the reflux rate}.
    \item If there is high C4 (Butane/Butylene) concentration in the overhead product, \textbf{increase the reflux ratio} and/or \textbf{reduce the reboiler duty}.
    \item ...
\end{enumerate}
...
\end{AssistantMessage}

\begin{AssistantMessage}
Based on the provided document excerpt, no explicit operational or safety rules are stated. The text is purely descriptive, outlining the unit's purpose, main equipment, and the beginning of a process flow description.\\[6pt]

\textbf{List of Extracted Rules:}

\textit{None}
\end{AssistantMessage}

\caption{Multiple outputs from different chunks}
\label{thread:1-chunking}
\end{figure}

\newpage

\section{Structured Output}\label{sec:implementation-2-structured}

\begin{figure}[ht]
    \centering
    \includegraphics{diagram_02.pdf}
    \caption{Naive Single-Document Extraction with Structured Output}
    \label{fig:diagram-2}
\end{figure}

While document segmentation enhances the precision of retrieval by focusing on local details, it introduces a trade-off between this gained precision and the coherence lost due to fragmentation. To mitigate the variability in generation within these segments and ensure downstream usability, this section explores \textbf{constrained decoding}. The core hypothesis is that forcing the LLM to output according to a strict schema will reduce generation variability and enable programmatic parsing of the extracted rules.

This approach addresses the \textbf{stochasticity-determinism conflict} inherent in autoregressive probabilistic models. Without constraints, the probability mass for valid syntax (e.g., \lstinline|{"val":|) competes with conversational filler (e.g., \lstinline{"Here is the data:"}), leading to \textit{format drift}. This drift is incompatible with industrial data pipelines, where downstream systems require deterministic data structures. Notably, even state-of-the-art models fail schema validation in over 15\% of complex cases when relying on prompting alone \cite{geng_jsonschemabench_2025}.

To resolve this, we employ \textbf{grammar-constrained decoding}, which restricts the LLM's output sampling to tokens that satisfy a formal grammar (such as a JSON Schema). Research indicates that this method eliminates syntax errors, reduces ``hallucinated formats,'' and guarantees 100\% parsability \cite{willard_efficient_2023}. Furthermore, contrary to the assumption that constraints add computational overhead, \textbf{Compressed Finite State Machines (FSMs)} accelerate inference \cite{zheng_sglang_2024}. By pre-computing deterministic token paths (e.g., automatically completing \lstinline{:} after \lstinline{"val"}), the system skips redundant sampling steps, thereby utilizing \textbf{state compression} to reduce latency.



\subsection*{Implementation}

We evaluate three common strategies for enforcing output schema:

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Prompt engineering:} we ask the model to output a certain JSON schema via prompting.
\item \textbf{Regex post-processing:} we extract JSON from text via regex.
\item \textbf{Native structured output (via grammar-constrained decoding):} we use the provider's API to enforce a schema at the decoding layer.
\end{enumerate}

We decide to use the latter option due to its ease of implementation and the guaranteed validity via FSM. It's important, however, to note that this enforces restrictions into the models that we can use, as some have not been trained for native structured output. The model we use, \textit{DeepSeek-R1}, can run in \textit{Ollama} with native structured output without issues.

Using LangChain, we can easily define the strict structure of a rule using \textit{Pydantic} models, effectively creating a ``type contract'' for the LLM. This Pydantic model is compiled into a JSON Schema, which the inference engine then converts into a FSM. During generation, the LLM is only permitted to select tokens that validly transition the FSM to the next state, guaranteeing the adherence of the generated content to such schema.

For this iteration, we define a strict \lstinline{ExtractedRules} schema. We allow for different kinds of rules to be generated via a \lstinline{Union}. Precisely, we generate:

\begin{enumerate}
    \item \textbf{\texttt{ThresholdRule}:} detects when a single variable crosses a specific threshold value.
    \item \textbf{\texttt{RangeRule}:} detects when a single variable is within a specific, inclusive range.
    \item \textbf{\texttt{OutOfRangeRule}:} detects when a single variable is outside of a specific, inclusive range.
    \item \textbf{\texttt{RateOfChangeRule}:} detects when the rate of change of a variable exceeds a threshold.
    \item \textbf{\texttt{TrendRule}:} detects a consistent trend (increasing, decreasing, or stable) over a minimum duration.
    \item \textbf{\texttt{ReferencePointRule}:} detects proximity to a specific point in a multi-variable state space.
\end{enumerate}

\begin{program}[hbtp]
\begin{CodeListing}
\begin{lstlisting}[language=Python]
from typing import Literal
from pydantic import BaseModel, Field
import re


class Rule(BaseModel):
    name: str = Field(description="A unique, descriptive name for the rule, e.g., 'position_high'.")
    description: str = Field(
        description="Understandable description and explanation of the rule. Include any additional information that is not obvious from the name."
    )
    width: float = Field(description="The smoothness of the transition for the event detection.")
    polarity: Literal[-1, 0, 1] = Field(
        description="Significance of the event: -1 for negative, 0 for neutral, 1 for positive."
    )


class ThresholdRule(Rule):
    """Detects when a single variable crosses a specific threshold value."""

    variable: str = Field(description="The name of the variable to monitor, e.g., 'position'.")
    threshold: float = Field(description="The numerical value to compare the variable against.")
    direction: Literal["above", "below"] = Field(
        description="The direction of the crossing required to trigger the event."
    )


...


RuleUnion = Union[
    ThresholdRule,
    RangeRule,
    OutOfRangeRule,
    RateOfChangeRule,
    TrendRule,
    ReferencePointRule
]

class ExtractedRules(BaseModel):
    rules: List[RuleUnion]  # Union of all Rule implementations
\end{lstlisting}
\end{CodeListing}
\caption{Pydantic model for \lstinline{ExtractedRules}}\label{program:operationalrule}
\end{program}

We show how these models are implemented in  \Cref{program:operationalrule}. Notice how Pydantic allows us to add descriptions to each of the fields and models. These descriptions are also fed into the LLM so that we don't need to specify them in the prompt. Models capable of structured output are trained to be able to understand these descriptions.

Once we define our model, the workflow proceeds as follows:

\begin{enumerate}
    \item A text chunk is sent to the LLM with the same zero-shot prompt as \Cref{sec:implementation-0,,sec:implementation-1}.
    \item The inference engine applies \textit{logit bias} based on the current FSM state (e.g., if the cursor is inside a boolean field, only \texttt{true} or \texttt{false} tokens are valid).
    \item The resulting response is deserialized directly into a Python object without additional parsing logic.
\end{enumerate}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert industrial engineer.

Read the following document and extract ALL operational and safety rules.

Choose the appropriate rule type.
\end{UserMessage}

\begin{AssistantMessage}
\begin{lstlisting}[language=Python]
ExtractedRules(
    rules=[
        ThresholdRule(
            name="column_pressure_high",
            description="Column pressure is too high, indicating potential overpressure risk in the distillation column. This is a critical safety parameter that must be monitored to prevent equipment damage or safety incidents.",
            width=0.5,
            polarity=-1,
            variable="14PC1037",
            threshold=15.0,
            direction="above",
        ),
        RangeRule(
            name="tray_10_temp_range",
            description="Tray 10 temperature must be maintained within optimal operating range for proper separation in the middle section of the column. Deviations indicate poor separation or flooding/dry tray conditions.",
            width=0.5,
            polarity=0,
            variable="14TI0041",
            min_val=65.0,
            max_val=75.0,
        ),
        RateOfChangeRule(
            name="temperature_increase_rate",
            description="During startup, temperatures should be increased gradually to operating targets to avoid thermal stress and operational upsets.",
            width=0.3,
            polarity=-1,
            variable="column_temperature",
            threshold=5.0,
            direction="increasing",
            window_size=10,
        ),
        ...
    ]
)
\end{lstlisting}
\end{AssistantMessage}


\end{minipage}

\caption{Message thread showcasing structured output extraction}
\label{thread:2-schema}
\end{figure}



\subsection*{Results}

The usage of structured output resulted in complete and strict \textbf{adherence to the defined schema}, ensuring type consistency (e.g., numeric thresholds were invariably instantiated as floating-point values rather than string representations). This is extremely useful, as we now have rules that can be used by automated systems.

However, the most generated rule types were \texttt{ThresholdRule} and \texttt{RangeRule}, while other rule types were never generated. This can be due to the nature of the provided documents, the redundancy of some rule types or due to constraints in the LLM (see \Cref{tab:2-extracted}).

In the end, even though we generate parseable rules, there's an inherent \textbf{limitation to the expressivenes} of them. JSON's lack of logical operators, and the variability in the kinds of rules that could be generated depending on the problem, make it necessary to develop specific types for each of the problems, that might not be usable in other contexts. Additionally, any kind of schema is an enforcement and, alas, a limitation. We will explore how to reduce this limit in \Cref{sec:implementation-4-python-functions}.

Most importantly, we still see some issues we identified earlier. The model is still prone to \textbf{hallucination and ambiguity}, especially when defining the affected sensor IDs. It often extracted generic terms (e.g., ``the reflux temperature'') rather than the specific instrument tag, as the mapping was often defined in a different part of the document, or could be defined in external documents. This motivates the need for a \textit{retrieval \& resolution} strategy that we will implement in \Cref{sec:implementation-3-rag}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}
        \toprule
        \textbf{Type of rule} & \textbf{\# extracted rules} \\
        \midrule
        \texttt{ThresholdRule} & 124 \\
        \texttt{RangeRule} & 30 \\
        \texttt{OutOfRangeRule} & 0 \\
        \texttt{RateOfChangeRule} & 14 \\
        \texttt{TrendRule} & 0 \\
        \texttt{ReferencePointRule} & 0 \\
        \midrule
        \textit{Total} & 168 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    \caption{Number of extracted rules per type of rule}
    \label{tab:2-extracted}
\end{table}

\newpage

\section{Context Gathering via RAG}\label{sec:implementation-3-rag}

\begin{figure}[ht]
    \centering
    \hspace*{-2.3cm}\includegraphics{diagram_03.pdf}
    \caption{Processing of provided collection of documents and ingestion into each database}
    \label{fig:diagram-3}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics{diagram_04.pdf}
    \caption{Rule retrieval workflow with context gathering (RAG)}
    \label{fig:diagram-4}
\end{figure}

We will investigate the integration of RAG to resolve context lost during document fragmentation and to accurately process complex visual structures. The hypothesis centers on the critical need for \textbf{layout-aware semantics}. Technical documentation is rarely unstructured text; it relies heavily on spatial layouts--including tables, headers, and indentation--to encode meaning. Traditional OCR and text extraction methods ``flatten'' this two-dimensional structure into a one-dimensional sequence, destroying relational data (e.g., that cell $i,j$ relates to header $j$). Research indicates that layout-aware parsing significantly improves IE recall by preserving the ``visual semantics'' of the document \cite{xu_layoutlm_2020, auer_docling_2024}.

Furthermore, this approach provides a \textbf{semantic bridge} over the ``keyhole problem'' identified in \Cref{sec:implementation-1}. The ``lost in the middle'' phenomenon persists even in chunked approaches if critical definitions reside in chunk $c_i$ while the rule resides in chunk $c_j$. Dense Vector Retrieval (RAG) bridges this gap by retrieving semantically relevant chunks based on meaning rather than physical proximity, effectively creating a dynamic context window \cite{lewis_retrieval-augmented_2020}. Finally, to satisfy industrial constraints, we leverage \textbf{data sovereignty \& compression} principles. MRL enables high-performance embeddings that can be truncated (compressed) with minimal accuracy loss, optimizing local storage without sacrificing retrieval quality \cite{kusupati_matryoshka_2022}.

\subsection*{Implementation}

The implementation required selecting strategies for parsing, embedding, and storage that balanced performance with strict industrial constraints.

\subsubsection*{Parsing strategy}

First of all, we need to decide how we are going to \textbf{parse} the documents provided, which do not necessarily have to be plain text. Industrial information can be stored in PDFs, images or other more complex formats.

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Text-only loaders} (PyPDF)\textbf{:} extract raw text strings from properly formatted PDF documents.
\item \textbf{Layout-aware parsing} (Docling)\textbf{:} Docling is IBM's multimodal parser that identifies document structures (tables, reading order) using computer vision models like DocLayNet, which is essential for preserving the row-column relationships in rule tables \cite{auer_docling_2024}.
\end{enumerate}

We opted to go for option B, as this library demonstrates great promise in document parsing from multiple formats, spanning further than plain text and PDFs. Additionally, the layout awareness makes this more suitable and more adaptable to our task.

\subsubsection*{Embedding strategy}

To perform RAG, we need to be able to compute embeddings for efficient retrieval of chunks. Two main options arise: cloud-first or local-first. Either of them have many possible embedding algorithms. Here we highlight two of the best ones, according to MTEB (Massive Text Embedding Benchmark) rankings \cite{muennighoff_mteb_2023, enevoldsen_mmteb_2025}.

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Cloud-based} (e.g., OpenAI's \texttt{text-embedding-3})\textbf{:} high-performance embedding model, but requires sending data to external servers.
\item \textbf{Local-based} (e.g., \texttt{mxbai-embed-large} via \textit{Ollama})\textbf{:} runs entirely on premise.
\end{enumerate}

We opted for the local option (option B), as it balances sate-of-the-art performance with data quality.

\subsubsection*{Vector storage}

To perform the retrieval stage, we need to store the embeddings somewhere for later efficient retrieval. The main options consist of a local, in-memory index, or a dedicated vector database. Again, there are many in-memory options, as well as vector databases. Here, we highlight two options.

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{In-memory indexing} (e.g., FAISS/Chroma)\textbf{:} ephemeral storage that completely runs in the Python process memory.
\item \textbf{Dedicated vector database} (e.g., \textit{Qdrant})\textbf{:} there are multiple dedicated vector databases, in which we highlight Qdrant, an open source database that can be run entirely on-premise, and that implements the Hirearchical Naviable Small World (HNSW) algorithm \cite{malkov_efficient_2020} for logaritmic time complexity search ($O(\log N)$).
\end{enumerate}

We also selected the latter option. While option A is the most straight-forward for testing, and is incredibly useful for low-performance scenarios, it lacks the ability to scale and, most importantly, to persist the data. Restarting the Python process would mean recalculating all the embeddings. This does not happen with Qdrant (option B), as a dedicated vector store provides the necessary persistence and speed, among other perks like metadata filtering.

\vspace{2em}

In order to create a fully traceable process, we will also introduce a relational database (PostgreSQL) to keep track of the documents, chunks and where their embeddings are stored in the vector store (Qdrant). Inserting a relational database allows us to introduce more complex logic (something we cannot do in vector stores), and to store and persist the generated rules.

Taking this into account, we explain how documents are processed and stored into the relational database (PostgreSQL) and the vector store (Qdrant). A visual representation can be seen in \Cref{fig:diagram-3}.

\begin{enumerate}
    \item \textbf{Ingestion (Docling):} the documents are processed with Docling, which serializes them into Markdown while strictly preserving table structures and hierarchy.
    \item \textbf{Embedding generation (\texttt{mxbai-embed-large}):} text chunks are converted into vector representations using the \texttt{mxbai-embed-large} model. This model is specifically optimized for retrieval tasks and supports MRL, allowing for high-quality representations with a reduced memory footprint.
    \item \textbf{Storage (Qdrant):} vectors are stored in Qdrant collections utilizing the HNSW index for approximate nearest neighbor search.
    \item \textbf{Retrieval workflow:} when a query is received, the system embeds it using the local model and calculates cosine similarity ($S_C(u, v) = \frac{u \cdot v}{\|u\|\|v\|}$). The top-$k$ most relevant chunks are retrieved and injected into the LLM context.
\end{enumerate}

The rule generation workflow is now fundamentally altered to accommodate this new RAG stage. From now on, we use LangGraph to be able to properly implement more complex flows (see \Cref{llm-orchestration-and-workflows}). We use both the chunking from \Cref{sec:implementation-1} and the structured output from \Cref{sec:implementation-2-structured}. However, we introduce an additional step before rule generation: \textbf{gathering context}. We get the $k$ most relevant chunks (the ones most related to the chunk we're processing at a certain stage) to obtain relevant contextual information. This way, we extend the context shared to the LLM, effectively solving the ``lost in the middle'' phenomenon, and leading to more precise, grounded results.

In order to do this, we implement the following process for each of the chunks (see \Cref{fig:diagram-4}):

\begin{enumerate}
    \item \textbf{Retrieval:} using Qdrant's HNSW we get the $k=5$ chunks that are closest to the current chunk via cosine similarity.
    \item \textbf{Prompt:} we modify the prompt from \Cref{thread:0-baseline} to clearly specify the chunking strategy, separating the ``current chunk to analyze'' from the ``additional context'' retrieved in the previous step (\Cref{thread:4-rag}).
    \item \textbf{Structured output:} we use the structured output approach from \Cref{sec:implementation-2-structured}, making the model output the rules in a certain schema.
    \item \textbf{Storage:} we store the generated rules in the PostgreSQL database
\end{enumerate}



\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert industrial engineer.\\
Read the following document chunk and extract ALL operational and safety rules. Use the context provided, if relevant.\\[6pt]
CURRENT CHUNK TO ANALYZE:\\
\textcolor{purple}{\{chunk $c_i$\}}\\[6pt]
ADDITIONAL CONTEXT FROM ALL DOCUMENTS:\\
\textcolor{purple}{\{chunks related to chunk $c_i$\}}
\end{UserMessage}

\begin{AssistantMessage}
%\tcblower
\begin{lstlisting}[language=Python]
ExtractedRules(
    rules=[
        RangeRule(
            name="tray_7_temperature_normal",
            description="Maintain tray 7 temperature around 80.2°C for normal operation. This is a target operating temperature for proper separation in the column.",
            width=2.0,
            polarity=-1,
            variable="tray_7_temperature",
            min_val=78.0,
            max_val=82.0,
        ),
        RangeRule(
            name="reflux_flow_normal",
            description="Keep reflux flow around 440 units to ensure proper separation. This is the target operating flow rate for normal column operation.",
            width=20.0,
            polarity=0,
            variable='reflux_flow',
            min_val=420.0,
            max_val=460.0,
        ),
        ...
    ]
)
\end{lstlisting}
\end{AssistantMessage}


\end{minipage}

\caption{Message thread showcasing RAG workflow for extraction}
\label{thread:4-rag}
\end{figure}

\subsection*{Results}

This RAG-powered strategy yields promising results. It achieved \textbf{contextual resolution}, significantly reducing the number of ambiguous rules and introducing relevant context necessary for generating the rules. Also, we see more consistent results, especially in tables and diagrams. This was all done \textbf{locally} using Ollama: the entire extraction pipeline still operates air-gapped, meeting strict industrial security requirements.

However, we see significant \textbf{retrieval noise}: occasionally, the embeddings retrieved semantically related but contextually irrelevant chunks. At the same time, we still have some of the issues we had before related to how sensors are parsed: this strategy still has difficulty obtaining the proper IDs (as evident in \Cref{thread:4-rag}). We will talk about this later, in \Cref{sec:implementation-5-sensors,,sec:implementation-6-time}.

Still, there's plenty of room for improvement in the retrieval stage. The system lacks \textbf{external validation}, as it only retrieves internal document data but has no context regarding physics or regulation that might be necessary. In the end, we're obtaining the real-world knowledge from the foundational model's training data. In \Cref{sec:implementation-7-grounding} we'll discuss some approaches to enchance this.

\newpage

\section{Executable Python Rules}\label{sec:implementation-4-python-functions}


\begin{figure}[ht]
    \centering
    \includegraphics{diagram_05.pdf}
    \caption{Rule retrieval workflow with context gathering (RAG) and Python function-based rule output}
    \label{fig:diagram-5}
\end{figure}

In this section, we propose framing industrial rule extraction as a \textit{Program-of-Thougts} (PoT) task. The core approach is that disentangling logic from computation via an \textbf{abstract API} in a high-resource language like Python leverages the \textit{Chain-of-Code} (CoC) reasoning capabilities of LLMs. We hypothesize this minimizes hallucination and captures complex executable logic more effectively than low-resource Domain Specific Languages (DSLs) or static schemas.

The necessity for this approach stems from the \textbf{expressiveness gap} inherent in standard extraction targets. Static data structures like JSON or XML fail to adequately capture the logical complexity of industrial operations, which frequently involve arithmetic calculations (e.g., differential pressure $\Delta P$), conditional branching, and temporal aggregation. Attempting to encode these dynamic behaviors into static schemas inevitably results in semantic loss. To bridge this gap, we adopt the \textit{Program of Thoughts} (PoT) methodology. Research demonstrates that disentangling the reasoning process (performed by the LLM) from the computation process (delegated to a Python Interpreter) reduces calculation errors by over 12\% compared to standard CoT prompting \cite{chen_program_2023}. We apply this principle to industrial rules, positing that deterministic operations such as averaging or thresholding are best handled by an external interpreter rather than the probabilistic forward pass of the model.

Beyond simple arithmetic correctness, the act of ``thinking in code'' unlocks reasoning paths inaccessible to pure natural language models. The strict syntax of programming languages forces the model to structure semantic ambiguities into explicit logical control flows, such as \texttt{if/else} blocks. This structure effectively acts as a ``formalizer'' for ambiguous industrial text, resolving vagueness that NLP might overlook \cite{li_chain_2024}.

The choice of the target language is equally critical. While DSLs theoretically offer conciseness, LLMs exhibit significant performance degradation when generating low-resource languages. This struggle arises from scarcity in pre-training data \cite{joel_survey_2025}. Consequently, by targeting standard Python, this implementation leverages the model's extensive pre-training on billions of tokens of code (e.g., GitHub), avoiding the need for the expensive fine-tuning typically required to teach a model a custom JSON-logic DSL. Finally, to mitigate the primary risk of code generation (hallucination of non-existent functions) we employ an \textbf{abstract API}. Constraining the LLM to a pre-defined set of high-level APIs (e.g., \lstinline{robot.grasp()}) drastically improves reliability \cite{liang_code_2023}. We hypothesize that exposing a semantic abstract API (with the \lstinline{sensor.get_status()} interface) similarly grounds the LLM, forcing it to focus on logic structure rather than entity existence.

\subsection*{Implementation}

As expressed before, the main decision of this section is to decide which language will be used for the generation. We propose three options:


\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Custom DSL.} Creating a custom grammar for rules, requiring training an LLM on the new syntax.
\item \textbf{Direct SQL Generation.} Creating SQL queries.
\item \textbf{General Purpose Code (Python) with Abstract APIs.} Generating Python functions that call a ``mock API.''
\end{enumerate}


While option A could be interesting, it falls out of the scope of this work, is high-maintenance, and suffers from ``low-resource'' performance penalty \cite{joel_survey_2025}. Options B and C are interesting choices because foundational models have been extensively trained with these languages. However, SQL is too brittle and might not be suitable for all tasks. We choose option C due to the high proficiency in Python of state-of-the-art LLMs.

In order to properly allow the model to query the sensors for rule generation, we will define a ``mock API'' via a standard interface that we could think of as an \textit{API contract}. This interface will consist of an abstract \lstinline{status} class with a \lstinline{get} method that will be used like so:

\hspace{1cm}\lstinline{status.get(sensor, time_expression)}

This abstract method has two arguments:

\begin{itemize}
    \item \textbf{\texttt{sensor}:} the sensor ID.
    \item \textbf{\texttt{time\_expression}:} the time expression to retrieve sensor information. It could be just to get the sensor at a specific point in time (e.g., \lstinline{"10 minutes ago"}, \lstinline{"now"}) or to compute an aggregate statistic over a time period (e.g., \lstinline{"mean over the last 15 minutes"}, \lstinline{"standard deviation over the last 5 min"}).
\end{itemize}

In this first iteration we will instruct the LLM to use natural language for these parameters. We will do so through \textit{prompt engineering}. We will keep using the structured output from \Cref{sec:implementation-4-python-functions}, iterating incrementally from last section's (\Cref{sec:implementation-3-rag}) implementation (that is, we still use RAG and chunking).

\subsubsection*{Output schema}

The output schema is specified in \Cref{program:python-structured-output}. For each of the generated rules, we define:

\begin{itemize}
    \item \textbf{\texttt{rule\_name}:} the name of the rule (function), in snake case
    \item \textbf{\texttt{rule\_description}:} human-readable description of the rule (what the rule does)
    \item \textbf{\texttt{rule\_reasoning}:} the reasoning behind why this rule was generated. This is extremely important as LLMs can be instructed to show their reasoning, increasing visibility and explainability.
    \item \textbf{\texttt{rule\_source}:} section or location in the documentation where the rule came from. This is apart from other observability, we instruct the LLM to say which part of the chunk provided was used to generate the rule. This is also used to increase visibility and explainability.
    \item \textbf{\texttt{rule\_body}:} actual Python function using the \lstinline{status.get()} abstract API.
\end{itemize}

\begin{program}[hbtp]
\begin{CodeListing}
\begin{lstlisting}[language=Python]
from pydantic import BaseModel, Field


class PythonRule(BaseModel):
    """
    Schema for a single Python operational rule.

    This model defines the structure that the LLM must follow when extracting
    rules from industrial documentation.
    """

    rule_name: str = Field(
        ..., description="Function name in snake_case (e.g., 'column_high_pressure_alert')"
    )
    rule_description: str = Field(
        ..., description="Brief human-readable description of what the rule does"
    )
    rule_reasoning: str = Field(
        ..., description="Explanation of why this rule exists and its importance"
    )
    rule_source: str = Field(
        ..., description="Section or location in the documentation where this rule came from"
    )
    rule_body: str = Field(
        ..., description="Complete Python function code using status.get() with natural language time expressions"
    )


class ExtractedRules(BaseModel):
    """
    Container for all rules extracted from a document chunk.
    """

    rules: list[PythonRule] = Field(
        default_factory=list, description="List of all operational rules extracted from the document chunk"
    )
\end{lstlisting}
\end{CodeListing}
\caption{Pydantic model for \lstinline{ExtractedRules} outputting Python methods using the abstract API}\label{program:python-structured-output}
\end{program}


\subsubsection*{Prompt engineering}

After defining the output with proper documentation (remember this will also be passed to the LLM as context natively by LangChain), we define a prompt that properly instructs the LLM to perform this highly cognitively difficult task.

The prompt will be similar to the previous ones, including Python-related instructions. This prompt is displayed in \Cref{thread:5-python}.


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert industrial engineer and Python programmer specializing in extracting operational rules from documentation. Your task is to analyze industrial process documentation and extract operational rules as Python functions.\\[6pt]

CRITICAL REQUIREMENTS:

\begin{enumerate}
    \item PYTHON FUNCTION FORMAT:
    \begin{itemize}
    \item Function name must be in snake\_case (e.g., \lstinline{'column_high_pressure_alert'})
    \item Must accept one parameter: \lstinline{'status'}
    \item Must return a descriptive string in snake\_case if condition is met, otherwise return \lstinline{None}
    \item Use \lstinline{status.get()} API with natural language sensor and time expressions
    \end{itemize}
    \item NATURAL LANGUAGE TIME EXPRESSIONS:

    You can write time expressions in natural language, such as:
    \begin{itemize}
        \item \lstinline{"current"} or \lstinline{"current temperature"} - for current/latest value
        \item \lstinline{"5 minutes ago"} - for a specific point in the past
        \item \lstinline{"average over the last 10 minutes"} - for statistics over time intervals
    \end{itemize}
   
   Available statistics for intervals:

    \begin{itemize}
        \item average/mean - average value over time
        \item maximum/max - maximum value over time
        \item minimum/min - minimum value over time
        \item standard deviation/std - variability measure
        \item variance - variance measure
    \end{itemize}

    \item COMPREHENSIVE ANALYSIS:

    \begin{itemize}
        \item  Extract ALL operational rules from the documentation
        \item Create rules for explicit conditions AND inferred operational knowledge
        \item Use your engineering expertise to identify important monitoring conditions
        \item Include safety rules, operational limits, and process optimization rules
    \end{itemize}
\end{enumerate}\vspace{6pt}

CURRENT CHUNK TO ANALYZE:\\
\textcolor{purple}{\{chunk $c_i$\}}\\[6pt]
ADDITIONAL CONTEXT FROM ALL DOCUMENTS:\\
\textcolor{purple}{\{chunks related to chunk $c_i$\}}
\end{UserMessage}

\end{minipage}

\caption{Python function-based rule extraction prompt}
\label{thread:5-python}
\end{figure}

\subsection*{Results}

The results are promising. We attach two sample generated rules in \Cref{generatedrule:5-inferential-control-deviation,,generatedrule:4-reboiler-temperature-trend-high}. We see that the system successfully extracts complex rules involving \textbf{arithmetic} (e.g., use of \lstinline{abs}, as seen in \Cref{generatedrule:5-inferential-control-deviation}) which were impossible in the schema-oriented implementation of \Cref{sec:implementation-2-structured}.

Allowing the LLM to use natural language for sensors (e.g., ``column pressure'') and time expressions allowed it to generate rules properly instead of forcing it to guess a sensor ID during this stage. This greatly \textbf{decreases hallucination}.

Most importantly, the output is \textbf{valid Python syntax}, allowing us to use Python's \texttt{ast} module to inspect the code structure before execution (something we will do in \Cref{sec:implementation-8-verification}).

However, there is still work left to do. There are some notable \textbf{gaps for execution}: the generated code is syntactically correct but not yet executable, because the natural language specifications cannot be directly used. We will address this immediately both for sensor resolution (in \Cref{sec:implementation-5-sensors}) and resolving the temporal expressions (in \Cref{sec:implementation-6-time}).


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reflux\_flow\_rate\_optimization}}

\textsf{Description:} Monitor reflux flow rate for optimal column separation\\[-4pt]

\textsf{Reasoning:} Reflux flow rate of 441.20 units indicates current operating point; deviations may affect separation efficiency\\

\begin{lstlisting}[language=Python]
def reflux_flow_rate_optimization(status) -> str:
    current_reflux = status.get("reflux flow", "current")
    if current_reflux and abs(current_reflux - 441.20) / 441.20 > 0.1:
        return "reflux_flow_rate_optimization"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reflux\_flow\_rate\_optimization} (\Cref{sec:implementation-5-sensors})}
\label{generatedrule:5-inferential-control-deviation}
\end{figure}

\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reboiler\_temperature\_trend\_high}}

\textsf{Description:} Alert when reboiler outlet temperature shows sustained increasing trend\\[-4pt]

\textsf{Reasoning:} Sustained temperature increase may indicate fouling, reduced heat transfer, or control issues\\

\begin{lstlisting}[language=Python]
def reboiler_temperature_trend_high(status) -> str:
    current_temp = status.get("reboiler outlet temperature", "current")
    avg_temp_30min = status.get("reboiler outlet temperature", "average over the last 30 minutes")
    if current_temp and avg_temp_30min and (current_temp - avg_temp_30min) > 5.0:
        return "reboiler_temperature_trend_high"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reboiler\_temperature\_trend\_high} (\Cref{sec:implementation-4-python-functions})}
\label{generatedrule:4-reboiler-temperature-trend-high}
\end{figure}

\newpage

\section{Sensor Resolution}\label{sec:implementation-5-sensors}

\begin{figure}[ht]
    \centering
    \hspace*{-2.3cm}\includegraphics{diagram_06.pdf}
    \caption{Processing and ingesting including sensors CSV}
    \label{fig:diagram-6}
\end{figure}

\begin{figure}[ht]
    \centering
    \hspace*{-0.6cm}\includegraphics{diagram_07.pdf}
    \caption{Rule retrieval workflow including sensor parsing}
    \label{fig:diagram-7}
\end{figure}

In \Cref{sec:implementation-4-python-functions} we discuss two main issues that need to be addressed: \textit{sensor resolution} and \textit{resolving temporal expressions}. In this section, we will focus on the first one.

We hypothesize that \textit{decomposed prompting} (separating logic extraction from entity linking) combined with \textit{listwise generative reranking} (injecting the candidate list directly into the context) allows LLMs to resolve \textit{tail entity} industrial sensors with higher precision than discriminative vector retrieval by leveraging the model's ability to reason about functional constraints (units, physics) rather than mere semantic similarity.

Attempting to extract complex Python logic and perform database lookups simultaneously splits the LLM's attention, leading to performance degradation \cite{khot_decomposed_2023}. Decoupling the task into \textit{logic extraction} (\Cref{sec:implementation-4-python-functions}) and \textit{entity linking} (this section) reduces the cognitive load per step, ensuring the model focuses solely on disambiguation during the resolution phase.

Standard dense retrieval (RAG) is discriminative: it calculates geometric distance. However, industrial sensors often have high semantic overlap (e.g., \texttt{TIC-101} ``Column Top Temp'' vs. \texttt{TIC-102} ``Column Bottom Temp''), which creates ``hard negatives'' for vector models. Research in Generative Entity Linking (GEL) demonstrates that autoregressive models (LLMs) outperform vector retrievers on these tasks because they can reason about the functional context (e.g., ``this rule implies a control loop, so use the Controller tag, not the Indicator'') \cite{cao_autoregressive_2021}.

Industrial sensors are \textit{tail entities}: they do not appear in the LLM's pre-training data \cite{orr_bootleg_2020}. While RAG is common, it is shown that \textit{listwise reranking} (injecting the candidate list directly into the context) significantly outperforms vector retrieval when the candidate set fits in the context window \cite{qin_large_2024}. This approach allows the LLM to perform joint inference, comparing candidates against each other rather than scoring them in isolation.

\subsection*{Implementation}

There are many possible ways to implement this sensor retrieval.

\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Fuzzy String Matching.} Using Levenshtein distance to match terms.
\item \textbf{Zero-Shot Vector Retrieval.} Embedding the natural language term and finding the closest sensor embedding.
\item \textbf{LLM-Based Contextual Resolution.} Passing the natural language term \emph{along with the surrounding code logic} to an LLM to select the correct ID from a candidate list.
\end{enumerate}

The most natural way to proceed according to our research was option C, as it leverages the foundational models' reasoning capabilities regarding units, location and physics, amongst other knowledge. Making fuzzy string retrieval (option A) would fail on synonyms or when the search query is different from the actual sensor name. Making a vector retrieval (option B) would lack the reasoning capacities of the latter option, not being able to distinguish between redundant sensors or physically close but operationally distinct tags.

In order to proceed with the implementation, we start with \textbf{regex extraction}, leveraging the Python Abstract API we created in \Cref{sec:implementation-4-python-functions}. The problem boils down to ``replacing'' the text inside \lstinline{status.get("sensor_description", ...)} into the actual sensor ID.

Then, we get this description and inject it into a prompt, that includes the entire sensor list with its properties (ID, name, description, unit).\footnote{This approach prioritizes precision over scalability. However, most LLMs would have more than enough context to inject thousands of sensors, as shown in \Cref{tab:context_windows}, making this a simple but effective and realistic, production-ready approach.} This prompt effectively describes the \textit{\textbf{sensor mapping agent}} and is shown in \Cref{thread:6-sensor-mapping}.

To include the list of sensors, we introduce a new object: the \textbf{sensors CSV}. This CSV is composed of a series of rows, one per sensor, with the following columns:

\begin{itemize}
    \item \textbf{\texttt{id}:} the unique identifier of the sensor and the main topic of this section. In order to properly automate rules, we need to be able to map them into entities automated workflows can use.
    \item \textbf{\texttt{name}:} the name of the sensor, in natural language (e.g., ``valve pressure'').
    \item \textbf{\texttt{description}:} a description of the sensor, as detailed as possible (while keeping it succint). It allows the LLM to know what the sensor is measuring.
    \item \textbf{\texttt{unit}:} the unit of measurement of the sensor (e.g., ``\textmu mol/$\text{m}^2$/s''). This is extremely relevant, as the LLM can interpret differences in units provided in the context and the ones read by the sensor, and act accordingly.
    \item \textbf{\texttt{example}:} an example sensor reading.
\end{itemize}

In order to be able to properly parse the agent responses, we enforce structured output (see \Cref{sec:implementation-2-structured}) with the model shown in \Cref{program:sensor-mapping-agent-output}.

Afterwards, we rewrite the code by replacing the string (for example, transforming \lstinline{status.get("column pressure", ...)} into \lstinline{status.get("PIC-101", ...)}.

We tag rules with a \lstinline{SensorParsingStatus}. If a rule has all of the sensors properly resolved, the rule is marked as \texttt{OK}. Otherwise, it is marked as \lstinline{SENSORS_NOT_FOUND} and excluded from later processing stages.

We integrate this seemlessly into the rule extraction workflow from the previous section by having it as an additional stage, as can be seen in \Cref{fig:diagram-7}.


\subsection*{Results}

The system successfully \textbf{resolved ambiguous terms} that could have baffled the vector search, correctly using units to filter candidates, and ensuring \textbf{code integrity} after sensor resolution. Of all the generated rules, only 10\% had sensor resolution issues.

An example of this mapping process can be found in \Cref{generatedrule:5-inferential-control-deviation,,generatedrule:5-reboiler-temperature-trend-high} (which correspond to \Cref{generatedrule:5-inferential-control-deviation,,generatedrule:4-reboiler-temperature-trend-high}, respectively).


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}

You are a sensor mapping expert. Your task is to map natural language sensor references to their correct sensor IDs.\\[6pt]

AVAILABLE SENSORS:\\
\textcolor{purple}{\{sensors list with all attributes\}}\\[6pt]

RULE BODY (for context):\\
\textcolor{purple}{\{rule body\}}\\[6pt]

SENSOR REFERENCES TO RESOLVE:\\
\textcolor{purple}{\{list of extracted sensor references in natural language\}}\\[6pt]

For each sensor reference, identify the matching sensor ID from the available sensors list. Consider the sensor name, description, and unit to find the best match.

If a sensor cannot be matched, use null as the value.
\end{UserMessage}

\end{minipage}

\caption{Sensor mapping agent prompt}
\label{thread:6-sensor-mapping}
\end{figure}

\begin{program}[hbtp]
\begin{CodeListing}
\begin{lstlisting}[language=Python]
from pydantic import BaseModel, Field

class SensorMapping(BaseModel):
    """Encapsulates sensor resolution mapping."""

    sensor_description: str = Field(
        ..., description="Sensor description in natural language."
    )
    sensor_id: str = Field(
        ..., description="Sensor ID that corresponds to the description."
    )

class SensorMappings(BaseModel):
    mappings: list[SensorMapping]
    
\end{lstlisting}
\end{CodeListing}
\caption{Structured output schema of sensor mapping agent}\label{program:sensor-mapping-agent-output}
\end{program}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reflux\_flow\_rate\_optimization}}

\textsf{Description:} Monitor reflux flow rate for optimal column separation\\[-4pt]

\textsf{Reasoning:} Reflux flow rate of 441.20 units indicates current operating point; deviations may affect separation efficiency\\

\begin{lstlisting}[language=Python]
def reflux_flow_rate_optimization(status) -> str:
    current_reflux = status.get("14FI0024", "current")
    if current_reflux and abs(current_reflux - 441.20) / 441.20 > 0.1:
        return "reflux_flow_rate_optimization"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reflux\_flow\_rate\_optimization} (\Cref{sec:implementation-5-sensors})}
\label{generatedrule:5-inferential-control-deviation}
\end{figure}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reboiler\_temperature\_trend\_high}}

\textsf{Description:} Alert when reboiler outlet temperature shows sustained increasing trend\\[-4pt]

\textsf{Reasoning:} Sustained temperature increase may indicate fouling, reduced heat transfer, or control issues\\

\begin{lstlisting}[language=Python]
def reboiler_temperature_trend_high(status) -> str:
    current_temp = status.get("14TI0046", "current")
    avg_temp_30min = status.get("14TI0046", "average over the last 30 minutes")
    if current_temp and avg_temp_30min and (current_temp - avg_temp_30min) > 5.0:
        return "reboiler_temperature_trend_high"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reboiler\_temperature\_trend\_high} (\Cref{sec:implementation-5-sensors})}
\label{generatedrule:5-reboiler-temperature-trend-high}
\end{figure}

\newpage

\section{Time Expression Resolution}\label{sec:implementation-6-time}

\begin{figure}[ht]
    \centering
    \hspace*{-0.6cm}\includegraphics{diagram_08.pdf}
    \caption{Rule retrieval workflow including time expressions parsing}
    \label{fig:diagram-8}
\end{figure}

LLMs exhibit a known weakness in temporal arithmetic and temporal reasoning. Although they excel at linguistic tasks, they struggle to ground temporal concepts (for example, understanding that ``10 minutes'' denotes a duration rather than a mere string) or to perform consistent arithmetic on time deltas (such as determining that ``10 minutes ago'' requires subtracting 600 seconds from $t_{now}$) \cite{chu_timebench_2024}. When the model is asked to directly produce executable time arithmetic, such as \lstinline{now() - timedelta(minutes=10)}, the result frequently contains syntax or semantic hallucinations, leading to high error rates \cite{chang_survey_2024}.

Conventional \textit{Temporal Information Extraction} (TIE) systems such as \textit{SUTime} \cite{chang_sutime_2012} and \textit{HeidelTime} \cite{strotgen_baseline_2015} are optimized for absolute temporal normalization, for example mapping ``next Friday'' to a calendar date like ``2023-11-24.'' Industrial monitoring, however, relies almost exclusively on relative temporal expressions such as ``rolling average over the last 10 minutes.'' Prior work shows that general-purpose tools perform poorly on these domain-specific sliding-window constructions, motivating the need for a custom semantic parser \cite{strotgen_multilingual_2013}.

To bridge this gap, we hypothesize that natural-language interpretation should be decoupled from execution. Mapping language to a constrained intermediate logical form--rather than allowing an LLM to produce direct executable code--has been shown to substantially improve robustness \cite{shin_constrained_2021}. We propose adopting a PromQL-style\footnote{PromQL (Prometheus Query Language) is a functional and declarative query language used to select and aggregate time-series data stored in Prometheus, enabling monitoring, visualization, and alerting on the performance of infrastructure and applications by querying real-time metrics with a powerful and flexible syntax \cite{brazil_prometheus_2018}.} range grammar as this ILF, enabling deterministic translation from fuzzy temporal language into precise queries.

\subsection*{Implementation}

Based on our research, we could implement various time parsing strategies:


\begin{enumerate}[label=\textbf{\Alph*.}]
\item \textbf{Direct Python Code Generation (\texttt{dateutil} / \texttt{datetime} / \texttt{pandas}):} asking the LLM to write \texttt{pandas} or \texttt{dateutil} methods directly.
\item \textbf{Standard NLP Taggers:} use \textit{SUTime} or \textit{HeidelTime}.
\item \textbf{Semantic parsing to intermediate grammar:} converting natural language to a strict, PromQL-inspired format (e.g., \lstinline{10m:}, \lstinline{5m:2m}) which is then deterministically compiled.
\end{enumerate}

While option A is the easiest to implement, it does not deliver good results, with high syntax errors, hallucinations and reduced expressive capacity. While it would make sense to use option B with already implemented libraries, we have already discussed its limitations. That is why we decided to create the time grammar specified in option C, focusing on making it understandeable for LLMs.

\subsubsection*{Formal time grammar}

We define a strict schema inspired by PromQL's range vectors \cite{brazil_prometheus_2018}, which naturally represents the sliding windows used in industrial telemetry. We distinguish between time \textit{points} and \textit{intervals}.

\begin{itemize}
\item \textbf{Time Point:} A single offset from \emph{now} (e.g., \texttt{0} = current, \texttt{5m} = 5 mins ago).
\item \textbf{Time Interval:} A range defined by start/end offsets (e.g., \texttt{10m:} = from 10m ago to now; \texttt{2h:1h} = from 2h ago to 1h ago).
\item \textbf{Statistic Requirement:} Intervals \emph{must} be accompanied by a statistical function (mean, max, std); Points \emph{must not}.
\end{itemize}

The result is a simple grammar that can be then parsed and used in streaming (see \Cref{ch:6-operational-runtime-dynamic-verification}). The possible units of time are defined in a \texttt{TimeUnit} enum (see \Cref{tab:timeunit}).

\vspace*{2em}

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}
        \toprule
        \textbf{Unit of time} & \textbf{Symbol in grammar} \\
        \midrule
        microsecond & \texttt{us} \\
        milisecond & \texttt{ms} \\
        second & \texttt{s} \\
        minute & \texttt{m} \\
        hour & \texttt{h} \\
        day & \texttt{d} \\
        \bottomrule
    \end{tabular}
    \footnotesize
    \caption{Possible time units (in \texttt{TimeUnit} enum)}
    \label{tab:timeunit}
\end{table}

\subsubsection*{Time parsing and validation}

We implement a similar approach to the sensor resolution (\Cref{sec:implementation-5-sensors}) by using regex and getting the string corresponding to the time expression. We then use a \textit{\textbf{time parser agent}} that consists of a simple prompt to translate the natural expression into the grammar. The prompt used can be seen in \Cref{thread:6-time-parsing}, along with the output schema in \Cref{program:time-parsing-output}.

Finally, we add another tag: \texttt{TimeParsingStatus}. If a rule has all of the time expressions properly parsed, the rule is marked as \texttt{OK}. Otherwise, it is marked as \lstinline{PARSE_ERROR} and excluded from leter processing stages.

\subsection*{Results}

This improvement successfully normalized diverse time expressions, allowing for direct queries to the provided information. It \textbf{correctly standardized} the diverse time expressions, such as ``average over the last 10 minutes'' \(\to\) \texttt{"10m:",\ "mean"}, or ``current temperature'' \(\to\) \texttt{"0",\ None}.

In \Cref{generatedrule:6-inferential-control-deviation,,generatedrule:6-reboiler-temperature-trend-high} we see how the rules changed after the processing of this time parsing agent (from \Cref{generatedrule:5-inferential-control-deviation,,generatedrule:5-reboiler-temperature-trend-high}, respectively).


Having this format, we are now able to programatically check any of the rules without the need for human intervention. In \Cref{ch:6-operational-runtime-dynamic-verification}, we detail how we can dynamically verify rules on runtime thanks to this spec.


\begin{program}[hbtp]
\begin{CodeListing}
\begin{lstlisting}[language=Python]
from pydantic import BaseModel, Field
from src.agent.domain.time.statistic import Statistic


class TimeMapping(BaseModel):
    """Encapsulates time parsing mapping."""

    time_description: str = Field(
        ..., description="Time expression description in natural language."
    )
    time_expression: str = Field(
        ..., description="Time point or interval in custom grammar."
    )
    time_statistic: Statistic | None = Field(
        ..., description="Aggreate statistic, only for time intervals"
    )


class TimeMappings(BaseModel):
    mappings: list[TimeMapping]
\end{lstlisting}
\end{CodeListing}
\caption{Structured output schema of time parsing agent}\label{program:time-parsing-output}
\end{program}

\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
Parse the following natural language time expression into structured format.\\[6pt]

TIME UNITS: us (microseconds), ms (milliseconds), s (seconds), m (minutes), h (hours), d (days)\\[6pt]

TIME FORMATS:
\begin{itemize}
    \item Point (single moment): \lstinline{"0"} for current, \lstinline{"5m"} for 5 minutes ago, \lstinline{"1h30m"} for 1 hour 30 min ago.
    \item Interval (range): \lstinline{"5m:"} for last 5 minutes, \lstinline{"10h:2m"} for from 10 hours ago to 2 minutes ago
\end{itemize}\vspace{1em}

STATISTICS (only for intervals): mean, max, min, std, variance\\[6pt]

EXAMPLES:
\begin{itemize}
    \item \lstinline|"current"| $\rightarrow$ \lstinline|{"time": "0", "statistic": null}|
    \item \lstinline|"5 minutes ago"| $\rightarrow$ \lstinline|{"time": "5m", "statistic": null}|
    \item \lstinline|"an hour ago"| $\rightarrow$ \lstinline|{"time": "1h", "statistic": null}|
    \item \lstinline|"average over the last ten minutes"| $\rightarrow$ \lstinline|{"time": "10m:", "statistic": "mean"}|
    \item \lstinline|"mean from the last hour"| $\rightarrow$ \lstinline|{"time": "1h:", "statistic": "mean"}|
    \item \lstinline|"mean from the hour before the last hour"| $\rightarrow$ \lstinline|{"time": "2h:1h", "statistic": "mean"}|
    \item \lstinline|"standard deviation over the last 30 minutes"| $\rightarrow$ \lstinline|{"time": "30m:", "statistic": "std"}|
    \item \lstinline|"maximum pressure in the last hour"| $\rightarrow$ \lstinline|{"time": "1h:", "statistic": "max"}|
    \item \lstinline|"last 5 minutes"| $\rightarrow$ \lstinline|{"time": "5m:", "statistic": null}|
\end{itemize}\vspace{1em}

Input: \textcolor{purple}{\texttt{"\{natural\_language\_expr\}"}}
\end{UserMessage}

\end{minipage}

\caption{Time parsing agent prompt}
\label{thread:6-time-parsing}
\end{figure}



\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reflux\_flow\_rate\_optimization}}

\textsf{Description:} Monitor reflux flow rate for optimal column separation\\[-4pt]

\textsf{Reasoning:} Reflux flow rate of 441.20 units indicates current operating point; deviations may affect separation efficiency\\

\begin{lstlisting}[language=Python]
def reflux_flow_rate_optimization(status) -> str:
    current_reflux = status.get("14FI0024", "0")
    if current_reflux and abs(current_reflux - 441.20) / 441.20 > 0.1:
        return "reflux_flow_rate_optimization"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reflux\_flow\_rate\_optimization}}
\label{generatedrule:6-inferential-control-deviation}
\end{figure}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{reboiler\_temperature\_trend\_high}}

\textsf{Description:} Alert when reboiler outlet temperature shows sustained increasing trend\\[-4pt]

\textsf{Reasoning:} Sustained temperature increase may indicate fouling, reduced heat transfer, or control issues\\

\begin{lstlisting}[language=Python]
def reboiler_temperature_trend_high(status) -> str:
    current_temp = status.get("14TI0046", "0")
    avg_temp_30min = status.get("14TI0046", "30m:", "mean")
    if current_temp and avg_temp_30min and (current_temp - avg_temp_30min) > 5.0:
        return "reboiler_temperature_trend_high"
    return None
\end{lstlisting}
\end{RuleMessage}

\end{minipage}

\caption{\texttt{reboiler\_temperature\_trend\_high}}
\label{generatedrule:6-reboiler-temperature-trend-high}
\end{figure}


\newpage

\section{External Knowledge Grounding}\label{sec:implementation-7-grounding}


\begin{figure}[ht]
    \centering
    \hspace*{-1.3cm}\includegraphics{diagram_09.pdf}
    \caption{Rule retrieval workflow including external knowledge grounding}
    \label{fig:diagram-9}
\end{figure}


LLMs suffer from a static ``knowledge cutoff’’ (e.g., GPT-4o: October 2023 \cite{openai_gpt-4o_2024}). It has been demonstrated that they exhibit severe degradation when asked about post-cutoff factual changes, such as newly issued emission standards \cite{cheng_dated_2024}. Relying solely on pre-trained weights for industrial compliance therefore introduces a critical temporal bias, rendering the system blind to recent regulatory updates.

While RAG (\Cref{sec:implementation-3-rag}) grounds the model in \textit{internal} documentation, it offers no mechanism to detect whether the internal corpus itself is outdated or physically unsound. \textit{WebGPT} shows that augmenting LLMs with a browsing or search tool substantially reduces hallucination on open-domain questions by allowing the model to cross-validate its generations against authoritative external sources \cite{nakano_webgpt_2022}. This effectively creates a double-verification loop that functions as a factuality layer beyond static pre-training.

Traditional search engines such as Google return SERPs\footnote{SERP: Search Engine Results Page} optimized for human navigation rather than machine consumption. These require an additional scraping stage that introduces large volumes of noisy HTML, dramatically increasing token usage. Prior work suggests that \textit{Search-Augmented Generation} (SAG) workflows achieve the greatest gains when relying on ``answer-first'' APIs that pre-process and synthesize content before it reaches the model \cite{lazaridou_internet-augmented_2022}. Such APIs maximize the agent’s \textit{Signal-to-Noise Ratio} (SNR)\footnote{In benchmarking, SNR measures the separation between meaningful model performance (signal) and evaluation variability or irrelevance (noise). High SNR indicates that the system extracts and generates a high proportion of useful information \cite{yuan_signal--noise_2019, heineman_signal_2025}.} and reduce both latency and cost compared to ingesting raw HTML DOM trees.

\subsection*{Implementation}

There are many possible options to implement this SAG, spanning from APIs specific for LLMs to more manual work by scraping websites or using output of general purpose APIs:

\begin{enumerate}[label=\textbf{\Alph*.}]
    \item \textbf{Tavily:} Returns synthesized answers and clean markdown, not just links. Benchmarks show 93.3\% accuracy on \textit{SimpleQA} tasks, outperforming raw SERP wrappers in zero-shot retrieval \cite{hadar_tavily_2025}.
    \item \textbf{Serper:} Best for ``deep long tail'' retrieval. Excellent if we needed to find a specific obscure manual but requires building a complex ``scraper + parser'' pipeline to extract clean text.
    \item \textbf{DuckDuckGo:} Unreliable (no SLA), aggressive rate limiting, and returns only short snippets (meta descriptions), which lack the detailed technical values needed for verification.
    \item \textbf{Google Programmable Search Engine (PSE):} High setup complexity, returns noisy JSON full of irrelevant metadata (ads, tracking pixels).
\end{enumerate}

Due to the ease of use and generous free tier, we decide to go towards option A.

We will implement an additional phase after the RAG (\Cref{sec:implementation-3-rag}) in which we perform another LLM call to decide which search queries to make, perform the search and inject the responses in the extraction prompt as context.

 \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  
  \item
    \textbf{Query Synthesis:} The LLM analyzes the extracted rule (e.g.,
    ``vent gas must be \textless{} 50 ppm'') and generates a query: \emph{``EPA regulation limit for propane vent
    gas flaring 2024''}. We do this through a prompt (\Cref{thread:7-ground-with-search}) and enforcing \lstinline{list[str]} as output schema.
  \item
    \textbf{Retrieval (Tavily):} The agent calls the search API.
    Crucially, we use:
    \begin{itemize}
        \item \lstinline{include_raw_content=False}: to get the synthesized answer.
        \item \lstinline{search_depth="advanced"}: triggering Tavily's internal ``deep search'' agent which aggregates data from 5-10 sources before returning a response.
    \end{itemize}
  \item
    \textbf{Injection into prompt:} We modify the rule generation prompt from \Cref{sec:implementation-4-python-functions} (see \Cref{thread:5-python}) to include the search results (see \Cref{thread:implementation-7}).
  \end{enumerate}

\subsection*{Results}

Incorporating external grounding yielded promising results. Looking at the traces, we can see that the agent was able to \textbf{introduce valuable information} as context, making appropriate questions and filling out missing information from other sources (see \Cref{thread:tavily-calls} for an example of tool call). Additionally, using Tavily's integration enabled the LLM to use text that was processed for \textbf{enhanced understanding}.

However, we need to take into account the \textbf{latency} this process introduces, as well as any possible \textbf{costs}. It is important to notice that grounding might not be suitable for all tasks. Also, we were able to observe that the agent wanted to search regarding proprietary terms, or some that do not exist outside the provided documentation. Further prompt engineering work would be suitable here to improve this behavior.


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert industrial engineer analyzing process documentation.\\
Your task is to identify what external knowledge would help extract better operational rules from this document chunk.\\[6pt]

DOCUMENT CHUNK:\\
\textcolor{purple}{\{chunk $c_i$\}}\\[6pt]
CONTEXT FROM SIMILAR DOCUMENTS:\\
\textcolor{purple}{\{chunks related to chunk $c_i$\}}\\[6pt]

CHAIN OF THOUGHT PROCESS:
\begin{enumerate}
    \item What are the key technical concepts, equipment, or processes mentioned?
    \item Which concepts might need external clarification (industry standards, best practices, typical ranges)?
    \item What specific information would help extract more accurate operational rules?
\end{enumerate}\vspace{1em}

Based on your analysis, provide \textcolor{purple}{\{\texttt{grounding\_min}\}} to \textcolor{purple}{\{\texttt{grounding\_max}\}} focused web search queries that would clarify:
\begin{itemize}
    \item Industry standards for mentioned processes
    \item Typical operational ranges or thresholds
    \item Safety considerations for equipment/chemicals mentioned
    \item Best practices for the process described
\end{itemize}
\end{UserMessage}

\end{minipage}

\caption{Grounding agent prompt}
\label{thread:7-ground-with-search}
\end{figure}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert industrial engineer and Python programmer specializing in [...]\\[6pt]

CRITICAL REQUIREMENTS: [...]\\[6pt]

CURRENT CHUNK TO ANALYZE:\\
\textcolor{purple}{\{chunk $c_i$\}}\\[6pt]
ADDITIONAL CONTEXT FROM ALL DOCUMENTS:\\
\textcolor{purple}{\{chunks related to chunk $c_i$\}}\\[6pt]
EXTERNAL KNOWLEDGE (from web search):\\
\textcolor{purple}{\{search agent output\}}
\end{UserMessage}

\end{minipage}

\caption{Include external knowledge in rule extraction prompt}
\label{thread:implementation-7}
\end{figure}

\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{ToolMessage}
\textsf{\textbf{Tavily Search} with query:}

``sour water collection system SWS industry standards operating parameters''\\

\textsf{Results:}

\begin{lstlisting}[language=JSON]
[{"url": "https://www.sciencedirect.com/...", "title": "Storage stability of FCC light cycle oil - ScienceDirect.com", "content": "The boiling range of diesel fuel is typically 180–360 °C. Hydrotreated ... Another fractions are produced through secondary processing of heavy oils (boiling", "score": 0.7163977, "raw_content": null}, {"url": "https://www.osti.gov/...", "title": "Light cycle oil from the FCC unit (Conference) | SciTech Connect", "content": "LCO is defined in our studies as 430ºF to 640ºF boiling range hydrocarbons. No. 1 (diesel quality) and No. 2 fuel oils generally", "score": 0.7092009, "raw_content": null}, ...]
\end{lstlisting}
\end{ToolMessage}

\end{minipage}

\caption{Example of Tavily tool call}
\label{thread:tavily-calls}
\end{figure}


\newpage 

\section{Rule Verification}\label{sec:implementation-8-verification}


\begin{figure}[ht]
    \centering
    \hspace*{-1.3cm}\includegraphics{diagram_10.pdf}
    \caption{Rule retrieval workflow including rule verification}
    \label{fig:diagram-10}
\end{figure}

Recent work has challenged the assumption that LLMs are capable of reliably identifying and correcting their own reasoning errors. The phenomenon, formally described as the \textit{\textbf{Self-Correction Fallacy}} \cite{huang_large_2024}, shows that when models are prompted to ``reflect'' or ``debug'' their outputs, they often hallucinate correctness or even degrade in performance, particularly in tasks that involve multi-step logical consistency. This aligns with other findings that demonstrate that self-repair in code generation rarely resolves subtle logic faults unless the model is guided by an external feedback signal such as a compiler diagnostic \cite{olausson_is_2024}. Together, these results suggest that neural self-correction alone cannot satisfy the reliability requirements of industrial environments, where silent logical errors are significantly more harmful than surface-level syntactic issues.

To address this limitation, we propose a \textbf{verification pipeline} designed to impose external structure and correctness constraints on LLM-generated code. The central premise is that trustworthiness cannot emerge solely from probabilistic refinement; instead, it requires the integration of symbolic mechanisms capable of enforcing guarantees independent of the model’s internal distributions. Static analysis plays a crucial role in this architecture. Unlike execution-based validation such as unit testing which, although powerful, is computationally expensive and often infeasible to simulate for complex industrial systems, static methods operate on the \textit{Abstract Syntax Tree} (AST) and provide an efficient way to eliminate entire classes of errors. Grammar-level and AST-level constraints can prevent syntactic violations, type mismatches, and structurally invalid program constructs at negligible computational cost compared to executing every candidate program in a sandbox \cite{poesia_synchromesh_2022}.

Beyond static safety, the pipeline incorporates symbolic constraints to ensure semantic correctness. By interpreting the generated code as an AST, it becomes possible to traverse the structure and apply formal verification principles that enforce domain-specific invariants, such as requiring every temporal interval to include an associated statistical operator. These invariants act as symbolic oracles that restrict the solution space to programs that are provably aligned with the specification. The effectiveness of such symbolic filtering is well supported in the literature: it is shown that constraining neural program synthesis with formally defined oracles significantly improves the functional reliability of the produced code, especially in settings where minor structural deviations can lead to major downstream failures \cite{austin_program_2021}.

In summary, our hypothesis is that neural self-correction must be complemented --not replaced-- by symbolic reasoning layers that provide structural and semantic guarantees at low computational cost. The proposed pipeline operationalizes this idea by combining Static Analysis as a fast, deterministic pre-filter with formal symbolic constraints that enforce correctness invariants. This architecture aims to deliver a level of reliability that is fundamentally unattainable through model self-reflection alone, thereby aligning neural generation with the stringent correctness requirements of industrial rule synthesis.

\subsection*{Implementation}

A range of implementation strategies was evaluated to determine the most effective approach for consolidating and optimizing the set of extracted industrial rules.

\begin{itemize}
\item \textbf{Option A: Execution/Unit Testing.} Running every rule in a sandboxed environment with mock data.
\item \textbf{Option B: LLM Self-Correction.} Asking the LLM to ``check its own work.''
\item \textbf{Option C: Static Analysis via AST.} Parsing the code structure to validate syntax and extracting function calls to validate arguments against constraints.
\end{itemize}

We decided to go towards the latest option, as it is fast, deterministic and accurate for structure checking. In the case of option A, there's significant computational overhead, and difficulty in generating realistic mock data for complex multi-sensor correlations. In the case of option B, we already discussed how research shows LLMs struggle to correct their own logic errors without external feedback.

We implement a three-layer process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}

\item
  \textbf{Layer 1: Syntactic Verification (\texttt{ast.parse})}: we parse the raw Python string into an AST. If \lstinline{ast.parse()} raises a \lstinline{SyntaxError}, the rule is immediately flagged.
\item
  \textbf{Layer 2: Domain Entity Verification (Sensor Auditing)}: a custom \lstinline{StatusCallExtractor} (inheriting from \texttt{ast.NodeVisitor}) traverses the tree to find all \lstinline{status.get()} calls. We then extract the sensor IDs and cross-reference them against the validated sensor collection. Any ID not present in the database triggers an \lstinline{INVALID_SENSOR} flag.\footnote{This is redundant with the processing of \Cref{sec:implementation-5-sensors}. However, we decide to proceed for extra caution and to add an additional layer of verification.}
\item
  \textbf{Layer 3: Temporal-Statistical Logic Check}: we use the extracted call from the previous layer and validate the logical consistency of the time expressions. The implementation already has incorporated a regex validator that raises a \lstinline{ValidationError} if the format is wrong. We also check for points vs. intervals, the statistic can only be shown in the latter. Violations would trigger \lstinline{INVALID_STATISTIC} (if it is part of a point expression or is not an allowed statistic).
\end{enumerate}

\subsection*{Results}

All the generated rules were verified to have \textbf{syntactically valid Python code} via \lstinline{ast}. Out of the rules that were valid (had proper sensor and time parsing), \textbf{100\% of them were verified to be correct}. This shows us our system is robust.

While we might think this robustness might make this stage unnecessary, that statement is no further from truth: it is extremely important to have a validation layer as we expressed earlier.

In the future, integrating type check mechanisms (using \texttt{mypy} or other static type checkers) would be interesting. Also, we could go beyond and verify more than the Python code, performing \textbf{sanity checks}: ``physic engines'' that check if the values make sense (the comparisons or the units of measurement make sense, for example).

\newpage

\section{Rule Consolidation}\label{sec:implementation-9-consolidation}


\begin{figure}[ht]
    \centering
    \hspace*{-2.3cm}\includegraphics{diagram_11.pdf}
    \caption{Full rule extraction process, including rule extraction and consolidation workflows}
    \label{fig:diagram-11}
\end{figure}

In software engineering, duplicates range from Type-1 (exact copy) to Type-4 (Semantic Clones) -- code that is syntactically different but functionally identical \cite{svajlenko_evaluating_2015}. Naive chunking produces massive Type-4 redundancy (e.g., ``keep pressure < 50'' vs. ``max pressure 50''). While traditional \textit{Association Rule Mining} (e.g., \textit{FP-Growth} \cite{han_mining_2000}, \textit{Apriori} \cite{agrawal_fast_1994}) excels at finding frequent correlations in structured data, it fails here because Python rules are not static ``itemsets'' but executable logic with variable control flow. Other ML approaches (e.g., \textit{Random Forests}, \textit{Jaccard Index}) fail here because they rely on token overlap and cannot capture the \textit{semantic intent} of the logic. \cite{svajlenko_evaluating_2015}

Instead, specialized DL models (e.g., \textit{CodeBERT}, \textit{GraphCodeBERT}) are the current academic standard for clone detection. However, their performance is heavily dependent on fine-tuning with large-scale labeled datasets. In an industrial ``cold start'' scenario, no such dataset exists.

LLMs can also be used for code detection. However, while models like GPT-4 may not beat fine-tuned \textit{CodeBERT} on synthetic benchmarks, they demonstrate superior generalization to novel code structures and can explain their reasoning, which is critical for the validation required in industrial settings \cite{zhang_assessing_2024}.

However, injecting all extracted rules into a single global context for cleaning creates the \textit{\textbf{``distractor effect.''}} Unrelated information in the context window exponentially degrades reasoning accuracy \cite{shi_large_2023}. Therefore, consolidation must occur in topologically isolated batches, grouping rules via a dependency graph (e.g., all nodes touching \texttt{TIC-101}), to maintain the high SNR required for precise merging.

We hypothesize that an LLM-based ``semantic linker'' will minimize the ``distractor effect'' and provide the necessary reasoning to merge complex Python logic that traditional symbolic solver and association rule algorithms cannot represent.

\subsection*{Implementation}

We reevaluate multiple ways to implement this rule consolidation:

\begin{enumerate}[label=\textbf{\Alph*.}]

\item
  \textbf{Syntactic Code Clone Detection (Traditional):} {Method:} Using AST fingerprinting or
    token-based similarity (e.g., Jaccard Index, SourcererCC) to find duplicate rules.
\item
  \textbf{Symbolic Logic Solvers (SMT/Z3):} Converting Python rules into First-Order Logic
    formulas and using an SMT solver (like Z3) to mathematically prove
    equivalence or redundancy (e.g., verifying that
    \texttt{x\ \textgreater{}\ 50} implies
    \texttt{x\ \textgreater{}\ 40}).
\item
  \textbf{Vector-Based Semantic Clustering:} Embedding rule bodies using CodeBERT and clustering
    centroids.

\item
  \textbf{LLM-Based Dependency Batching:} Grouping rules by Sensor ID topology and using an LLM
    as a ``semantic compiler'' to refactor code.
\end{enumerate}

In case of option A, traditional algorithms are blind to semantics. While they can identify exact duplicates, they fail to merge the logic. Regarding option B, while mathematically rigorous, it would require a perfect formal translation of Python code, something difficult due to the stochastic nature of LLMs.

Out of the two last options, option C is interesting and good for grouping rules, but not suitable for merging, as it lacks the reasoning capabilities that option D has. The latter combines the ``grouping'' benefit of clustering with the code generation capabilities required to rewrite and merge Python functions.

We will implement this through an additional workflow. It is important to note that this workflow uses as input the rules generated by the extracted workflow up until now. \Cref{fig:diagram-11} shows how it is integrated into the full process, including the rule extraction workflow.

It consists on three stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}

\item
  \textbf{Dependency Analysis:} The system scans the extracted Python code (see \Cref{sec:implementation-4-python-functions}) to
    build a dependency map (e.g., \texttt{Rule\ 1} $\rightarrow$ \texttt{\{TIC-101,\ PIC-204\}}). It creates sensor groups using a clustering strategy. Rules touching the same sensors are grouped into a single batch (capped at
    \texttt{MAX\_RULES\_PER\_BATCH} to preserve attention).
\item
  \textbf{Semantic Optimization:} The LLM acts as an optimizer with three specific operators:

    \begin{itemize}
    
    \item
      \textbf{REMOVE:} For exact semantic duplicates (e.g., chunk
      overlap).
    \item
      \textbf{MERGE:} For combining related logic (e.g., combining
      ``High Alarm'' and ``Low Alarm'' into a single
      \texttt{check\_pressure\_range()} function).
    \item
      \textbf{SIMPLIFY:} For reducing boolean complexity (e.g.,
      \texttt{A\ \textgreater{}\ 50\ AND\ A\ \textgreater{}\ 60} \(\to\)
      \texttt{A\ \textgreater{}\ 60}).
    \end{itemize}
\item[] This optimization is effectively an LLM call with the prompt specified in \Cref{thread:9-consolidation} and the structured output from \Cref{program:consolidated-output}.
\item
  \textbf{Safety Verification:} The \emph{newly generated} consolidated code must pass the same
    rigorous verification as the original rules (see \Cref{sec:implementation-8-verification}). This
    prevents the optimizer from introducing syntax errors or
    hallucinating new sensors during the merge.
\end{enumerate}

\subsection*{Results}

The system achieved an impressive \textbf{consolidation ratio} of 5:1 (sustained across several tests, on average processing 290 rules resulted in 57 consolidated rules), generating clean, high-value consolidated rules.

Additionally, the proposed framework allows us to \textbf{explain} how those rules were consolidated, and from which ones they originate, which is critical for further verification.

This strategy successfully \textbf{identified conflicting rules} and prompted resolution, generating rules of much \textbf{higher quality}. An example can be seen in \Cref{generatedrule:consolidated}, a rule consolidated from the rules in \Cref{generatedrule:consolidated-before}. Notice how the conditions are seamlessly integrated into one single function.

Future implementations could have more sophisticated conciliation mechanisms with cross-batch dependencies (using graph-based clustering algorithms), as well as allowing the system to use RAG (\Cref{sec:implementation-3-rag}) and SAG (\Cref{sec:implementation-7-grounding}) to clarify conflicts.


\begin{program}[hbtp]
\begin{CodeListing}
\begin{lstlisting}[language=Python]
from enum import StrEnum
from pydantic import BaseModel, Field
from src.agent.domain.models import PythonRule


class RuleConsolidationActionType(StrEnum):
    REMOVE = "remove"
    MERGE = "merge"
    SIMPLIFY = "simplify"


class RuleConsolidation(BaseModel):
    """Encapsulates consolidated rule."""

    action_type: RuleConsolidationActionType = Field(
        ..., description="Type of consolidation performed."
    )
    input_rule_ids: list[int] = Field(
        ..., description="IDs of rules used in consolidation."
    )
    output_rule: PythonRule | None = Field(
        ..., description="Consolidated rule."
    )
    confidence: float = Field(
        ..., description="Confidence coefficient in consolidated rule, in [0, 1]."
    )
    reasoning: str = Field(
        ..., description="Reasoning behind consolidation."
    )


class TimeMappings(BaseModel):
    mappings: list[TimeMapping]
\end{lstlisting}
\end{CodeListing}
\caption{Structured output schema of consolidated rules}\label{program:consolidated-output}
\end{program}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{UserMessage}
You are an expert at optimizing Python rule sets for industrial process monitoring.\\[6pt]

AVAILABLE SENSORS:\\
\textcolor{purple}{\{list of sensors\}}\\[6pt]

AVAILABLE STATISTICS:\\
\textcolor{purple}{\{list of available statistics\}}\\[6pt]

RULES TO CONSOLIDATE:\\
\textcolor{purple}{\{list of rules to consolidate\}}\\[6pt]

YOUR TASK:

\begin{enumerate}
    \item Identify REDUNDANT rules:
    \begin{itemize}
    \item Exact duplicates (same sensor, same threshold, same logic)
    \item Semantic duplicates (different wording but identical meaning)
    \item Subset rules (one rule is contained in another)
    \end{itemize}
    \item Identify rules that can be MERGED:
    \begin{itemize}
        \item Multiple conditions for same alert (combine with AND)
        \item Same action for different triggers (combine with OR)
        \item Hierarchical alerts (warning + critical thresholds for same sensor)
    \end{itemize}
    \item Identify rules that can be SIMPLIFIED:

    \begin{itemize}
        \item Redundant conditions (e.g., temp > 100 AND temp > 90 $\rightarrow$ temp > 100)
        \item Overly complex boolean logic that can be simplified
        \item Constant expressions that can be pre-computed
    \end{itemize}
\end{enumerate}\vspace{6pt}

IMPORTANT:

\begin{itemize}
    \item Preserve ALL required fields in \lstinline{output_rule}: \lstinline{rule_name}, \lstinline{rule_description}, \lstinline{rule_reasoning}, \lstinline{rule_source}, \lstinline{rule_body}
    \item Keep sensor IDs and time expressions exactly as they are in \lstinline{rule_body}
    \item Only suggest changes you're confident about ($\geq 0.5$)
    \item If merging rules from different sources, combine their \lstinline{rule_source} values
\end{itemize}
\end{UserMessage}

\end{minipage}

\caption{Rule consolidation prompt}
\label{thread:9-consolidation}
\end{figure}




\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{column\_differential\_pressure\_high}}

\textsf{Description:} Alert when differential pressure across column exceeds normal range\\[-4pt]

\textsf{Reasoning:} High differential pressure indicates potential flooding, fouling, or excessive vapor flow\\

\begin{lstlisting}[language=Python]
def column_differential_pressure_high(status) -> str:
    diff_pressure = status.get("14PI1036", "0")
    if diff_pressure and diff_pressure > 0.3:
        return "column_differential_pressure_high"
    return None
\end{lstlisting}

\end{RuleMessage}

\begin{RuleMessage}
\subsection*{\texttt{pressure\_drop\_tray\_monitoring}}

\textsf{Description:} Monitor pressure drop across column trays for fouling detection\\[-4pt]

\textsf{Reasoning:} Increasing pressure drop indicates potential fouling or maldistribution in column trays requiring maintenance\\

\begin{lstlisting}[language=Python]
def pressure_drop_tray_monitoring(status) -> str:
    pressure_drop = status.get("14PI1036", "0")
    if pressure_drop and pressure_drop > 1.5:
        return "pressure_drop_tray_monitoring"
    return None
\end{lstlisting}

\end{RuleMessage}

\end{minipage}

\caption{Extracted rules that will be consolidated}
\label{generatedrule:consolidated-before}
\end{figure}


\begin{figure}[hbtp]
\ContinuedFloat
\captionsetup{list=off,format=cont}

\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{column\_pressure\_drop\_monitor}}

\textsf{Description:} Monitor pressure drop across column trays for maldistribution or fouling\\[-4pt]

\textsf{Reasoning:} Pressure drop monitoring helps detect tray fouling, liquid maldistribution, or other internal issues\\

\begin{lstlisting}[language=Python]
def column_pressure_drop_monitor(status) -> str:
    # Monitor column pressure drop
    column_pressure_drop = status.get("14PI1036", "0")
    
    if column_pressure_drop:
        # Check for excessive pressure drop indicating fouling
        if column_pressure_drop > 1.5:
            return "column_pressure_drop_monitor"
        
        # Check for trending increase in pressure drop
        avg_pressure_drop = status.get("14PI1036", "7d:", "mean")
        if avg_pressure_drop and column_pressure_drop > avg_pressure_drop * 1.2:
            return "column_pressure_drop_monitor"
    
    return None
\end{lstlisting}

\end{RuleMessage}

\begin{RuleMessage}
\subsection*{\texttt{pressure\_drop\_tray\_monitoring}}

\textsf{Description:} Monitor pressure drop across column trays for fouling detection\\[-4pt]

\textsf{Reasoning:} Increasing pressure drop indicates potential fouling or maldistribution in column trays requiring maintenance\\

\begin{lstlisting}[language=Python]
def pressure_drop_tray_monitoring(status) -> str:
    pressure_drop = status.get("14PI1036", "0")
    if pressure_drop and pressure_drop > 1.5:
        return "pressure_drop_tray_monitoring"
    return None
\end{lstlisting}

\end{RuleMessage}

\end{minipage}

\caption{Extracted rules that will be consolidated}
\label{generatedrule:consolidated-before}
\end{figure}


\begin{figure}[hbtp]
\begin{minipage}{\textwidth}

\begin{RuleMessage}
\subsection*{\texttt{column\_pressure\_drop\_monitoring}}

\textsf{\textbf{Consolidated rule}} \\[-4pt]
\begin{adjustwidth}{1.5em}{0pt}

\textsf{Consolidated from:} 4 rules
\begin{adjustwidth}{1.5em}{0pt}
\lstinline{column_differential_pressure_high}\\
\lstinline{pressure_drop_tray_monitoring}\\
\lstinline{column_tray_pressure_drop_monitor}\\
\lstinline{column_pressure_drop_monitor} \\[-4pt]
\end{adjustwidth}

\textsf{Consolidation reason:} All four rules monitor the same sensor (\texttt{14PI1036}) with overlapping thresholds and purposes. Merging creates a hierarchical alert system with clear escalation from operational to maintenance alerts.\\
\end{adjustwidth}

\textsf{Description:} Comprehensive monitoring of column differential pressure for operational and maintenance alerts\\[-4pt]

\textsf{Reasoning:} Differential pressure monitoring detects flooding, fouling, maldistribution, and excessive vapor flow across column trays with hierarchical alert levels\\

\begin{lstlisting}[language=Python]
def column_pressure_drop_monitoring(status) -> str:
    current_pressure_drop = status.get("14PI1036", "0")
    
    if current_pressure_drop:
        # Critical maintenance alert - severe fouling or flooding
        if current_pressure_drop > 1.5:
            return "column_pressure_drop_maintenance_critical"
        
        # Operational alert - potential flooding or excessive vapor flow
        elif current_pressure_drop > 0.3:
            return "column_differential_pressure_high"
        
        # Early warning - tray maldistribution or fouling onset
        elif current_pressure_drop > 0.1:
            # Check for trending increase
            avg_pressure_drop = status.get("14PI1036", "7d:", "mean")
            if avg_pressure_drop and current_pressure_drop > avg_pressure_drop * 1.2:
                return "column_pressure_drop_trending_high"
    
    return None
\end{lstlisting}

\end{RuleMessage}

\end{minipage}

\caption{Consolidated rule: \texttt{column\_pressure\_drop\_monitoring}}
\label{generatedrule:consolidated}
\end{figure}



\newpage

\section{System Orchestration and Observability}\label{llm-orchestration-and-workflows}

The complete solution presented in this chapter is not merely a linear sequence of scripts, but a sophisticated agentic system. To manage the complexity of state transitions, context retention, and multi-step reasoning, we architected the solution using \textbf{LangGraph} for orchestration and \textbf{LangFuse} for observability.

\subsection{Orchestration via LangGraph}

Standard LLM pipelines (chains) are often directed acyclic graphs (DAGs), which limits their ability to handle loops and conditional retries. Our solution, particularly the refinement and validation loops, requires a cyclic graph architecture.

We implemented the system using \textbf{LangGraph}, a library that models the agent workflows as valid \textit{Finite State Machines} (FSM).
\begin{itemize}
    \item \textbf{State:} A shared, typed dictionary (using Pydantic) that persists across graph nodes. This state holds the current list of rules, the document chunk being processed, and any validation errors caught during execution.
    \item \textbf{Nodes:} Each major processing step (Extraction, Sensor Resolution, Time Resolution, Verification) is an independent node that receives the state, modifies it, and passes it forward.
    \item \textbf{Edges:} Conditional logic determines the next step. For example, if the Verification Agent (\Cref{sec:implementation-8-verification}) returns a syntax error, the graph follows a ``feedback edge'' back to the Generation Agent with the error message as new context, effectively implementing a ``self-correction'' loop.
\end{itemize}

\subsection{Observability via LangFuse}

Developing non-deterministic systems requires rigorous tracing capabilities. Unlike standard software where a breakpoint suffices, LLM agents require visibility into the prompt variations, temperature settings, and intermediate ``thoughts.''

We integrated \textbf{LangFuse} to provide full observability into the runtime execution:
\begin{itemize}
    \item \textbf{Tracing:} Every node execution in LangGraph is recorded as a ``trace''. These traces are grouped in ``sessions'', a single session spans a full workflow run. This allows us to visualize the entire lifecycle of a workflow execution.
    \item \textbf{Cost \& Latency Tracking:} We monitor the token consumption and execution time of each step, allowing us to identify bottlenecks (e.g., discovering that the ``consolidation'' step consumed 40\% of the total latency).
    \item \textbf{Dataset Management:} Input/Output pairs from successful runs are saved as a dataset within LangFuse.
\end{itemize}

\subsection{Data Traceability and Persistence}

A core requirement for industrial auditing is lineage: knowing exactly \textit{why} a rule exists. To achieve this, the graph relies on a relational persistence layer (PostgreSQL) co-located with the vector store.

The schema is designed to enforce referential integrity:
\begin{enumerate}
    \item \textbf{\texttt{Documents} Table:} meta-data about the original source file.
    \item \textbf{\texttt{Chunks} Table:} stores the extracted text segments and their Qdrant vector IDs.
    \item \textbf{\texttt{Rules} Table:} stores the final Python code and metadata. Crucially, this table has a foreign key to the \texttt{Chunks} table.
\end{enumerate}

This design guarantees \textbf{complete lineage}: for any active rule in the runtime engine, an operator can query the specific paragraph of text (and the source document page) that inspired its creation, providing the ``reasoning source'' demanded by safety standards.

Additionally, we incorporate tables to audit more processes of our workflow:

\begin{enumerate}  \setcounter{enumi}{3}

    \item \textbf{\texttt{RuleContextChunks} Table:} full traceability of the chunks retrieved for each of the chunks processed in the RAG step (\Cref{sec:implementation-3-rag}).
    \item \textbf{\texttt{RuleGroundingSearches} Table:} every search performed in each of the chunks processed is stored in this table (\Cref{sec:implementation-7-grounding}).
\end{enumerate}

We also must not forget that the consolidation process is by itself explainable, as the base rules used in a consolidated rule are also stored in the database (see \Cref{sec:implementation-9-consolidation}).

