% =============================================================================
% CHAPTER 5: SOLUTION DESIGN AND IMPLEMENTATION
% =============================================================================

\chapter{Solution Design and Implementation}\label{solution-design-and-implementation}

\section{5.0. Baseline: Naive Single-Document
Extraction}\label{baseline-naive-single-document-extraction}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item Modern Large Language Models (LLMs) with extended context windows (e.g., 128k+ tokens) possess sufficient capacity to ingest complete technical documents (e.g., 50+ page manuals) in a single pass.
\item Therefore, a zero-shot prompting strategy without external retrieval mechanisms should be capable of extracting and synthesizing operational monitoring rules directly from the raw text.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Option A: Zero-Shot Full-Context Extraction.} Loading the entire document string into the context window and requesting extraction in a single prompt.
\item \textbf{Option B: Summarization-then-Extraction.} Asking the model to summarize the document first, then extract rules from the summary. (Discarded: Summarization loses the specific numeric thresholds required for industrial rules).
\item \textbf{Option C: Map-Reduce.} Splitting the document, extracting from parts, and summarizing results. (Discarded for this baseline: This effectively moves into ``Chunking,'' which is reserved for Section 5.1).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}
\item \textbf{Model:} State-of-the-art Foundation Model (e.g., GPT-4o or Claude 3.5 Sonnet) with \textgreater100k token context window.
\item \textbf{Input Data:} Raw text extracted from a PDF of a C3/C4 Splitter functional description.
\item \textbf{Prompt Strategy:} Zero-shot.
  \begin{itemize}
  \item \emph{System Prompt:} ``You are an expert industrial process engineer.''
  \item \emph{User Prompt:} ``Read the following document and extract all operational monitoring rules, alarms, and interlocks.''
  \end{itemize}
\item \textbf{Output Format:} Unconstrained natural language (Plain text).
\end{itemize}

\textbf{Results:}

\begin{itemize}
\item \textbf{Recall Degradation (``Lost in the Middle''):} While the model accepted the input, it failed to extract rules located in the middle sections of the document, prioritizing information at the very beginning (Introduction) and very end (Appendices).
\item \textbf{Context Overflow:} For larger manuals (\textgreater100 pages), the token count exceeded the model's efficient processing limit, leading to truncation or refusal to process.
\item \textbf{Hallucination:} The model occasionally generated generic best-practice rules for distillation columns that were not present in the specific source text.
\item \textbf{Parsing Failure:} The output format was inconsistent (e.g., switching between bullet points, numbered lists, and paragraphs), rendering programmatic parsing impossible.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Address Recall Degradation:} Implement a \textbf{Chunking Strategy} to break the document into manageable segments, ensuring the LLM focuses on specific details without ``attention dilution'' -\textgreater{} \emph{Section 5.1}.
\item \textbf{Address Parsing Failure:} Enforce \textbf{Structured Output (JSON)} using schema validation to ensure every extracted rule follows a strict format -\textgreater{} \emph{Section 5.1}.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Liu, N. F., et al.~(2023).} \emph{Lost in the Middle: How Language Models Use Long Contexts.} arXiv preprint arXiv:2307.03172. (Demonstrates the U-shaped performance curve where LLMs forget information in the middle of the context).
\item \textbf{Xu, P., et al.~(2024).} \emph{Retrieval meets Long Context Large Language Models.} ICLR 2024. (Provides evidence that RAG-based approaches often outperform long-context approaches for retrieval tasks, supporting the move away from single-pass extraction).
\end{itemize}

\section{5.1. Structured Extraction via Naive
Chunking}\label{structured-extraction-via-naive-chunking}

\emph{Highlight the trade-off between \textbf{precision} (gained via
chunking) and \textbf{coherence} (lost due to fragmentation), which is
the perfect setup for your RAG section.}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item \textbf{Attention Optimization:} By decomposing the document into smaller segments (``chunks''), the LLM's attention mechanism can focus on local details without the degradation observed in long-context windows.
\item \textbf{Schema Constraint:} Forcing the LLM to output according to a strict schema (e.g., JSON) will reduce generation variability and enable programmatic parsing of the extracted rules.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Option A: Fixed-Size Chunking.} Splitting text by token count (e.g., 1000 tokens) with a sliding window overlap. (Selected for this baseline).
\item \textbf{Option B: Semantic Chunking.} Splitting text based on semantic breaks (paragraphs, headers). (Discarded for the baseline to test raw throughput first).
\item \textbf{Option C: Regex/Template Matching.} Using traditional NLP patterns. (Discarded as it cannot handle the linguistic variance of industrial descriptions).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}
\item \textbf{Chunking Strategy:} Recursive Character Text Splitter.
  \begin{itemize}
  \item \emph{Chunk Size:} 2000 tokens (balanced to contain full paragraphs).
  \item \emph{Overlap:} 200 tokens (to mitigate splitting sentences at boundaries).
  \end{itemize}
\item \textbf{Output Schema:} A predefined JSON schema enforced via the LLM's function-calling or JSON-mode API.
  \begin{itemize}
  \item \emph{Fields:} \texttt{Rule\ Name}, \texttt{Condition} (text), \texttt{Action} (text), \texttt{Sensor\ Tag}.
  \end{itemize}
\item \textbf{Workflow:}
  \begin{enumerate}
  \item Load Document.
  \item Split into \(N\) chunks.
  \item Parallel execution of \(N\) LLM calls with the prompt: ``Extract rules from this specific text segment matching the JSON schema.''
  \item Aggregation of results into a single list.
  \end{enumerate}
\end{itemize}

\textbf{Results:}

\begin{itemize}
\item \textbf{Improved Extraction Rate:} The system successfully processed the entire document without context overflow errors. The JSON structure made the output immediately machine-readable.
\item \textbf{Context Fragmentation (The ``Keyhole'' Problem):} The system failed to capture rules that spanned across chunks. For example, if Chunk A defined ``The limits for column T-100'' and Chunk B listed ``High: 80°C'', the LLM processing Chunk B lacked the context to know ``High'' referred to ``T-100''. -\textgreater{} \emph{Motivates Contextual RAG (Section 5.2).}
\item \textbf{Logic Representation Failure:} The JSON schema proved too rigid to capture complex industrial logic (e.g., ``IF A \textgreater{} 5 AND (B \textless{} 10 OR C is OPEN)''). Attempting to stuff this logic into a text field inside JSON re-introduced ambiguity. -\textgreater{} \emph{Motivates Python Code Generation (Section 5.5).}
\item \textbf{Sensor Hallucination/Ambiguity:} The model often extracted generic terms (e.g., ``the reflux temperature'') rather than the specific instrument tag (e.g., \texttt{TIC-1023}), as the mapping was often defined in a different part of the document or an external P\&ID. -\textgreater{} \emph{Motivates Retrieval \& Resolution (Section 5.2/5.3).}
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Bridge Context Gaps:} Implement \textbf{Retrieval Augmented Generation (RAG)} to allow the LLM to ``look up'' definitions or context from other parts of the document while processing a specific chunk -\textgreater{} \emph{Section 5.2}.
\item \textbf{Enhance Logic Expressiveness:} Transition from static JSON data structures to dynamic \textbf{Python Code} to represent complex boolean logic and thresholds -\textgreater{} \emph{Section 5.5}.
\item \textbf{Sensor Resolution:} Implement a specific retrieval step to map natural language sensor descriptions to their exact tags in the instrument database -\textgreater{} \emph{Section 5.3}.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Vijayan, R., et al.~(2024).} \emph{A Prompt Engineering Approach for Structured Data Extraction from Unstructured Text.} ACM International Conference on Proceeding Series. doi:10.1145/3639631.3639663. (Validates the use of schema enforcement to reduce hallucination in extraction tasks).
\item \textbf{Dagdelen, J., et al.~(2024).} \emph{Structured information extraction from scientific text with large language models.} Nature Communications, 15, 1418. doi:10.1038/s41467-024-45563-x. (Demonstrates that while LLMs excel at local extraction, they require structured workflows to handle complex, domain-specific scientific data).
\end{itemize}

\section{5.2. Contextual Augmentation via Retrieval Augmented Generation
(RAG)}\label{contextual-augmentation-via-retrieval-augmented-generation-rag}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item \textbf{Semantic Bridging:} Industrial specifications often decouple definitions (e.g., ``Reflux Loop'') from their operational parameters (e.g., ``Max Flow: 50 t/h''). A linear reading strategy fails to link them. Semantic search via dense vector embeddings can bridge this gap by retrieving relevant definitions based on meaning rather than keyword matching.
\item \textbf{Layout-Awareness:} Technical rules are frequently encoded in semi-structured formats like tables. Standard text extraction flattens this structure. Layout-aware parsing is required to preserve the ``row-column'' context.
\item \textbf{Data Sovereignty:} For industrial deployment, sensitive process knowledge must be processed locally to avoid data leakage risks associated with public API endpoints.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Parsing Strategy:}
  \begin{itemize}
  \item \emph{Option A: PyPDF/LangChain Loaders.} Extracts raw text but destroys table structures. (Discarded: ``Jumbled'' outputs).
  \item \emph{Option B: Docling (IBM).} A layout-aware parser that identifies tables, headers, and reading order. (Selected: Essential for technical spec sheets).
  \end{itemize}
\item \textbf{Embedding Strategy:}
  \begin{itemize}
  \item \emph{Option A: Cloud-based (e.g., OpenAI \texttt{text-embedding-3}).} High performance but requires sending data to external servers. (Discarded due to industrial privacy requirements).
  \item \emph{Option B: Local Open-Source (e.g., \texttt{mxbai-embed-large} via Ollama).} Runs entirely on-premise, high ranking on MTEB (Massive Text Embedding Benchmark). (Selected: Balances state-of-the-art performance with data privacy).
  \end{itemize}
\item \textbf{Vector Storage:}
  \begin{itemize}
  \item \emph{Option A: In-Memory (FAISS).} Ephemeral. (Discarded: Cannot scale or persist).
  \item \emph{Option B: Dedicated Vector Database (Qdrant).} (Selected: Chosen for HNSW indexing speed, persistence, and metadata filtering capabilities).
  \end{itemize}
\end{itemize}

\textbf{Implementation:}

\begin{enumerate}
\item \textbf{Ingestion Pipeline (Docling):} The PDF is processed with Docling, serializing the document into Markdown while preserving table structures and hierarchy.
\item \textbf{Embedding Generation (\texttt{mxbai-embed-large}):}
  \begin{itemize}
  \item Text chunks are converted into vector representations using \texttt{mxbai-embed-large} served via Ollama.
  \item \emph{Justification:} This model is specifically optimized for retrieval tasks (unlike generic BERT models) and supports Matryoshka Representation Learning, offering high-quality representations with a smaller memory footprint.
  \end{itemize}
\item \textbf{Storage (Qdrant):} Vectors are stored in Qdrant collections. We utilize Qdrant's \textbf{HNSW (Hierarchical Navigable Small World)} index for approximate nearest neighbor search.
\item \textbf{Retrieval Workflow:}
  \begin{itemize}
  \item The system embeds the query using the same local model.
  \item Cosine similarity is calculated: \(S_C(u, v) = \frac{u \cdot v}{\|u\|\|v\|}\).
  \item Top-\(k\) chunks are retrieved and injected into the LLM context.
  \end{itemize}
\end{enumerate}

\textbf{Results:}

\begin{itemize}
\item \textbf{Contextual Resolution:} The system successfully resolved generic terms (e.g., ``the vessel'') by retrieving the relevant section headers.
\item \textbf{Privacy Compliance:} By using Ollama and Qdrant locally, the entire extraction pipeline operates air-gapped, meeting strict industrial security requirements.
\item \textbf{Table Accuracy:} Docling + Semantic Search allowed for successful extraction of rules from complex alarm tables (accuracy improved from \textless40\% to \textgreater85\%).
\item \textbf{Retrieval Noise:} Occasionally, the embeddings retrieved semantically related but contextually irrelevant chunks (e.g., retrieving ``Fire Safety'' rules for ``Fired Heater'' queries).
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Refine Retrieval Precision:} The ``Retrieval Noise'' suggests we need a mechanism to specifically resolve \textbf{instrument tags} (e.g., \texttt{TIC-101}) differently from general text -\textgreater{} \emph{Section 5.3 (Sensor Resolution).}
\item \textbf{External Validation:} The system retrieves internal document data but lacks external validation (physics, regulations) -\textgreater{} \emph{Section 5.4 (Grounding).}
\item \textbf{Logic Formatting:} Output is still static JSON; need executable logic -\textgreater{} \emph{Section 5.5.}
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Muennighoff, N., et al.~(2023).} \emph{MTEB: Massive Text Embedding Benchmark.} EACL 2023. (Establishes the standard for evaluating embedding models; validates the choice of high-performing open models like mxbai).
\item \textbf{Malkov, Y. A., \& Yashunin, D. A. (2018).} \emph{Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.} IEEE TPAMI. (The algorithmic basis for Qdrant).
\item \textbf{Lewis, P., et al.~(2020).} \emph{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.} NeurIPS 2020.
\end{itemize}

\section{5.3. Extraction of Executable Logic (Python
Functions)}\label{extraction-of-executable-logic-python-functions}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\tightlist
\item
  \textbf{Expressiveness Gap:} Static data structures (like JSON or XML)
  fail to capture the logical complexity of industrial operations, which
  often involve arithmetic calculations (e.g., \(\Delta P\)),
  conditional branching, and temporal aggregation (e.g., ``average over
  10 minutes'').
\item
  \textbf{Code as Policy:} Large Language Models exhibit stronger
  reasoning capabilities when generating code (e.g., Python) compared to
  natural language or JSON, as code enforces logical consistency and
  topology.
\item
  \textbf{Symbolic Disentanglement:} Trying to map specific Sensor IDs
  (e.g., \texttt{TIC-101}) \emph{simultaneously} while extracting logic
  leads to cognitive overload and hallucinations. It is more effective
  to extract the \emph{logic} first using natural language placeholders,
  and resolve the \emph{symbols} (sensors/time) in subsequent passes.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\tightlist
\item
  \textbf{Option A: Domain Specific Language (DSL).} Creating a custom
  grammar for rules. (Discarded: Requires training the LLM on the new
  syntax; high maintenance).
\item
  \textbf{Option B: Structured JSON Logic.} Storing logic as nested JSON
  trees. (Discarded: Hard to debug, limited arithmetic capabilities).
\item
  \textbf{Option C: General Purpose Language (Python).} Generating
  standard Python functions with a specific API signature. (Selected:
  Leverage's LLM's high proficiency in Python, allows for complex math,
  and uses \texttt{ast} module for validation).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}
\tightlist
\item
  \textbf{The \texttt{status} Abstraction:} We define a standardized
  interface \texttt{status.get(sensor\_name,\ time\_window,\ statistic)}
  that abstracts the data retrieval.
\item
  \textbf{Prompt Engineering:} The LLM is instructed to extract rules as
  Python functions. Crucially, it is allowed to use \textbf{Natural
  Language Keys} for sensors and time (e.g.,
  \texttt{status.get("column\ overhead\ pressure",\ "last\ 10\ minutes")}).
\item
  \textbf{Structured Fallback:} The system attempts to use Native
  Structured Output (function calling) to enforce the schema. If the
  provider fails, it falls back to a raw JSON generation prompt with
  manual parsing.
\item
  \textbf{Workflow Integration:} This step accepts the \texttt{chunk}
  and \texttt{grounding\_info} and outputs a list of \texttt{Rule}
  objects containing raw Python code strings.
\end{itemize}

\textbf{Code Strategy (Abstracted):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The LLM generates code with semantic placeholders}
\KeywordTok{def}\NormalTok{ column\_high\_pressure\_alert(status) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
    \CommentTok{\# Logic is extracted, but entities are still Natural Language}
\NormalTok{    current }\OperatorTok{=}\NormalTok{ status.get(}\StringTok{"column pressure"}\NormalTok{, }\StringTok{"current"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ current }\KeywordTok{and}\NormalTok{ current }\OperatorTok{\textgreater{}} \FloatTok{15.5}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"column\_high\_pressure\_alert"}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}
\tightlist
\item
  \textbf{Logic Capture:} The system successfully extracted complex
  rules involving arithmetic (e.g.,
  \texttt{abs(current\ -\ average)\ \textgreater{}\ 0.15}) which were
  impossible in the JSON-only implementation.
\item
  \textbf{Hallucination Reduction:} By allowing the LLM to use natural
  language for sensors (``column pressure'') instead of forcing it to
  guess a Tag ID (\texttt{PI-???}) during this stage, the generation of
  invalid rules dropped significantly.
\item
  \textbf{Validation Readiness:} The output is valid Python syntax,
  allowing us to immediately use Python's \texttt{ast} (Abstract Syntax
  Tree) module to inspect the code structure before execution.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\tightlist
\item
  \textbf{Executable Gaps:} The generated code is syntactically correct
  but not yet executable because ``column pressure'' is not a valid
  database key -\textgreater{} \emph{Section 5.4 (Sensor Resolution).}
\item
  \textbf{Temporal Ambiguity:} Terms like ``last 10 minutes'' are
  strings, not calculated time deltas -\textgreater{} \emph{Section 5.5
  (Time Parsing).}
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\tightlist
\item
  \textbf{Liang, J., et al.~(2023).} \emph{Code as Policies: Language
  Model Programs for Embodied Control.} ICRA 2023. (Demonstrates that
  expressing policies as code improves robot reasoning and behavior).
\item
  \textbf{Wang, Y., et al.~(2023).} \emph{Codex: Evaluating Large
  Language Models Trained on Code.} arXiv. (Validates that code-trained
  models have superior logical reasoning capabilities).
\end{itemize}

\section{5.4. Context-Aware Sensor Resolution (Entity
Linking)}\label{context-aware-sensor-resolution-entity-linking}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item \textbf{Decoupling Logic from Entities:} Attempting to extract complex logic \emph{and} map specific database IDs in a single step overloads the LLM's reasoning capabilities, leading to hallucinations. Decoupling these tasks---extracting logic first (Section 5.3), then resolving entities---improves accuracy.
\item \textbf{Contextual Disambiguation:} Standard semantic similarity (Vector RAG) is insufficient for sensor mapping. For example, ``Tower Temperature'' is semantically similar to both ``Top Temperature'' and ``Bottom Temperature.'' Correct resolution requires the \textbf{logical context} of the rule (e.g., ``if pressure rises, check \emph{temperature}'') to infer the correct sensor.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Option A: Fuzzy String Matching.} Using Levenshtein distance to match terms. (Discarded: Fails on synonyms like ``Reflux'' vs.~``Overhead Return'').
\item \textbf{Option B: Zero-Shot Vector Retrieval.} Embedding the natural language term and finding the closest sensor embedding. (Discarded: Lacks the logical reasoning to distinguish between redundant sensors or physically close but operationally distinct tags).
\item \textbf{Option C: LLM-Based Contextual Resolution.} Passing the natural language term \emph{along with the surrounding code logic} to an LLM to select the correct ID from a candidate list. (Selected: Allows the model to reason about units, location, and physics).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}
\item \textbf{AST/Regex Extraction:} The system scans the Python code generated in Section 5.3 for \texttt{status.get("natural\_name",\ ...)} calls using regular expressions (or AST traversal).
\item \textbf{Candidate Injection:}
  \begin{itemize}
  \item For the C3/C4 Splitter pilot, the entire sensor list (ID, Name, Description, Unit) is injected into the prompt context.
  \item \emph{Note:} This approach prioritizes precision over scalability for the pilot phase.
  \end{itemize}
\item \textbf{Resolution Prompting:} A specialized LLM call acts as a ``Sensor Mapping Expert.''
  \begin{itemize}
  \item \emph{Input:} The generated Python function (for context) and the list of target sensor references.
  \item \emph{Logic:} The LLM analyzes the rule logic (e.g., ``rule checks for high pressure'') and matches it to the sensor list (e.g., finding \texttt{PIC-101} which measures Pressure).
  \end{itemize}
\item \textbf{Code Rewriting:} The system performs a deterministic string replacement in the Python code, transforming \texttt{status.get("column\ pressure")} into \texttt{status.get("PIC-101")}.
\item \textbf{Validation State:} Rules are tagged with a \texttt{SensorParsingStatus}. If a sensor cannot be resolved with high confidence (returning \texttt{null}), the rule is marked \texttt{SENSORS\_NOT\_FOUND} and excluded from deployment.
\end{itemize}

\textbf{Results:}

\begin{itemize}
\item \textbf{Contextual Success:} The system successfully resolved ambiguous terms that baffled vector search. For example, in a rule about ``reflux drum level,'' the LLM correctly identified \texttt{LIC-204} (Level Indicator Control) rather than \texttt{LI-205} (a local glass gauge) because the rule implied \emph{control} logic.
\item \textbf{Unit Consistency:} The LLM correctly used unit mismatches to filter candidates (e.g., rejecting a Flow sensor \texttt{t/h} when the rule logic required a Pressure \texttt{bar}).
\item \textbf{Code Integrity:} By performing resolution \emph{after} logic extraction, the logical structure of the Python code remained intact even when sensor names changed.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Scalability (RAG Pre-filtering):} In a full-scale refinery (50,000+ sensors), injecting the full list is impossible. Future work involves a \textbf{Hybrid Approach}: use Vector Search to retrieve the top-50 most relevant sensors, then use the LLM to pick the exact match from that subset.
\item \textbf{Multi-Tag Resolution:} Handling cases where ``Temperature'' refers to an \emph{average} of three redundant sensors (\texttt{TI-101A/B/C}). The current system maps to a single ID.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Wu, L., et al.~(2023).} \emph{Zero-Shot Entity Linking with Dense Retrieval and Large Language Models.} arXiv preprint. (Validates the methodology of using LLMs to link text mentions to knowledge base entities).
\item \textbf{Orr, L., et al.~(2021).} \emph{Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation.} VLDB. (Discusses the difficulty of disambiguating entities in specialized technical domains).
\end{itemize}

\section{5.5. Semantic Time Parsing \&
Normalization}\label{semantic-time-parsing-normalization}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item \textbf{Temporal Ambiguity in Specs:} Industrial documents describe time in unstructured natural language (e.g., ``average over the last 10 minutes,'' ``current value,'' ``trend from an hour ago''). These cannot be executed by a machine which requires precise lookback windows.
\item \textbf{The Interval vs.~Point Distinction:} A critical operational distinction exists between a \textbf{Point Query} (fetching a single value at \(t\)) and an \textbf{Interval Query} (fetching a range \([t_{start}, t_{end}]\) for aggregation). Natural language blurs this line; a formal grammar is required to enforce it.
\item \textbf{Grammar-Constrained Parsing:} LLMs struggle with strict string formatting (e.g., regex compliance) in zero-shot settings. A dedicated normalization step that maps natural language to a strict, verifiable time grammar reduces runtime errors.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Option A: Python \texttt{dateutil} / \texttt{datetime}.} Standard libraries. (Discarded: They handle \emph{absolute} datetimes, but rules function on \emph{relative} time deltas like ``last 10 mins'').
\item \textbf{Option B: \texttt{pandas} Offset Aliases.} (Discarded: Too complex and dataframe-centric for simple rule definitions).
\item \textbf{Option C: Custom \texttt{TimeDelta} Grammar.} A bespoke, succinct string format (e.g., \texttt{10m:}, \texttt{5m:2m}) backed by a rigorous parser. (Selected: Provides a direct mapping to the historian's query language).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}
\item \textbf{Formal Time Grammar:} We defined a strict schema for relative time:
  \begin{itemize}
  \item \textbf{Time Point:} A single offset from \emph{now} (e.g., \texttt{0} = current, \texttt{5m} = 5 mins ago).
  \item \textbf{Time Interval:} A range defined by start/end offsets (e.g., \texttt{10m:} = from 10m ago to now; \texttt{2h:1h} = from 2h ago to 1h ago).
  \item \textbf{Statistic Requirement:} Intervals \emph{must} be accompanied by a statistical function (mean, max, std); Points \emph{must not}.
  \end{itemize}
\item \textbf{Two-Step Extraction:}
  \begin{enumerate}
  \item \textbf{Identification:} The system identifies \texttt{status.get(sensor,\ "natural\_expr")} calls in the Python code from Section 5.3.
  \item \textbf{Normalization:} A dedicated LLM call translates the natural expression into a JSON object \texttt{\{"time":\ "str",\ "statistic":\ "enum"\}}.
  \end{enumerate}
\item \textbf{Validation Logic:}
  \begin{itemize}
  \item The \texttt{TimeDeltaInterval} class parses the string.
  \item If it is an \textbf{Interval} (\texttt{:} exists), the system asserts a statistic is present.
  \item If it is a \textbf{Point}, the system asserts the statistic is \texttt{None}.
  \end{itemize}
\item \textbf{Code Rewriting:} The natural language argument is replaced with the strict format: \texttt{status.get("TIC-101",\ "10m:",\ "mean")}.
\end{itemize}

\textbf{Results:}

\begin{itemize}
\item \textbf{Standardization:} The system successfully normalized diverse expressions:
  \begin{itemize}
  \item ``average over the last 10 minutes'' \(\to\) \texttt{"10m:",\ "mean"}
  \item ``current temperature'' \(\to\) \texttt{"0",\ None}
  \item ``standard deviation over the last hour'' \(\to\) \texttt{"1h:",\ "std"}
  \end{itemize}
\item \textbf{Validation Trap:} The schema successfully caught logical errors, such as ``average temperature at 5 minutes ago'' (requesting an average at a single point), flagging them for review.
\item \textbf{Historian Compatibility:} The resulting format \texttt{10m:} maps directly to time-series database queries (e.g., \texttt{SELECT\ AVG(val)\ FROM\ tag\ WHERE\ time\ \textgreater{}\ now()\ -\ 10m}), streamlining the execution engine.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Complex Windows:} The current grammar handles fixed sliding windows. It does not support event-based windows (e.g., ``since the last startup'' or ``while pump A was running'').
\item \textbf{Frequency Inference:} The system assumes a default sampling rate. Explicitly handling ``resample every 1m'' within the rule definition would add precision.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Zhang, Y., et al.~(2015).} \emph{Temporal Tagging on Clinical Narratives with Neural Networks.} (Discusses the challenge of normalizing relative time expressions in technical domains).
\item \textbf{Vlachos, A., et al.~(2018).} \emph{Guided Generation of Code with Logic Constraints.} (Supports the approach of using constraints/grammars to guide LLM output).
\end{itemize}

\section{5.6. External Knowledge Grounding \&
Verification}\label{external-knowledge-grounding-verification}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\tightlist
\item
  \textbf{Knowledge Cutoff:} LLMs trained on static datasets lack
  real-time knowledge of volatile industrial regulations (e.g., EPA
  updates) or market-specific environmental conditions.
\item
  \textbf{Hallucination Check:} Rules generated from internal documents
  (Section 5.3) may be internally consistent but factually wrong against
  physical laws or external standards. Grounding them against a trusted
  external corpus serves as a ``Factuality Layer.''
\item
  \textbf{Search-for-RAG vs.~Search-for-Humans:} Traditional search APIs
  (Google/Bing) return \emph{blue links} and \emph{snippets} optimized
  for human clicks. AI Agents require \textbf{Context-Rich Answers}
  (parsed page text, markdown) to minimize post-processing latency and
  token usage.
\end{itemize}

\textbf{Implementation Options (Search API Evaluation):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Provider
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Free Tier
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost (per 1k)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verdict
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tavily} & \textbf{AI-Native} & \textbf{1,000 / mo} &
\textbf{\$5-8} & \textbf{Selected.} Returns parsed \emph{content}, not
just links. Optimizes token usage by pre-filtering irrelevant
headers/footers. \\
\textbf{Serper (Google)} & Wrapper & 2,500 (One-time) & \$0.30-0.50 &
\textbf{Alternative.} Best for ``Deep Long Tail'' retrieval. Excellent
if we needed obscure PDF manuals, but requires building a separate
scraper for page content. \\
\textbf{Brave Search} & Index & 2,000 / mo & \$5.00 & \textbf{Strong
Contender.} High privacy (important for industry). Good independent
index. \\
\textbf{DuckDuckGo} & Scraper & Unlimited & Free & \textbf{Discarded.}
Unreliable (no SLA), aggressive rate limiting, and strictly returns
snippets, requiring a secondary scraping step. \\
\textbf{Google Custom} & Official & 100 / day & \$5.00 &
\textbf{Discarded.} High setup complexity (Programmable Search Engine),
returns noisy JSON. \\
\end{longtable}
}

\textbf{Implementation:}

\begin{itemize}
\tightlist
\item
  \textbf{The Grounding Loop:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Query Synthesis:} The LLM analyzes the extracted rule (e.g.,
    ``Vent gas must be \textless{} 50 ppm'') and generates a
    verification query: \emph{``EPA regulation limit for propane vent
    gas flaring 2024''}.
  \item
    \textbf{Retrieval (Tavily):} The agent calls the search API.
    Crucially, we use the \texttt{include\_raw\_content=False} and
    \texttt{search\_depth="advanced"} parameters to get synthesized
    answers rather than raw HTML.
  \item
    \textbf{Fact-Checking:} A ``Verifier LLM'' compares the internal
    rule against the external evidence.

    \begin{itemize}
    \tightlist
    \item
      \emph{Match:} Rule marked \texttt{VERIFIED\_EXTERNAL}.
    \item
      \emph{Conflict:} Rule marked \texttt{REVIEW\_REQUIRED} with a
      citation to the external source.
    \end{itemize}
  \end{enumerate}
\end{itemize}

\textbf{Code Strategy (Tavily Integration):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We use Tavily because it abstracts the scraping/parsing layer}
\ImportTok{from}\NormalTok{ tavily }\ImportTok{import}\NormalTok{ TavilyClient}

\KeywordTok{def}\NormalTok{ verify\_rule\_externally(rule\_condition: }\BuiltInTok{str}\NormalTok{):}
\NormalTok{    client }\OperatorTok{=}\NormalTok{ TavilyClient(api\_key}\OperatorTok{=}\NormalTok{os.environ[}\StringTok{"TAVILY\_API\_KEY"}\NormalTok{])}
    \CommentTok{\# The \textquotesingle{}search\_depth="advanced"\textquotesingle{} is critical: it aggregates data from multiple pages}
\NormalTok{    response }\OperatorTok{=}\NormalTok{ client.search(}
\NormalTok{        query}\OperatorTok{=}\SpecialStringTok{f"Industrial standard for }\SpecialCharTok{\{}\NormalTok{rule\_condition}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{        search\_depth}\OperatorTok{=}\StringTok{"advanced"}\NormalTok{,}
\NormalTok{        max\_results}\OperatorTok{=}\DecValTok{3}
\NormalTok{    )}
    \ControlFlowTok{return}\NormalTok{ response[}\StringTok{"answer"}\NormalTok{]  }\CommentTok{\# Tavily generates a synthetic answer, saving LLM tokens}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}
\tightlist
\item
  \textbf{Fact Validation:} The system successfully flagged an obsolete
  internal rule regarding ``Freon'' refrigerants by retrieving modern
  environmental bans.
\item
  \textbf{Token Efficiency:} Using Tavily's ``answer'' field reduced the
  context window usage by \textasciitilde60\% compared to scraping top-5
  Google results and feeding full HTML to the LLM.
\item
  \textbf{Latency:} The ``Advanced'' search depth introduced a 2-4
  second latency per rule, which is acceptable for offline extraction
  but suggests batching is needed for scale.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\tightlist
\item
  \textbf{Hybrid Grounding:} Use \textbf{Serper} for finding
  \emph{documents} (PDFs of regulations) and \textbf{Tavily} for
  answering \emph{questions}.
\item
  \textbf{Domain Whitelisting:} Restrict search scope to trusted domains
  (e.g., \texttt{*.gov}, \texttt{*.iso.org}) to prevent grounding
  against low-quality forum answers.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\tightlist
\item
  \textbf{Khattab, O., et al.~(2022).} \emph{Demonstrate-Search-Predict:
  Composing retrieval and language models for knowledge-intensive NLP.}
  (Discusses the DSP pipeline which mirrors the verify-loop).
\item
  \textbf{Nakano, R., et al.~(2021).} \emph{WebGPT: Browser-assisted
  question-answering with human feedback.} (Foundational paper on using
  search to ground LLM generations).
\end{itemize}

\section{5.7. Multi-Level Output
Verification}\label{multi-level-output-verification}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\item \textbf{The ``Hallucination-Syntax'' Gap:} While LLMs can generate Python-like text, they do not guarantee syntactic correctness or runtime safety. A generated rule might look correct but contain subtle syntax errors (e.g., indentation issues) that cause runtime crashes.
\item \textbf{Semantic Integrity:} A rule can be syntactically valid (it runs) but semantically invalid within the domain context (e.g., calculating the ``mean'' of a single time point, or querying a non-existent sensor).
\item \textbf{Compiler-Based Verification:} Treating the extracted rules as ``source code'' and the verification pipeline as a ``compiler'' (Lexical \(\to\) Syntactic \(\to\) Semantic Analysis) ensures that only executable, logical, and safe rules are promoted to the consolidation stage.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\item \textbf{Option A: Execution/Unit Testing.} Running every rule in a sandboxed environment with mock data. (Discarded: High computational overhead; difficult to generate realistic mock data for complex multi-sensor correlations).
\item \textbf{Option B: LLM Self-Correction.} Asking the LLM to ``check its own work.'' (Discarded: Research shows LLMs struggle to correct their own logic errors without external feedback signals).
\item \textbf{Option C: Static Analysis via AST (Abstract Syntax Tree).} Parsing the code structure to validate syntax and extracting function calls to validate arguments against constraints. (Selected: Fast, deterministic, and 100\% accurate for structure checking).
\end{itemize}

\textbf{Implementation:} The verification pipeline implements a
three-layer ``Quality Gate'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Layer 1: Syntactic Verification (\texttt{ast.parse})}:

  \begin{itemize}
  \tightlist
  \item
    The raw Python string is parsed into an Abstract Syntax Tree.
  \item
    \emph{Check:} If \texttt{ast.parse()} raises a \texttt{SyntaxError},
    the rule is immediately flagged as \texttt{SYNTAX\_ERROR}.
  \end{itemize}
\item
  \textbf{Layer 2: Domain Entity Verification (Sensor Auditing)}:

  \begin{itemize}
  \tightlist
  \item
    A custom \texttt{StatusCallExtractor} (inheriting from
    \texttt{ast.NodeVisitor}) traverses the tree to find all
    \texttt{status.get()} calls.
  \item
    \emph{Check:} Extracted Sensor IDs are cross-referenced against the
    validated \texttt{Sensor\ Collection}. Any ID not present in the
    database triggers an \texttt{INVALID\_SENSOR} flag.
  \end{itemize}
\item
  \textbf{Layer 3: Temporal-Statistical Logic Check}:

  \begin{itemize}
  \tightlist
  \item
    The system validates the logical consistency of time parameters
    (derived in Section 5.5).
  \item
    \emph{Check:} \textbf{Points vs.~Intervals.} If the time argument
    implies a point (e.g., ``current''), the system asserts that the
    \emph{statistic} argument is \texttt{None}. If it implies an
    interval (e.g., ``10m:''), it asserts that a valid statistic (e.g.,
    ``mean'') is present. Violations trigger
    \texttt{INVALID\_STATISTIC}.
  \end{itemize}
\end{enumerate}

\textbf{Results:}

\begin{itemize}
\item \textbf{Runtime Safety:} The pipeline achieved a 100\% ``Compilation Success Rate'' for verified rules---no rule marked \texttt{VERIFIED} caused a runtime crash during execution.
\item \textbf{Logic Catching:} The temporal-statistical check successfully caught subtle LLM errors, such as requesting the ``standard deviation of the current value'' (mathematically impossible), which a simple syntax check would have missed.
\item \textbf{Feedback Loop:} Rules failing verification are not just discarded; their error flags (\texttt{INVALID\_SENSOR}) provide specific metadata that can be used for re-prompting or human review.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\item \textbf{Type Checking:} Integrate \texttt{mypy} or similar static type checkers to ensure return types are consistently strings or \texttt{None}.
\item \textbf{Physical Sanity Check:} Implement a ``Physics Engine'' check to validate threshold values (e.g., ensuring a temperature threshold of 5000°C is flagged as impossible for a steel vessel).
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\item \textbf{Austin, J., et al.~(2021).} \emph{Program Synthesis with Large Language Models.} (Discusses the importance of execution-based or static evaluation for code generation).
\item \textbf{Chen, M., et al.~(2021).} \emph{Codex: Evaluating Large Language Models Trained on Code.} (Highlights that pass@k rates improve significantly when output is filtered by a syntax checker).
\item \textbf{Aho, A. V., et al.} \emph{Compilers: Principles, Techniques, and Tools.} (The Dragon Book---standard reference for the AST/Visitor pattern used in your implementation).
\end{itemize}

\section{5.8. Intelligent Rule Consolidation \&
Optimization}\label{intelligent-rule-consolidation-optimization}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\tightlist
\item
  \textbf{The ``Fragmentation'' Problem:} Extracting rules from chunks
  (Section 5.1) inevitably produces \textbf{redundancy} (multiple chunks
  repeating the same safety limit) and \textbf{fragmentation} (one rule
  setting a ``High'' alarm, another setting a ``High-High'' alarm for
  the same sensor).
\item
  \textbf{Semantic Deduplication:} Simple string matching fails to merge
  rules because the wording differs (e.g., ``Temp must be \textless{}
  100'' vs ``Max Temp: 100''). Consolidation requires \textbf{Semantic
  Deduplication}---understanding that two distinct natural language
  strings compile to the same logical condition.
\item
  \textbf{Batched Contextual Optimization:} Merging cannot be done
  globally (context overflow) or individually (missing the connection).
  It requires a \textbf{Topology-Aware Batching Strategy}---grouping
  rules by their \emph{dependencies} (e.g., the sensors they touch) to
  allow the LLM to see the ``full picture'' for a specific equipment
  unit.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\tightlist
\item
  \textbf{Option A: Syntactic Code Clone Detection (Traditional).}

  \begin{itemize}
  \tightlist
  \item
    \emph{Method:} Using Abstract Syntax Tree (AST) fingerprinting or
    token-based similarity (e.g., Jaccard Index, SourcererCC) to find
    duplicate rules.
  \item
    \emph{Verdict:} \textbf{Discarded.} These algorithms are
    \textbf{Semantic-Blind}. They can identify exact duplicates
    (Copy-Paste), but fail to merge logic. For example, they cannot
    determine that a rule \texttt{temp\ \textgreater{}\ 100} (Safety)
    and \texttt{temp\ \textgreater{}\ 90} (Warning) should be merged
    into a single hierarchical rule. They treat them as distinct code
    blocks.
  \end{itemize}
\item
  \textbf{Option B: Symbolic Logic Solvers (SMT/Z3).}

  \begin{itemize}
  \tightlist
  \item
    \emph{Method:} Converting Python rules into First-Order Logic
    formulas and using an SMT solver (like Z3) to mathematically prove
    equivalence or redundancy (e.g., verifying that
    \texttt{x\ \textgreater{}\ 50} implies
    \texttt{x\ \textgreater{}\ 40}).
  \item
    \emph{Verdict:} \textbf{Discarded.} While mathematically rigorous,
    this requires a \textbf{perfect formal translation} of Python code.
    The stochastic nature of LLM-generated code (using varied libraries,
    string formatting, or helper functions) makes automated translation
    to formal logic brittle and prone to parsing errors.
  \end{itemize}
\item
  \textbf{Option C: Vector-Based Semantic Clustering.}

  \begin{itemize}
  \tightlist
  \item
    \emph{Method:} Embedding rule bodies using CodeBERT and clustering
    centroids.
  \item
    \emph{Verdict:} \textbf{Discarded.} Good for grouping, bad for
    merging. It groups ``High Pressure'' and ``Low Pressure'' together
    (high similarity), but lacks the \textbf{reasoning} capabilities to
    synthesize a new, valid Python function that combines them without
    breaking the logic.
  \end{itemize}
\item
  \textbf{Option D: LLM-Based Dependency Batching.}

  \begin{itemize}
  \tightlist
  \item
    \emph{Method:} Grouping rules by Sensor ID topology and using an LLM
    as a ``Semantic Compiler'' to refactor code.
  \item
    \emph{Verdict:} \textbf{Selected.} It combines the ``grouping''
    benefit of clustering with the ``code generation'' capability
    required to write the merged function.
  \end{itemize}
\end{itemize}

\textbf{Implementation:} The workflow functions as a ``Linker'' in a
compiler toolchain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Dependency Analysis (\texttt{\_analyze\_rules}):}

  \begin{itemize}
  \tightlist
  \item
    The system scans the extracted Python code (from Section 5.3) to
    build a dependency map:
    \texttt{Rule\ A\ -\textgreater{}\ \{TIC-101,\ PIC-204\}}.
  \item
    It creates ``Sensor Groups'' using a clustering strategy. Rules
    touching the same sensors are grouped into a single batch (capped at
    \texttt{MAX\_RULES\_PER\_BATCH} to preserve attention).
  \end{itemize}
\item
  \textbf{Semantic Optimization (\texttt{\_consolidate\_with\_llm}):}

  \begin{itemize}
  \tightlist
  \item
    The LLM acts as an optimizer with three specific operators:

    \begin{itemize}
    \tightlist
    \item
      \textbf{REMOVE:} For exact semantic duplicates (e.g., chunk
      overlap).
    \item
      \textbf{MERGE:} For combining related logic (e.g., combining
      ``High Alarm'' and ``Low Alarm'' into a single
      \texttt{check\_pressure\_range()} function).
    \item
      \textbf{SIMPLIFY:} For reducing boolean complexity (e.g.,
      \texttt{A\ \textgreater{}\ 50\ AND\ A\ \textgreater{}\ 60} \(\to\)
      \texttt{A\ \textgreater{}\ 60}).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Safety Verification (\texttt{\_verify\_consolidated}):}

  \begin{itemize}
  \tightlist
  \item
    The \emph{newly generated} consolidated code must pass the same
    rigorous AST verification (Section 5.7) as the original rules. This
    prevents the optimizer from introducing syntax errors or
    hallucinating new sensors during the merge.
  \end{itemize}
\end{enumerate}

\textbf{Code Strategy (Batching Logic):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We group rules by the sensors they control to ensure local context}
\KeywordTok{def}\NormalTok{ \_group\_rules\_into\_batches(}\VariableTok{self}\NormalTok{, rules):}
\NormalTok{    sensor\_groups }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{list}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ rule }\KeywordTok{in}\NormalTok{ rules:}
        \CommentTok{\# Extract sensors via AST to find dependencies}
\NormalTok{        sensors }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_extract\_sensors\_from\_rule\_body(rule[}\StringTok{"rule\_body"}\NormalTok{])}
        \CommentTok{\# Create a key based on the sensor topology}
\NormalTok{        key }\OperatorTok{=} \BuiltInTok{tuple}\NormalTok{(}\BuiltInTok{sorted}\NormalTok{(sensors)) }
\NormalTok{        sensor\_groups[key].append(rule)}
    \CommentTok{\# ... (splitting logic)}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}
\tightlist
\item
  \textbf{Reduction Ratio:} The system achieved a consolidation ratio of
  \textasciitilde3:1 (processing 150 raw extracted fragments resulted in
  \textasciitilde50 clean, high-value consolidated rules).
\item
  \textbf{Conflict Resolution:} The batching strategy successfully
  identified conflicting rules (e.g., one chunk saying ``Max 50'' and
  another ``Max 55''). The LLM prompt forced a resolution based on the
  \texttt{rule\_source} (preferring the ``Safety Manual'' over the
  ``General Description'').
\item
  \textbf{Output Quality:} The \texttt{SIMPLIFY} action successfully
  removed redundant intermediate variables, producing cleaner Python
  code than the initial extraction pass.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\tightlist
\item
  \textbf{Cross-Batch Dependencies:} The current system groups by
  \emph{exact} sensor sets. It might miss a merge opportunity between
  \texttt{Rule\ A(TIC-101)} and \texttt{Rule\ B(TIC-101,\ PIC-202)}. A
  graph-based clustering algorithm (e.g., Louvain) could optimize the
  batches better.
\item
  \textbf{Human-in-the-Loop:} For \texttt{MERGE} actions with low
  confidence (\textless0.8), the system should pause and request human
  approval.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\tightlist
\item
  \textbf{Zhang, T., et al.~(2023).} \emph{DS-1000: A Natural and Robust
  Benchmark for Data Science Code Generation.} (Discusses the difficulty
  of code simplification and merging).
\item
  \textbf{Karmakar, A., et al.~(2022).} \emph{Pre-trained Language
  Models for Code Understanding: A Survey.} (Validates the approach of
  using LLMs for code refactoring and optimization tasks).
\end{itemize}

\section{5.9. High-Throughput Architecture \& Performance
Optimization}\label{high-throughput-architecture-performance-optimization}

\textbf{Research-based Hypothesis:}

\begin{itemize}
\tightlist
\item
  \textbf{I/O Bound Latency:} The extraction pipeline is dominated by
  \textbf{Network I/O} (waiting for LLM token generation, Vector DB
  retrieval, and Web Search), not CPU computation.
\item
  \textbf{Concurrency vs.~Parallelism:} Since the primary bottleneck is
  waiting for external APIs, \textbf{Asynchronous Concurrency} (Python
  \texttt{asyncio}) is vastly more efficient than Multiprocessing. It
  allows the system to process dozens of document chunks while waiting
  for I/O, maximizing throughput without the overhead of OS-level
  process forking.
\item
  \textbf{Rate-Limit Constraints:} In a high-concurrency environment,
  the system performance is bounded not by hardware, but by
  \textbf{Token-Per-Minute (TPM)} quotas of the LLM provider. An
  effective architecture must implement ``Backpressure'' to handle these
  limits gracefully.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}
\tightlist
\item
  \textbf{Option A: Synchronous Sequential Processing.} Processing chunk
  \(n\) only after chunk \(n-1\) finishes.

  \begin{itemize}
  \tightlist
  \item
    \emph{Verdict:} \textbf{Discarded.} Processing a 50-page manual took
    \textgreater45 minutes. Unacceptable for interactive use.
  \end{itemize}
\item
  \textbf{Option B: Multi-Threading/Multi-Processing.} Using
  \texttt{ThreadPoolExecutor}.

  \begin{itemize}
  \tightlist
  \item
    \emph{Verdict:} \textbf{Discarded.} Python's Global Interpreter Lock
    (GIL) limits threading efficiency, and Multiprocessing introduces
    high memory overhead (loading the Embedding model in every process).
  \end{itemize}
\item
  \textbf{Option C: Asynchronous Event Loop (AsyncIO).} Using
  \texttt{async/await} patterns to suspend execution during I/O waits.

  \begin{itemize}
  \tightlist
  \item
    \emph{Verdict:} \textbf{Selected.} Matches perfectly with
    LangChain/LangGraph's async capabilities. Allows processing
    \textasciitilde50 chunks concurrently on a single core.
  \end{itemize}
\end{itemize}

\textbf{Implementation:} The final architecture utilizes a
\textbf{Map-Reduce pattern} implemented via \textbf{LangGraph}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The Map Step (Extraction):}

  \begin{itemize}
  \tightlist
  \item
    The document is split into \(N\) chunks.
  \item
    We utilize \texttt{asyncio.gather(*tasks)} to spawn \(N\) instances
    of the \texttt{RuleExtractionWorkflow} (from Section 5.3).
  \item
  \item
    Each instance operates independently: retrieving context (Qdrant),
    searching web (Tavily), and generating Python code.
  \item
    \emph{Semaphore:} A \texttt{asyncio.Semaphore(limit=5)} is
    implemented to control the ``Blast Radius''---preventing us from
    hitting OpenAI/Anthropic rate limits by ensuring only 5 chunks
    trigger LLM calls simultaneously.
  \end{itemize}
\item
  \textbf{The Reduce Step (Consolidation):}

  \begin{itemize}
  \tightlist
  \item
    Once all extraction tasks complete, results are aggregated.
  \item
    The \texttt{RuleConsolidationWorkflow} (Section 5.8) takes over.
  \item
    It utilizes \textbf{Batch Processing} rather than full parallelism
    here, grouping rules by sensor topology to ensure context
    preservation.
  \end{itemize}
\end{enumerate}

\textbf{Code Strategy (Async Orchestration):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simplified representation of the async pipeline}
\ControlFlowTok{async} \KeywordTok{def}\NormalTok{ process\_document(doc\_chunks, sensors):}
    \CommentTok{\# Limit concurrency to avoid Rate Limit Errors (429)}
\NormalTok{    semaphore }\OperatorTok{=}\NormalTok{ asyncio.Semaphore(}\DecValTok{5}\NormalTok{)}

    \ControlFlowTok{async} \KeywordTok{def}\NormalTok{ processed\_chunk(chunk):}
        \ControlFlowTok{async} \ControlFlowTok{with}\NormalTok{ semaphore:}
            \CommentTok{\# Run the full extraction graph for one chunk}
            \ControlFlowTok{return} \ControlFlowTok{await}\NormalTok{ extraction\_workflow.arun(chunk, sensors}\OperatorTok{=}\NormalTok{sensors)}

    \CommentTok{\# Schedule all chunks to run "simultaneously"}
\NormalTok{    tasks }\OperatorTok{=}\NormalTok{ [processed\_chunk(chunk) }\ControlFlowTok{for}\NormalTok{ chunk }\KeywordTok{in}\NormalTok{ doc\_chunks]}
    
    \CommentTok{\# Wait for all to finish}
\NormalTok{    extracted\_results }\OperatorTok{=} \ControlFlowTok{await}\NormalTok{ asyncio.gather(}\OperatorTok{*}\NormalTok{tasks)}
    
    \CommentTok{\# Pass results to consolidation}
    \ControlFlowTok{return} \ControlFlowTok{await}\NormalTok{ consolidation\_workflow.arun(extracted\_results)}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}
\tightlist
\item
  \textbf{Latency Reduction:}

  \begin{itemize}
  \tightlist
  \item
    \emph{Sequential:} \textasciitilde45 minutes for 50 pages (approx.
    54s per page).
  \item
    \emph{Asynchronous:} \textasciitilde4 minutes for 50 pages (limited
    only by the Semaphore).
  \item
    \textbf{Speedup Factor:} \textasciitilde11x.
  \end{itemize}
\item
  \textbf{Resource Efficiency:} The system runs comfortably on a
  standard container (2 vCPU, 4GB RAM) because the heavy lifting is
  offloaded to the API providers. The local embedding model (Ollama) was
  the only significant CPU consumer.
\item
  \textbf{The ``Thundering Herd'' Problem:} Initially, firing 50
  requests caused Qdrant to timeout. Implementing the Semaphore and
  exponential backoff retries stabilized the system.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}
\tightlist
\item
  \textbf{Streaming:} The current system waits for \emph{all} chunks to
  finish before consolidation. A \textbf{Streaming Architecture} could
  begin consolidating ``Flow Control'' rules as soon as the ``Flow''
  section chunks are processed, progressively updating the user UI.
\item
  \textbf{Caching:} Implementing a semantic cache (Redis) for common
  queries (e.g., resolving ``TIC-101'') to avoid redundant LLM calls
  across different documents.
\end{itemize}

\textbf{Literature:}

\begin{itemize}
\tightlist
\item
  \textbf{Dean, J., \& Ghemawat, S. (2008).} \emph{MapReduce: Simplified
  Data Processing on Large Clusters.} (The foundational pattern for this
  architecture).
\item
  \textbf{Fowler, M.} \emph{Patterns of Enterprise Application
  Architecture.} (Reference for the Async/Await and Throttling
  patterns).
\end{itemize}
