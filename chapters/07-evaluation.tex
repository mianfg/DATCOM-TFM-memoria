% =============================================================================
% CHAPTER 7: EVALUATION AND RESULTS
% =============================================================================

\chapter{Evaluation and Results}\label{evaluation-and-results}

\section{Evaluation Methodology}\label{evaluation-methodology}

Since historical anomaly logs are unavailable for the Repsol case study,
we adopt a \textbf{Component-Wise Evaluation Strategy}. We decouple the
assessment of the \emph{Language Model's understanding} from the
\emph{Runtime Engine's performance}.

We establish a \textbf{Taxonomy of Rule Complexity} to categorize test cases:

\begin{enumerate}
\item \textbf{Type I (Explicit):} ``The maximum temperature of T-101 is 50Â°C.'' (Simple extraction).
\item \textbf{Type II (Tabular):} Rules hidden inside specification tables. (Layout awareness).
\item \textbf{Type III (Temporal/Complex):} ``If pressure drops by 10\% over 5 minutes while flow is stable\ldots{}'' (Logic + Time parsing).
\item \textbf{Type IV (Implicit/Domain):} ``Ensure the reflux drum does not run dry.'' (Requires grounding to know ``dry'' means Level \textless{} 5\%).
\end{enumerate}

\section{Evaluation of Rule Extraction (NLP Performance)}\label{evaluation-of-rule-extraction-nlp-performance}

\emph{Instead of guessing if the extraction on the real document is
correct, we ``Reverse Engineer'' a test document.}

\subsection{Dataset Generation (The ``Twin'' Approach)}\label{dataset-generation-the-twin-approach}

We created a \textbf{Synthetic Evaluation Dataset} consisting of 50 ``Ground Truth Rules'' (\(R_{GT}\)) covering the four complexity types.

\begin{enumerate}
\item \textbf{Step 1 (Define Truth):} We manually wrote 50 Python functions representing ideal operational logic.
\item \textbf{Step 2 (Generate Artifact):} We used an LLM to ``reverse'' these functions into a generic Industrial Technical Manual (PDF) containing natural language, tables, and scattered context.
\item \textbf{Step 3 (Blind Extraction):} The pipeline processed this synthetic PDF to generate \(R_{Pred}\) (Predicted Rules).
\end{enumerate}

\subsection{Semantic Similarity Metrics}\label{semantic-similarity-metrics}

Since the extracted text won't match the Ground Truth word-for-word, we evaluate based on \textbf{Semantic Equivalence} (using \texttt{ast} comparison or embedding similarity):

\begin{itemize}
\item \textbf{Precision:} \(\frac{\text{Correctly Extracted Rules}}{\text{Total Extracted Rules}}\) (Did we hallucinate?)
\item \textbf{Recall:} \(\frac{\text{Correctly Extracted Rules}}{\text{Total Ground Truth Rules}}\) (Did we miss anything?)
\item \textbf{Code Pass Rate:} Percentage of extracted rules that are syntactically valid Python.
\end{itemize}

\textbf{Expected Results Table:} \textbar{} Complexity \textbar{}
Precision \textbar{} Recall \textbar{} Challenges Identified \textbar{}
\textbar{} --- \textbar{} --- \textbar{} --- \textbar{} --- \textbar{}
\textbar{} Type I (Explicit) \textbar{} 1.00 \textbar{} 1.00 \textbar{}
Solved problem. \textbar{} \textbar{} Type II (Tables) \textbar{} 0.95
\textbar{} 0.90 \textbar{} Docling handles this well; merged cells cause
minor issues. \textbar{} \textbar{} Type III (Temporal)\textbar{} 0.85
\textbar{} 0.80 \textbar{} ``5 minutes ago'' vs ``last 5 minutes''
ambiguity. \textbar{} \textbar{} Type IV (Implicit)\textbar{} 0.70
\textbar{} 0.60 \textbar{} Requires Grounding (Search) to resolve ``run
dry.'' \textbar{}

\section{Evaluation of Sensor Resolution (Entity Linking)}\label{evaluation-of-sensor-resolution-entity-linking}

\emph{Here, you test the mapping of ``Column Temp'' -\textgreater{}
``TIC-101''.}

\begin{itemize}

\item
  \textbf{Method:} Create a ``Mock Sensor Database'' (CSV) with 100 tags
  (some with similar names).
\item
  \textbf{Test:} Check if the pipeline resolves ambiguous terms
  correctly in the Synthetic Document.
\item
  \textbf{Metric:} \textbf{Resolution Accuracy @ k} (Did the correct tag
  appear in the top 1 or top 3 choices?).
\item
  \textbf{Result:} Show that your \textbf{Context-Aware} approach
  (Section 5.4) outperforms standard Vector Search (RAG) by correct
  disambiguating ``Control'' vs.~``Indicator'' tags.
\end{itemize}

\section{Evaluation of Runtime Execution (System Performance)}\label{evaluation-of-runtime-execution-system-performance}

\emph{Since you cannot verify if real alarms are correct, you verify if
the ENGINE is mathematically correct and fast.}

\subsection{Correctness Verification (The ``Shadow Test'')}\label{correctness-verification-the-shadow-test}

We prove that the \texttt{RuleChecker} (River/Streaming) produces the exact same math as a standard batch processor (Pandas).

\begin{itemize}
\item \textbf{Setup:} Take a slice of your sensor data (e.g., 10,000 rows).
\item \textbf{Method:}
\begin{enumerate}
\item Run Rule X using \texttt{pandas.rolling(window=\textquotesingle{}1h\textquotesingle{}).mean()}.
\item Run Rule X using \texttt{RuleChecker} (River).
\item Compare the output values at every timestamp.
\end{enumerate}
\item \textbf{Result:} Mean Squared Error (MSE) between the two should be \(\approx 0\). This proves your \(O(1)\) streaming implementation is \textbf{mathematically trustworthy}.
\end{itemize}

\subsection{Fault Injection (The ``Fire Drill'')}\label{fault-injection-the-fire-drill}

Since the data might be ``normal'' (no alarms), we artificially inject faults to prove the rules \emph{would} trigger.

\begin{itemize}
\item \textbf{Method:}
\begin{enumerate}
\item Select a rule: ``Alert if Temp \textgreater{} 100''.
\item Modify the CSV: At \(t=500\), manually set Temp = 105.
\item Run \texttt{RuleChecker}.
\end{enumerate}
\item \textbf{Success Metric:} \textbf{Detection Rate}. Did the system log an event at exactly \(t=500\)?
\item \emph{This validates that your system isn't just running code, but actually catching events.}
\end{itemize}

\subsection{Computational Performance (Scalability)}\label{computational-performance-scalability}

\emph{Measure the efficiency of your \texttt{RuleChecker}.}

\begin{itemize}
\item \textbf{Metric 1: Latency per Row.} (e.g., ``Processed 1,000 rules in 2.5ms'').
\item \textbf{Metric 2: Memory Stability.} Show that memory usage remains flat (constant) over 1 hour of processing, proving the \(O(1)\) claim vs.~Pandas which might grow with window size.
\end{itemize}
