\chapter{Evaluation and Results}\label{ch:evaluation}

Since historical anomaly logs are unavailable for the industrial case study, we adopt a \textbf{component-wise evaluation strategy} that decouples assessment of the rule extractor (\Cref{sec:eval-extraction}) from the runtime engine's performance (\Cref{sec:eval-runtime}).


\section{Evaluation of Rule Extraction}\label{sec:eval-extraction}

In order to generate synthetic rules for evaluation, we establish a \textbf{taxonomy of rule complexity}:
\begin{enumerate}
    \item \textbf{Type I (Explicit):} Simple threshold rules (e.g, ``temperature $>$ 100Â°C'')
    \item \textbf{Type II (Tabular):} Multi-sensor rules from specification tables
    \item \textbf{Type III (Temporal):} Time-window rules using rolling statistics
    \item \textbf{Type IV (Implicit):} Domain knowledge rules requiring grounding
\end{enumerate}

\subsection{Dataset Generation}

We created a synthetic evaluation dataset consisting of 32 ground truth rules ($R_{GT}$) covering the four complexity types (8 rules per type).

\begin{enumerate}
    \item \textbf{Step 1 (Define Truth):} LLM generates 32 Python rule functions following the \texttt{status.get()} API.
    \item \textbf{Step 2 (Generate Artifact):} LLM creates synthetic technical documents embedding these rules as natural language prose and tables.
    \item \textbf{Step 3 (Blind Extraction):} The pipeline processes these documents to generate $R_{Pred}$.
\end{enumerate}

\subsection{Semantic Similarity Metrics}

Since extracted text won't match ground truth word-for-word, we evaluate using \textbf{semantic equivalence}, using a combination of rule name token similarity (50\%) and sensor overlap (50\%) with threshold $\geq 0.3$. The rules are then manually checked to see if they matched the rules originally created.

Afterwards, we set certain metrics:

\begin{itemize}
    \item \textbf{Recall:} $\frac{\text{Matched Rules}}{\text{Total Ground Truth Rules}}$
    \item \textbf{Code Pass Rate:} Percentage of extracted rules that are valid according to the method laid out in \Cref{sec:implementation-8-verification}.
    \item \textbf{Extraction Ratio:} proportion of rules generated in contrast with ground truth rules.
\end{itemize}

The results can be seen in \Cref{tab:extraction-results, graph:recall}. We see that the rules are correctly recalled, with the rules of higher type being more difficult to find. Also, we see that the code pass rate is globally below 60\%. Closer inspection of the database reveals most of this is because of sensor parsing. The list of sensors generated might not be realistic enough. Also, the extraction ratio is really big. This is something we expect, as the system is prone to generating many rules, and because the documents contain more information than the 8 rules explicitly set.

\vspace*{2em}

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Complexity Type} & \textbf{Ground Truth} & \textbf{Matched} & \textbf{Recall} & \textbf{Code Pass Rate} & \textbf{Extraction Ratio} \\
\midrule
Type I (Explicit) & 8 & 8 & 100.00\% & 62.68\% & 17.8x \\
Type II (Tabular) & 8 & 7 & 87.50\% & 58.97\% & 19.5x \\
Type III (Temporal) & 8 & 8 & 100.00\% & 57.43\% & 18.5x \\
Type IV (Implicit) & 8 & 6 & 75.00\% & 55.86\% & 18.1x\\
\midrule
\textbf{Overall} & 32 & 27 & 90.63\% & 58.73\% & 18.46x \\
\bottomrule
\end{tabular}
\caption{Rule extraction recall and code pass rate by complexity type}
\label{tab:extraction-results}
\end{table}


\begin{figure}[hbtp]
\hspace*{-0.1\textwidth}\includegraphics[width=1.2\textwidth]{img/recall.png}
\caption{Performance metrics for rule extraction module}
\label{graph:recall}
\end{figure}

\section{Evaluation of Rule Execution}\label{sec:eval-runtime}

To evaluate the runtime, we take the rules generated and apply them to a series of synthetically generated sensor data. This data is generated randomly.

Our goal is to see how the increase in rules affects our execution framework in time and space. The results are shown in \Cref{graph:streaming}. As we can see, the latency is increased with the number of rules, but our CV score from the latency vs. window size (lower than 10\%) shows us that our model is very efficient. This is most probably due to the usage of a well-mantained and production prone library such as River.

Another interesting comparison is to see the difference in execution times between using streaming (River) and normal dataframes (Pandas). We can see an average of a 46x speedup in using the streaming approach.

Regarding memory, we can see that the memory usage is flat, revealing an $O(1)$ memory usage.


\begin{figure}[hbtp]
\hspace*{-0.1\textwidth}\includegraphics[width=1.2\textwidth]{img/streaming.png}
\caption{Performance metrics for rule execution module}
\label{graph:streaming}
\end{figure}

