\chapter{0. Abstract}\label{abstract}

\emph{Brief summary of problem, approach, contributions, and results}

\begin{itemize}

\item
  Ensure the abstract follows a strict ``narrative arc'': Context (1
  sentence) \(\to\) Gap (1 sentence) \(\to\) Hypothesis/Method (2
  sentences) \(\to\) Results (quantified) \(\to\) Impact.
\item
  ``we propose a multi-stage semantic compilation framework that treats
  natural language specifications as source code, formalized into
  executable rules and deployed on a lightweight streaming engine.''
\item
  Quantify: Mention the exact metric improvement if possible (e.g.,
  consolidation ratio, \% valid rules)
\item
  Application in industrial contexts: ``Validation on a streaming
  runtime demonstrates that the extracted rules can be evaluated in
  \(O(1)\) time using incremental statistics, confirming suitability for
  high-frequency industrial edge deployment''
\end{itemize}

\chapter{1. Introduction}\label{introduction}

\section{1.1. Motivation and Industrial
Context}\label{motivation-and-industrial-context}

\begin{itemize}

\item
  Industrial process monitoring and anomaly detection challenges,
  knowledge extraction challenge (from an \emph{epistemological}
  perspective, it's not just a ``messy PDF'' problem)

  \begin{itemize}
  
  \item
    Dark data: industry-oprational knowledge that exists but is
    unqueryable and unexecutable
  \item
    Runtime gap: even if rules are extracted, they are often expressed
    in natural language complexities (e.g., `rolling average') that are
    computationally expensive to compute on historical data. A bridge is
    needed between semantic rule definition and efficient stream
    processing
  \end{itemize}
\item
  \textbf{Case Study Introduction}: C3/C4 splitter separation at Repsol

  \begin{itemize}
  
  \item
    Process description (what can be shared publicly)
  \item
    Why this is a representative problem: complexity, usage
  \item
    The knowledge documentation challenge
  \end{itemize}
\item
  The gap: No consistent automated rule extraction framework from
  technical documents
\end{itemize}

\section{1.2. Problem Statement}\label{problem-statement}

\begin{itemize}

\item
  Knowledge accessibility: Knowledge trapped in unstructured
  documentation (PDFs, manuals, specs)
\item
  Domain experts required for rule definition (expensive, not scalable)
\item
  Translation gap: there's no formal grammar mapping natural language
  (ambiguous, context-heavy) to control logic (binary, precise)
\item
  Silent failure risk: manual rule creation is prone to human error that
  goes undetected until a safety incident occurs. Automated extraction
  offers \emph{systematic} verification
\item
  Usefulness and application of these rules (control of processes,
  alarms, tracking of ideal behavior\ldots)
\item
  Need for: automated extraction, explainability, maintainability,
  traceability
\end{itemize}

\section{1.3. Research Questions}\label{research-questions}

\emph{These questions will be addressed and referenced throughout the
whole work}

\begin{itemize}

\item
  \emph{RQ1 (Feasibility)}: How can operational knowledge embedded in
  unstructured industrial documentation be automatically extracted and
  formalized into executable rules? / To what extent can Large Language
  Models serve as \textbf{semantic compilers} for translating
  unstructured industrial specifications into executable logic?

  \begin{itemize}
  
  \item
    Opens up: is automated extraction feasible? what representations
    work? what extraction methods? what are the challenges?
  \item
    Answer: proposed method (LLMs + multi-stage pipeline with retrieval
    and grounding + Python functions)
  \end{itemize}
\item
  \emph{RQ2 (Ambiguity)}: What are the fundamental challenges in
  translating natural language process specifications into
  machine-executable monitoring rules, and how can they be addressed? /
  What are the key technical barriers to automating industrial rule
  extraction, and what approaches can address them? / How can the
  \textbf{ontological gap} between natural language descriptions and
  precise sensor instrumentation be bridged without extensive manual
  labeling?

  \begin{itemize}
  
  \item
    Opens up: sensor ambiguity and retrieval, time expressions,
    validation, domain knowledge gaps
  \item
    Answer: proposed method (sensor resolution, time parsing, grounding,
    verification)
  \end{itemize}
\item
  \emph{RQ3 (Reasoning)}: To what extent can large language models
  reason about industrial processes? / What role can external knowledge
  and context play in improving the accuracy and completeness of
  LLM-based rule extraction?

  \begin{itemize}
  
  \item
    Opens up: do LLMs understand industrial concepts? do they need
    context or grounding?
  \item
    Answer: RAG + grounding approach proposed, interesting to talk about
    RAG vs.~fine-tuning here and general-purpose (foundational) models
  \end{itemize}
\item
  \emph{RQ4 (Explainability and trust)}: What properties must an
  automated rule extraction system possess to be trustworthy and
  deployable in industrial settings? / How can automatically extracted
  rules achieve the level of trustworthiness required for industrial
  deployment?

  \begin{itemize}
  
  \item
    Opens up: explainability, traceability; as future work: HITL and
    human validation/oversight
  \item
    Answer: traceability, explainability in extraction and consolidation
  \end{itemize}
\item
  \emph{RQ5 (Quality)}: How can the quality and consistency of
  automatically extracted rules be assessed and improved without
  extensive manual review?

  \begin{itemize}
  
  \item
    Opens up: quality metrics, redundancy detection, consolidation
  \item
    Answer: consolidation workflow
  \end{itemize}
\item
  \emph{RQ6 (Complexity)}: How can the computationally expensive logic
  inherent in natural language specifications (e.g., sliding windows,
  statistical aggregations) be executed efficiently in real-time
  industrial environments?

  \begin{itemize}
  
  \item
    Opens up: streaming framework and rule visualization
  \end{itemize}
\end{itemize}

TODO: To what extent can this approach generalize to real-world
industrial use cases? (not as research question, but in conclusions)

\section{1.4. Research Approach and
Objectives}\label{research-approach-and-objectives}

\textbf{Research approach:} - Iterative design methodology:
\textbf{``Design Science Research'' (DSR)} - Focus on modular, swappable
components: analyze implementation options and justify chosen one, for
each component - Validation with real industrial documentation

\textbf{Primary Objective:} Design and validate an end-to-end framework
for automated extraction of operational monitoring rules from
unstructured industrial documentation

\textbf{Specific Objectives:} - \emph{O1}: Develop a rule extraction
methodology that bridges the gap between natural language specifications
and executable code - \emph{O2}: Investigate and implement mechanisms to
overcome LLM limitations in domain-specific industrial contexts -
\emph{O3}: Address the semantic ambiguity inherent in industrial
documentation - \emph{O4}: Establish quality assurance mechanisms for
automatically extracted rules - \emph{O5}: Ensure trustworthiness
through explainability and traceability - \emph{O6}: Efficiently execute
rules in an industrial context - \emph{O7}: Validate the approach on
mock and on real-world industrial use cases - \emph{O8}: Design a
scalable, production-ready system architecture - \emph{O9}: Establish
evaluation methodology and conduct systematic assessment

\chapter{2. Background and State of the
Art}\label{background-and-state-of-the-art}

\section{2.1. Anomaly Detection in Industrial
Processes}\label{anomaly-detection-in-industrial-processes}

\begin{itemize}

\item
  Traditional, data-driven approaches (statistical, ML-based)

  \begin{itemize}
  
  \item
    Popular, but lacks interpretability and requires massive historical
    failure data (which is rare in safe plants)
  \end{itemize}
\item
  Rule extraction-based approaches (knowledge-driven) instead of
  statistical/ML: generate explainable rules

  \begin{itemize}
  
  \item
    Role of domain knowledge
  \end{itemize}
\item
  \textbf{Gap}: Automated rule extraction from documentation
\end{itemize}

\section{2.2. Knowledge Extraction (IE)}\label{knowledge-extraction-ie}

\begin{itemize}

\item
  Traditional ML-based approach for knowledge extraction: ontology
  learning, relation extraction
\item
  Limitations for complex rule extraction

  \begin{itemize}
  
  \item
    Traditional NLP (spacy, regex) fails on nested logic and implicit
    content
  \item
    Current IE systems extract facts (Subject-Predicate-Object), but
    rarely extract procedures or executable logic (If-Then-Else), or too
    strict
  \end{itemize}
\end{itemize}

\section{2.3. LLM-based Rule
Generation}\label{llm-based-rule-generation}

\begin{itemize}

\item
  Why LLMs are ideal in this use case (multiple research papers showing
  comparison and results)

  \begin{itemize}
  
  \item
    Versatile reasoning and generalization
  \item
    Contextual understanding
  \item
    Code generation
  \item
    Automation and control
  \end{itemize}
\item
  LLM techniques normally used in this context
\item
  State of the art in information extraction (structured information
  from scientific papers, industrial documents\ldots)

  \begin{itemize}
  
  \item
    Code-as-Policy literature
  \end{itemize}
\item
  State of the art in rule generation. Show existing approaches and its
  limitations
\item
  Positioning: apply Code-as-Policy to industrial process domain
\end{itemize}

\chapter{3. Technical Foundations}\label{technical-foundations}

\emph{Explain the building blocks without committing to how we use them
yet}

\section{3.1. Large Language Models}\label{large-language-models}

\begin{itemize}

\item
  Architecture overview (transformers, attention). Foundational models
\item
  Capabilities: text understanding, reasoning (Chain-of-Thought,
  Program-of-Thought), code generation
\item
  Limitations: hallucination, context windows (lost in the middle paper,
  recall), domain knowledge gaps
\item
  Prompt engineering fundamentals
\item
  Structured outputs
\end{itemize}

\section{3.2. Retrieval Augmented Generation
(RAG)}\label{retrieval-augmented-generation-rag}

\begin{itemize}

\item
  The RAG paradigm: retrieval + generation
\item
  Why RAG? Overcoming context limitations and knowledge gaps
\item
  RAG vs.~fine-tuning
\item
  Vector embeddings and semantic search
\item
  Retrieval strategies: dense, sparse, hybrid
\item
  Limitations and challenges
\end{itemize}

\section{3.3. Grounding and External Knowledge
Integration}\label{grounding-and-external-knowledge-integration}

\begin{itemize}

\item
  Concept of grounding in LLMs
\item
  Web search as knowledge source
\item
  When and why grounding helps
\item
  Trade-offs: latency, cost, accuracy
\end{itemize}

\section{3.4. LLM Orchestration and
Workflows}\label{llm-orchestration-and-workflows}

\begin{itemize}

\item
  Beyond single LLM calls: multi-step reasoning
\item
  State machines and workflow frameworks
\item
  LangChain and LangGraph concepts
\item
  Checkpointing and resumability
\end{itemize}

\section{3.5. Traceability and Explainability in AI
Systems}\label{traceability-and-explainability-in-ai-systems}

\begin{itemize}

\item
  Observability and OpenTelemetry
\item
  Provenance tracking
\item
  Human-in-the-loop considerations
\end{itemize}

\chapter{3.6. Online Machine Learning \& Stream
Processing}\label{online-machine-learning-stream-processing}

\begin{itemize}

\item
  Batch vs.~Stream processing
\item
  The concept of Incremental Statistics
\item
  Introduction to River (Python library for online ML)
\item
  Why \(O(1)\) complexity matters for Edge Computing
\end{itemize}

\chapter{4. Methodology and Design}\label{methodology-and-design}

\emph{Briefly state the approach when designing and implementing the
main solution of this work. Dual engineering+research-based approach}

\section{4.1. Requirements}\label{requirements}

\begin{itemize}

\item
  Functional requirements (input/output, deterministic output, rule
  format, validation)
\item
  Non-functional requirements (accuracy, traceability, modularity,
  scalability, low-latency execution)
\item
  Derived from Repsol use case and general industrial needs
\end{itemize}

\section{4.2. Design Principles}\label{design-principles}

\emph{Show design principles followed and why they are important} -
Modularity and composability (why important, protocol-based design) -
Separation of concerns * \emph{LLM Role:} Translation (Text \(\to\)
Code). * \emph{System Role:} Verification (Code \(\to\) Safety). *
\emph{Human Role:} Approval. - Decoupling of Logic and State: extracted
rule (Logic) must be stateless; the execution engine (State) handles the
memory buffers - Traceability and explainability (from documents to
rules)

\section{4.3. Research Principles (this is a research
problem!)}\label{research-principles-this-is-a-research-problem}

\emph{We want to show that every step of the pipeline is based on a
research approach, evaluating alternatives and checking consistency and
results in order to find the best options for designing the system}

(Ablation Study Design? Think with and without and isolate contributions
to performance, might be hard do to this given the time)

\chapter{5. Solution Design and
Implementation}\label{solution-design-and-implementation}

\section{5.0. Baseline: Naive Single-Document
Extraction}\label{baseline-naive-single-document-extraction}

\textbf{Research-based Hypothesis:} * Modern Large Language Models
(LLMs) with extended context windows (e.g., 128k+ tokens) possess
sufficient capacity to ingest complete technical documents (e.g., 50+
page manuals) in a single pass. * Therefore, a zero-shot prompting
strategy without external retrieval mechanisms should be capable of
extracting and synthesizing operational monitoring rules directly from
the raw text.

\textbf{Implementation Options:} * \textbf{Option A: Zero-Shot
Full-Context Extraction.} Loading the entire document string into the
context window and requesting extraction in a single prompt. *
\textbf{Option B: Summarization-then-Extraction.} Asking the model to
summarize the document first, then extract rules from the summary.
(Discarded: Summarization loses the specific numeric thresholds required
for industrial rules). * \textbf{Option C: Map-Reduce.} Splitting the
document, extracting from parts, and summarizing results. (Discarded for
this baseline: This effectively moves into ``Chunking,'' which is
reserved for Section 5.1).

\textbf{Implementation:} * \textbf{Model:} State-of-the-art Foundation
Model (e.g., GPT-4o or Claude 3.5 Sonnet) with \textgreater100k token
context window. * \textbf{Input Data:} Raw text extracted from a PDF of
a C3/C4 Splitter functional description. * \textbf{Prompt Strategy:}
Zero-shot. * \emph{System Prompt:} ``You are an expert industrial
process engineer.'' * \emph{User Prompt:} ``Read the following document
and extract all operational monitoring rules, alarms, and interlocks.''
* \textbf{Output Format:} Unconstrained natural language (Plain text).

\textbf{Results:} * \textbf{Recall Degradation (``Lost in the
Middle''):} While the model accepted the input, it failed to extract
rules located in the middle sections of the document, prioritizing
information at the very beginning (Introduction) and very end
(Appendices). * \textbf{Context Overflow:} For larger manuals
(\textgreater100 pages), the token count exceeded the model's efficient
processing limit, leading to truncation or refusal to process. *
\textbf{Hallucination:} The model occasionally generated generic
best-practice rules for distillation columns that were not present in
the specific source text. * \textbf{Parsing Failure:} The output format
was inconsistent (e.g., switching between bullet points, numbered lists,
and paragraphs), rendering programmatic parsing impossible.

\textbf{Possible Improvements:} * \textbf{Address Recall Degradation:}
Implement a \textbf{Chunking Strategy} to break the document into
manageable segments, ensuring the LLM focuses on specific details
without ``attention dilution'' -\textgreater{} \emph{Section 5.1}. *
\textbf{Address Parsing Failure:} Enforce \textbf{Structured Output
(JSON)} using schema validation to ensure every extracted rule follows a
strict format -\textgreater{} \emph{Section 5.1}.

\textbf{Literature:} * \textbf{Liu, N. F., et al.~(2023).} \emph{Lost in
the Middle: How Language Models Use Long Contexts.} arXiv preprint
arXiv:2307.03172. (Demonstrates the U-shaped performance curve where
LLMs forget information in the middle of the context). * \textbf{Xu, P.,
et al.~(2024).} \emph{Retrieval meets Long Context Large Language
Models.} ICLR 2024. (Provides evidence that RAG-based approaches often
outperform long-context approaches for retrieval tasks, supporting the
move away from single-pass extraction).

\section{5.1. Structured Extraction via Naive
Chunking}\label{structured-extraction-via-naive-chunking}

\emph{Highlight the trade-off between \textbf{precision} (gained via
chunking) and \textbf{coherence} (lost due to fragmentation), which is
the perfect setup for your RAG section.}

\textbf{Research-based Hypothesis:} * \textbf{Attention Optimization:}
By decomposing the document into smaller segments (``chunks''), the
LLM's attention mechanism can focus on local details without the
degradation observed in long-context windows. * \textbf{Schema
Constraint:} Forcing the LLM to output according to a strict schema
(e.g., JSON) will reduce generation variability and enable programmatic
parsing of the extracted rules.

\textbf{Implementation Options:} * \textbf{Option A: Fixed-Size
Chunking.} Splitting text by token count (e.g., 1000 tokens) with a
sliding window overlap. (Selected for this baseline). * \textbf{Option
B: Semantic Chunking.} Splitting text based on semantic breaks
(paragraphs, headers). (Discarded for the baseline to test raw
throughput first). * \textbf{Option C: Regex/Template Matching.} Using
traditional NLP patterns. (Discarded as it cannot handle the linguistic
variance of industrial descriptions).

\textbf{Implementation:} * \textbf{Chunking Strategy:} Recursive
Character Text Splitter. * \emph{Chunk Size:} 2000 tokens (balanced to
contain full paragraphs). * \emph{Overlap:} 200 tokens (to mitigate
splitting sentences at boundaries). * \textbf{Output Schema:} A
predefined JSON schema enforced via the LLM's function-calling or
JSON-mode API. * \emph{Fields:} \texttt{Rule\ Name}, \texttt{Condition}
(text), \texttt{Action} (text), \texttt{Sensor\ Tag}. *
\textbf{Workflow:} 1. Load Document. 2. Split into \(N\) chunks. 3.
Parallel execution of \(N\) LLM calls with the prompt: ``Extract rules
from this specific text segment matching the JSON schema.'' 4.
Aggregation of results into a single list.

\textbf{Results:} * \textbf{Improved Extraction Rate:} The system
successfully processed the entire document without context overflow
errors. The JSON structure made the output immediately machine-readable.
* \textbf{Context Fragmentation (The ``Keyhole'' Problem):} The system
failed to capture rules that spanned across chunks. For example, if
Chunk A defined ``The limits for column T-100'' and Chunk B listed
``High: 80Â°C'', the LLM processing Chunk B lacked the context to know
``High'' referred to ``T-100''. -\textgreater{} \emph{Motivates
Contextual RAG (Section 5.2).} * \textbf{Logic Representation Failure:}
The JSON schema proved too rigid to capture complex industrial logic
(e.g., ``IF A \textgreater{} 5 AND (B \textless{} 10 OR C is OPEN)'').
Attempting to stuff this logic into a text field inside JSON
re-introduced ambiguity. -\textgreater{} \emph{Motivates Python Code
Generation (Section 5.5).} * \textbf{Sensor Hallucination/Ambiguity:}
The model often extracted generic terms (e.g., ``the reflux
temperature'') rather than the specific instrument tag (e.g.,
\texttt{TIC-1023}), as the mapping was often defined in a different part
of the document or an external P\&ID. -\textgreater{} \emph{Motivates
Retrieval \& Resolution (Section 5.2/5.3).}

\textbf{Possible Improvements:} * \textbf{Bridge Context Gaps:}
Implement \textbf{Retrieval Augmented Generation (RAG)} to allow the LLM
to ``look up'' definitions or context from other parts of the document
while processing a specific chunk -\textgreater{} \emph{Section 5.2}. *
\textbf{Enhance Logic Expressiveness:} Transition from static JSON data
structures to dynamic \textbf{Python Code} to represent complex boolean
logic and thresholds -\textgreater{} \emph{Section 5.5}. *
\textbf{Sensor Resolution:} Implement a specific retrieval step to map
natural language sensor descriptions to their exact tags in the
instrument database -\textgreater{} \emph{Section 5.3}.

\textbf{Literature:} * \textbf{Vijayan, R., et al.~(2024).} \emph{A
Prompt Engineering Approach for Structured Data Extraction from
Unstructured Text.} ACM International Conference on Proceeding Series.
doi:10.1145/3639631.3639663. (Validates the use of schema enforcement to
reduce hallucination in extraction tasks). * \textbf{Dagdelen, J., et
al.~(2024).} \emph{Structured information extraction from scientific
text with large language models.} Nature Communications, 15, 1418.
doi:10.1038/s41467-024-45563-x. (Demonstrates that while LLMs excel at
local extraction, they require structured workflows to handle complex,
domain-specific scientific data).

\section{5.2. Contextual Augmentation via Retrieval Augmented Generation
(RAG)}\label{contextual-augmentation-via-retrieval-augmented-generation-rag}

\textbf{Research-based Hypothesis:} * \textbf{Semantic Bridging:}
Industrial specifications often decouple definitions (e.g., ``Reflux
Loop'') from their operational parameters (e.g., ``Max Flow: 50 t/h'').
A linear reading strategy fails to link them. Semantic search via dense
vector embeddings can bridge this gap by retrieving relevant definitions
based on meaning rather than keyword matching. *
\textbf{Layout-Awareness:} Technical rules are frequently encoded in
semi-structured formats like tables. Standard text extraction flattens
this structure. Layout-aware parsing is required to preserve the
``row-column'' context. * \textbf{Data Sovereignty:} For industrial
deployment, sensitive process knowledge must be processed locally to
avoid data leakage risks associated with public API endpoints.

\textbf{Implementation Options:} * \textbf{Parsing Strategy:} *
\emph{Option A: PyPDF/LangChain Loaders.} Extracts raw text but destroys
table structures. (Discarded: ``Jumbled'' outputs). * \emph{Option B:
Docling (IBM).} A layout-aware parser that identifies tables, headers,
and reading order. (Selected: Essential for technical spec sheets). *
\textbf{Embedding Strategy:} * \emph{Option A: Cloud-based (e.g., OpenAI
\texttt{text-embedding-3}).} High performance but requires sending data
to external servers. (Discarded due to industrial privacy requirements).
* \emph{Option B: Local Open-Source (e.g., \texttt{mxbai-embed-large}
via Ollama).} Runs entirely on-premise, high ranking on MTEB (Massive
Text Embedding Benchmark). (Selected: Balances state-of-the-art
performance with data privacy). * \textbf{Vector Storage:} *
\emph{Option A: In-Memory (FAISS).} Ephemeral. (Discarded: Cannot scale
or persist). * \emph{Option B: Dedicated Vector Database (Qdrant).}
(Selected: Chosen for HNSW indexing speed, persistence, and metadata
filtering capabilities).

\textbf{Implementation:} 1. \textbf{Ingestion Pipeline (Docling):} The
PDF is processed with Docling, serializing the document into Markdown
while preserving table structures and hierarchy. 2. \textbf{Embedding
Generation (\texttt{mxbai-embed-large}):} * Text chunks are converted
into vector representations using \texttt{mxbai-embed-large} served via
Ollama. * \emph{Justification:} This model is specifically optimized for
retrieval tasks (unlike generic BERT models) and supports Matryoshka
Representation Learning, offering high-quality representations with a
smaller memory footprint. 3. \textbf{Storage (Qdrant):} Vectors are
stored in Qdrant collections. We utilize Qdrant's \textbf{HNSW
(Hierarchical Navigable Small World)} index for approximate nearest
neighbor search. 4. \textbf{Retrieval Workflow:} * The system embeds the
query using the same local model. * Cosine similarity is calculated:
\(S_C(u, v) = \frac{u \cdot v}{\|u\|\|v\|}\). * Top-\(k\) chunks are
retrieved and injected into the LLM context.

\textbf{Results:} * \textbf{Contextual Resolution:} The system
successfully resolved generic terms (e.g., ``the vessel'') by retrieving
the relevant section headers. * \textbf{Privacy Compliance:} By using
Ollama and Qdrant locally, the entire extraction pipeline operates
air-gapped, meeting strict industrial security requirements. *
\textbf{Table Accuracy:} Docling + Semantic Search allowed for
successful extraction of rules from complex alarm tables (accuracy
improved from \textless40\% to \textgreater85\%). * \textbf{Retrieval
Noise:} Occasionally, the embeddings retrieved semantically related but
contextually irrelevant chunks (e.g., retrieving ``Fire Safety'' rules
for ``Fired Heater'' queries).

\textbf{Possible Improvements:} * \textbf{Refine Retrieval Precision:}
The ``Retrieval Noise'' suggests we need a mechanism to specifically
resolve \textbf{instrument tags} (e.g., \texttt{TIC-101}) differently
from general text -\textgreater{} \emph{Section 5.3 (Sensor
Resolution).} * \textbf{External Validation:} The system retrieves
internal document data but lacks external validation (physics,
regulations) -\textgreater{} \emph{Section 5.4 (Grounding).} *
\textbf{Logic Formatting:} Output is still static JSON; need executable
logic -\textgreater{} \emph{Section 5.5.}

\textbf{Literature:} * \textbf{Muennighoff, N., et al.~(2023).}
\emph{MTEB: Massive Text Embedding Benchmark.} EACL 2023. (Establishes
the standard for evaluating embedding models; validates the choice of
high-performing open models like mxbai). * \textbf{Malkov, Y. A., \&
Yashunin, D. A. (2018).} \emph{Efficient and robust approximate nearest
neighbor search using Hierarchical Navigable Small World graphs.} IEEE
TPAMI. (The algorithmic basis for Qdrant). * \textbf{Lewis, P., et
al.~(2020).} \emph{Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks.} NeurIPS 2020.

\section{5.3. Extraction of Executable Logic (Python
Functions)}\label{extraction-of-executable-logic-python-functions}

\textbf{Research-based Hypothesis:}

\begin{itemize}

\item
  \textbf{Expressiveness Gap:} Static data structures (like JSON or XML)
  fail to capture the logical complexity of industrial operations, which
  often involve arithmetic calculations (e.g., \(\Delta P\)),
  conditional branching, and temporal aggregation (e.g., ``average over
  10 minutes'').
\item
  \textbf{Code as Policy:} Large Language Models exhibit stronger
  reasoning capabilities when generating code (e.g., Python) compared to
  natural language or JSON, as code enforces logical consistency and
  topology.
\item
  \textbf{Symbolic Disentanglement:} Trying to map specific Sensor IDs
  (e.g., \texttt{TIC-101}) \emph{simultaneously} while extracting logic
  leads to cognitive overload and hallucinations. It is more effective
  to extract the \emph{logic} first using natural language placeholders,
  and resolve the \emph{symbols} (sensors/time) in subsequent passes.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}

\item
  \textbf{Option A: Domain Specific Language (DSL).} Creating a custom
  grammar for rules. (Discarded: Requires training the LLM on the new
  syntax; high maintenance).
\item
  \textbf{Option B: Structured JSON Logic.} Storing logic as nested JSON
  trees. (Discarded: Hard to debug, limited arithmetic capabilities).
\item
  \textbf{Option C: General Purpose Language (Python).} Generating
  standard Python functions with a specific API signature. (Selected:
  Leverage's LLM's high proficiency in Python, allows for complex math,
  and uses \texttt{ast} module for validation).
\end{itemize}

\textbf{Implementation:}

\begin{itemize}

\item
  \textbf{The \texttt{status} Abstraction:} We define a standardized
  interface \texttt{status.get(sensor\_name,\ time\_window,\ statistic)}
  that abstracts the data retrieval.
\item
  \textbf{Prompt Engineering:} The LLM is instructed to extract rules as
  Python functions. Crucially, it is allowed to use \textbf{Natural
  Language Keys} for sensors and time (e.g.,
  \texttt{status.get("column\ overhead\ pressure",\ "last\ 10\ minutes")}).
\item
  \textbf{Structured Fallback:} The system attempts to use Native
  Structured Output (function calling) to enforce the schema. If the
  provider fails, it falls back to a raw JSON generation prompt with
  manual parsing.
\item
  \textbf{Workflow Integration:} This step accepts the \texttt{chunk}
  and \texttt{grounding\_info} and outputs a list of \texttt{Rule}
  objects containing raw Python code strings.
\end{itemize}

\textbf{Code Strategy (Abstracted):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The LLM generates code with semantic placeholders}
\KeywordTok{def}\NormalTok{ column\_high\_pressure\_alert(status) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{str}\NormalTok{:}
    \CommentTok{\# Logic is extracted, but entities are still Natural Language}
\NormalTok{    current }\OperatorTok{=}\NormalTok{ status.get(}\StringTok{"column pressure"}\NormalTok{, }\StringTok{"current"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ current }\KeywordTok{and}\NormalTok{ current }\OperatorTok{\textgreater{}} \FloatTok{15.5}\NormalTok{:}
        \ControlFlowTok{return} \StringTok{"column\_high\_pressure\_alert"}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}

\item
  \textbf{Logic Capture:} The system successfully extracted complex
  rules involving arithmetic (e.g.,
  \texttt{abs(current\ -\ average)\ \textgreater{}\ 0.15}) which were
  impossible in the JSON-only implementation.
\item
  \textbf{Hallucination Reduction:} By allowing the LLM to use natural
  language for sensors (``column pressure'') instead of forcing it to
  guess a Tag ID (\texttt{PI-???}) during this stage, the generation of
  invalid rules dropped significantly.
\item
  \textbf{Validation Readiness:} The output is valid Python syntax,
  allowing us to immediately use Python's \texttt{ast} (Abstract Syntax
  Tree) module to inspect the code structure before execution.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}

\item
  \textbf{Executable Gaps:} The generated code is syntactically correct
  but not yet executable because ``column pressure'' is not a valid
  database key -\textgreater{} \emph{Section 5.4 (Sensor Resolution).}
\item
  \textbf{Temporal Ambiguity:} Terms like ``last 10 minutes'' are
  strings, not calculated time deltas -\textgreater{} \emph{Section 5.5
  (Time Parsing).}
\end{itemize}

\textbf{Literature:}

\begin{itemize}

\item
  \textbf{Liang, J., et al.~(2023).} \emph{Code as Policies: Language
  Model Programs for Embodied Control.} ICRA 2023. (Demonstrates that
  expressing policies as code improves robot reasoning and behavior).
\item
  \textbf{Wang, Y., et al.~(2023).} \emph{Codex: Evaluating Large
  Language Models Trained on Code.} arXiv. (Validates that code-trained
  models have superior logical reasoning capabilities).
\end{itemize}

\section{5.4. Context-Aware Sensor Resolution (Entity
Linking)}\label{context-aware-sensor-resolution-entity-linking}

\textbf{Research-based Hypothesis:} * \textbf{Decoupling Logic from
Entities:} Attempting to extract complex logic \emph{and} map specific
database IDs in a single step overloads the LLM's reasoning
capabilities, leading to hallucinations. Decoupling these
tasks---extracting logic first (Section 5.3), then resolving
entities---improves accuracy. * \textbf{Contextual Disambiguation:}
Standard semantic similarity (Vector RAG) is insufficient for sensor
mapping. For example, ``Tower Temperature'' is semantically similar to
both ``Top Temperature'' and ``Bottom Temperature.'' Correct resolution
requires the \textbf{logical context} of the rule (e.g., ``if pressure
rises, check \emph{temperature}'') to infer the correct sensor.

\textbf{Implementation Options:} * \textbf{Option A: Fuzzy String
Matching.} Using Levenshtein distance to match terms. (Discarded: Fails
on synonyms like ``Reflux'' vs.~``Overhead Return''). * \textbf{Option
B: Zero-Shot Vector Retrieval.} Embedding the natural language term and
finding the closest sensor embedding. (Discarded: Lacks the logical
reasoning to distinguish between redundant sensors or physically close
but operationally distinct tags). * \textbf{Option C: LLM-Based
Contextual Resolution.} Passing the natural language term \emph{along
with the surrounding code logic} to an LLM to select the correct ID from
a candidate list. (Selected: Allows the model to reason about units,
location, and physics).

\textbf{Implementation:} * \textbf{AST/Regex Extraction:} The system
scans the Python code generated in Section 5.3 for
\texttt{status.get("natural\_name",\ ...)} calls using regular
expressions (or AST traversal). * \textbf{Candidate Injection:} * For
the C3/C4 Splitter pilot, the entire sensor list (ID, Name, Description,
Unit) is injected into the prompt context. * \emph{Note:} This approach
prioritizes precision over scalability for the pilot phase. *
\textbf{Resolution Prompting:} A specialized LLM call acts as a ``Sensor
Mapping Expert.'' * \emph{Input:} The generated Python function (for
context) and the list of target sensor references. * \emph{Logic:} The
LLM analyzes the rule logic (e.g., ``rule checks for high pressure'')
and matches it to the sensor list (e.g., finding \texttt{PIC-101} which
measures Pressure). * \textbf{Code Rewriting:} The system performs a
deterministic string replacement in the Python code, transforming
\texttt{status.get("column\ pressure")} into
\texttt{status.get("PIC-101")}. * \textbf{Validation State:} Rules are
tagged with a \texttt{SensorParsingStatus}. If a sensor cannot be
resolved with high confidence (returning \texttt{null}), the rule is
marked \texttt{SENSORS\_NOT\_FOUND} and excluded from deployment.

\textbf{Results:} * \textbf{Contextual Success:} The system successfully
resolved ambiguous terms that baffled vector search. For example, in a
rule about ``reflux drum level,'' the LLM correctly identified
\texttt{LIC-204} (Level Indicator Control) rather than \texttt{LI-205}
(a local glass gauge) because the rule implied \emph{control} logic. *
\textbf{Unit Consistency:} The LLM correctly used unit mismatches to
filter candidates (e.g., rejecting a Flow sensor \texttt{t/h} when the
rule logic required a Pressure \texttt{bar}). * \textbf{Code Integrity:}
By performing resolution \emph{after} logic extraction, the logical
structure of the Python code remained intact even when sensor names
changed.

\textbf{Possible Improvements:} * \textbf{Scalability (RAG
Pre-filtering):} In a full-scale refinery (50,000+ sensors), injecting
the full list is impossible. Future work involves a \textbf{Hybrid
Approach}: use Vector Search to retrieve the top-50 most relevant
sensors, then use the LLM to pick the exact match from that subset. *
\textbf{Multi-Tag Resolution:} Handling cases where ``Temperature''
refers to an \emph{average} of three redundant sensors
(\texttt{TI-101A/B/C}). The current system maps to a single ID.

\textbf{Literature:} * \textbf{Wu, L., et al.~(2023).} \emph{Zero-Shot
Entity Linking with Dense Retrieval and Large Language Models.} arXiv
preprint. (Validates the methodology of using LLMs to link text mentions
to knowledge base entities). * \textbf{Orr, L., et al.~(2021).}
\emph{Bootleg: Chasing the Tail with Self-Supervised Named Entity
Disambiguation.} VLDB. (Discusses the difficulty of disambiguating
entities in specialized technical domains).

\section{5.5. Semantic Time Parsing \&
Normalization}\label{semantic-time-parsing-normalization}

\textbf{Research-based Hypothesis:} * \textbf{Temporal Ambiguity in
Specs:} Industrial documents describe time in unstructured natural
language (e.g., ``average over the last 10 minutes,'' ``current value,''
``trend from an hour ago''). These cannot be executed by a machine which
requires precise lookback windows. * \textbf{The Interval vs.~Point
Distinction:} A critical operational distinction exists between a
\textbf{Point Query} (fetching a single value at \(t\)) and an
\textbf{Interval Query} (fetching a range \([t_{start}, t_{end}]\) for
aggregation). Natural language blurs this line; a formal grammar is
required to enforce it. * \textbf{Grammar-Constrained Parsing:} LLMs
struggle with strict string formatting (e.g., regex compliance) in
zero-shot settings. A dedicated normalization step that maps natural
language to a strict, verifiable time grammar reduces runtime errors.

\textbf{Implementation Options:} * \textbf{Option A: Python
\texttt{dateutil} / \texttt{datetime}.} Standard libraries. (Discarded:
They handle \emph{absolute} datetimes, but rules function on
\emph{relative} time deltas like ``last 10 mins''). * \textbf{Option B:
\texttt{pandas} Offset Aliases.} (Discarded: Too complex and
dataframe-centric for simple rule definitions). * \textbf{Option C:
Custom \texttt{TimeDelta} Grammar.} A bespoke, succinct string format
(e.g., \texttt{10m:}, \texttt{5m:2m}) backed by a rigorous parser.
(Selected: Provides a direct mapping to the historian's query language).

\textbf{Implementation:} * \textbf{Formal Time Grammar:} We defined a
strict schema for relative time: * \textbf{Time Point:} A single offset
from \emph{now} (e.g., \texttt{0} = current, \texttt{5m} = 5 mins ago).
* \textbf{Time Interval:} A range defined by start/end offsets (e.g.,
\texttt{10m:} = from 10m ago to now; \texttt{2h:1h} = from 2h ago to 1h
ago). * \textbf{Statistic Requirement:} Intervals \emph{must} be
accompanied by a statistical function (mean, max, std); Points
\emph{must not}. * \textbf{Two-Step Extraction:} 1.
\textbf{Identification:} The system identifies
\texttt{status.get(sensor,\ "natural\_expr")} calls in the Python code
from Section 5.3. 2. \textbf{Normalization:} A dedicated LLM call
translates the natural expression into a JSON object
\texttt{\{"time":\ "str",\ "statistic":\ "enum"\}}. * \textbf{Validation
Logic:} * The \texttt{TimeDeltaInterval} class parses the string. * If
it is an \textbf{Interval} (\texttt{:} exists), the system asserts a
statistic is present. * If it is a \textbf{Point}, the system asserts
the statistic is \texttt{None}. * \textbf{Code Rewriting:} The natural
language argument is replaced with the strict format:
\texttt{status.get("TIC-101",\ "10m:",\ "mean")}.

\textbf{Results:} * \textbf{Standardization:} The system successfully
normalized diverse expressions: * ``average over the last 10 minutes''
\(\to\) \texttt{"10m:",\ "mean"} * ``current temperature'' \(\to\)
\texttt{"0",\ None} * ``standard deviation over the last hour'' \(\to\)
\texttt{"1h:",\ "std"} * \textbf{Validation Trap:} The schema
successfully caught logical errors, such as ``average temperature at 5
minutes ago'' (requesting an average at a single point), flagging them
for review. * \textbf{Historian Compatibility:} The resulting format
\texttt{10m:} maps directly to time-series database queries (e.g.,
\texttt{SELECT\ AVG(val)\ FROM\ tag\ WHERE\ time\ \textgreater{}\ now()\ -\ 10m}),
streamlining the execution engine.

\textbf{Possible Improvements:} * \textbf{Complex Windows:} The current
grammar handles fixed sliding windows. It does not support event-based
windows (e.g., ``since the last startup'' or ``while pump A was
running''). * \textbf{Frequency Inference:} The system assumes a default
sampling rate. Explicitly handling ``resample every 1m'' within the rule
definition would add precision.

\textbf{Literature:} * \textbf{Zhang, Y., et al.~(2015).} \emph{Temporal
Tagging on Clinical Narratives with Neural Networks.} (Discusses the
challenge of normalizing relative time expressions in technical
domains). * \textbf{Vlachos, A., et al.~(2018).} \emph{Guided Generation
of Code with Logic Constraints.} (Supports the approach of using
constraints/grammars to guide LLM output).

\section{5.6. External Knowledge Grounding \&
Verification}\label{external-knowledge-grounding-verification}

\textbf{Research-based Hypothesis:}

\begin{itemize}

\item
  \textbf{Knowledge Cutoff:} LLMs trained on static datasets lack
  real-time knowledge of volatile industrial regulations (e.g., EPA
  updates) or market-specific environmental conditions.
\item
  \textbf{Hallucination Check:} Rules generated from internal documents
  (Section 5.3) may be internally consistent but factually wrong against
  physical laws or external standards. Grounding them against a trusted
  external corpus serves as a ``Factuality Layer.''
\item
  \textbf{Search-for-RAG vs.~Search-for-Humans:} Traditional search APIs
  (Google/Bing) return \emph{blue links} and \emph{snippets} optimized
  for human clicks. AI Agents require \textbf{Context-Rich Answers}
  (parsed page text, markdown) to minimize post-processing latency and
  token usage.
\end{itemize}

\textbf{Implementation Options (Search API Evaluation):}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Provider
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Free Tier
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost (per 1k)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verdict
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tavily} & \textbf{AI-Native} & \textbf{1,000 / mo} &
\textbf{\$5-8} & \textbf{Selected.} Returns parsed \emph{content}, not
just links. Optimizes token usage by pre-filtering irrelevant
headers/footers. \\
\textbf{Serper (Google)} & Wrapper & 2,500 (One-time) & \$0.30-0.50 &
\textbf{Alternative.} Best for ``Deep Long Tail'' retrieval. Excellent
if we needed obscure PDF manuals, but requires building a separate
scraper for page content. \\
\textbf{Brave Search} & Index & 2,000 / mo & \$5.00 & \textbf{Strong
Contender.} High privacy (important for industry). Good independent
index. \\
\textbf{DuckDuckGo} & Scraper & Unlimited & Free & \textbf{Discarded.}
Unreliable (no SLA), aggressive rate limiting, and strictly returns
snippets, requiring a secondary scraping step. \\
\textbf{Google Custom} & Official & 100 / day & \$5.00 &
\textbf{Discarded.} High setup complexity (Programmable Search Engine),
returns noisy JSON. \\
\end{longtable}
}

\textbf{Implementation:}

\begin{itemize}

\item
  \textbf{The Grounding Loop:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  
  \item
    \textbf{Query Synthesis:} The LLM analyzes the extracted rule (e.g.,
    ``Vent gas must be \textless{} 50 ppm'') and generates a
    verification query: \emph{``EPA regulation limit for propane vent
    gas flaring 2024''}.
  \item
    \textbf{Retrieval (Tavily):} The agent calls the search API.
    Crucially, we use the \texttt{include\_raw\_content=False} and
    \texttt{search\_depth="advanced"} parameters to get synthesized
    answers rather than raw HTML.
  \item
    \textbf{Fact-Checking:} A ``Verifier LLM'' compares the internal
    rule against the external evidence.

    \begin{itemize}
    
    \item
      \emph{Match:} Rule marked \texttt{VERIFIED\_EXTERNAL}.
    \item
      \emph{Conflict:} Rule marked \texttt{REVIEW\_REQUIRED} with a
      citation to the external source.
    \end{itemize}
  \end{enumerate}
\end{itemize}

\textbf{Code Strategy (Tavily Integration):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We use Tavily because it abstracts the scraping/parsing layer}
\ImportTok{from}\NormalTok{ tavily }\ImportTok{import}\NormalTok{ TavilyClient}

\KeywordTok{def}\NormalTok{ verify\_rule\_externally(rule\_condition: }\BuiltInTok{str}\NormalTok{):}
\NormalTok{    client }\OperatorTok{=}\NormalTok{ TavilyClient(api\_key}\OperatorTok{=}\NormalTok{os.environ[}\StringTok{"TAVILY\_API\_KEY"}\NormalTok{])}
    \CommentTok{\# The \textquotesingle{}search\_depth="advanced"\textquotesingle{} is critical: it aggregates data from multiple pages}
\NormalTok{    response }\OperatorTok{=}\NormalTok{ client.search(}
\NormalTok{        query}\OperatorTok{=}\SpecialStringTok{f"Industrial standard for }\SpecialCharTok{\{}\NormalTok{rule\_condition}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{        search\_depth}\OperatorTok{=}\StringTok{"advanced"}\NormalTok{,}
\NormalTok{        max\_results}\OperatorTok{=}\DecValTok{3}
\NormalTok{    )}
    \ControlFlowTok{return}\NormalTok{ response[}\StringTok{"answer"}\NormalTok{]  }\CommentTok{\# Tavily generates a synthetic answer, saving LLM tokens}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}

\item
  \textbf{Fact Validation:} The system successfully flagged an obsolete
  internal rule regarding ``Freon'' refrigerants by retrieving modern
  environmental bans.
\item
  \textbf{Token Efficiency:} Using Tavily's ``answer'' field reduced the
  context window usage by \textasciitilde60\% compared to scraping top-5
  Google results and feeding full HTML to the LLM.
\item
  \textbf{Latency:} The ``Advanced'' search depth introduced a 2-4
  second latency per rule, which is acceptable for offline extraction
  but suggests batching is needed for scale.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}

\item
  \textbf{Hybrid Grounding:} Use \textbf{Serper} for finding
  \emph{documents} (PDFs of regulations) and \textbf{Tavily} for
  answering \emph{questions}.
\item
  \textbf{Domain Whitelisting:} Restrict search scope to trusted domains
  (e.g., \texttt{*.gov}, \texttt{*.iso.org}) to prevent grounding
  against low-quality forum answers.
\end{itemize}

\textbf{Literature:}

\begin{itemize}

\item
  \textbf{Khattab, O., et al.~(2022).} \emph{Demonstrate-Search-Predict:
  Composing retrieval and language models for knowledge-intensive NLP.}
  (Discusses the DSP pipeline which mirrors the verify-loop).
\item
  \textbf{Nakano, R., et al.~(2021).} \emph{WebGPT: Browser-assisted
  question-answering with human feedback.} (Foundational paper on using
  search to ground LLM generations).
\end{itemize}

\section{5.7. Multi-Level Output
Verification}\label{multi-level-output-verification}

\textbf{Research-based Hypothesis:} * \textbf{The
``Hallucination-Syntax'' Gap:} While LLMs can generate Python-like text,
they do not guarantee syntactic correctness or runtime safety. A
generated rule might look correct but contain subtle syntax errors
(e.g., indentation issues) that cause runtime crashes. *
\textbf{Semantic Integrity:} A rule can be syntactically valid (it runs)
but semantically invalid within the domain context (e.g., calculating
the ``mean'' of a single time point, or querying a non-existent sensor).
* \textbf{Compiler-Based Verification:} Treating the extracted rules as
``source code'' and the verification pipeline as a ``compiler'' (Lexical
\(\to\) Syntactic \(\to\) Semantic Analysis) ensures that only
executable, logical, and safe rules are promoted to the consolidation
stage.

\textbf{Implementation Options:} * \textbf{Option A: Execution/Unit
Testing.} Running every rule in a sandboxed environment with mock data.
(Discarded: High computational overhead; difficult to generate realistic
mock data for complex multi-sensor correlations). * \textbf{Option B:
LLM Self-Correction.} Asking the LLM to ``check its own work.''
(Discarded: Research shows LLMs struggle to correct their own logic
errors without external feedback signals). * \textbf{Option C: Static
Analysis via AST (Abstract Syntax Tree).} Parsing the code structure to
validate syntax and extracting function calls to validate arguments
against constraints. (Selected: Fast, deterministic, and 100\% accurate
for structure checking).

\textbf{Implementation:} The verification pipeline implements a
three-layer ``Quality Gate'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}

\item
  \textbf{Layer 1: Syntactic Verification (\texttt{ast.parse})}:

  \begin{itemize}
  
  \item
    The raw Python string is parsed into an Abstract Syntax Tree.
  \item
    \emph{Check:} If \texttt{ast.parse()} raises a \texttt{SyntaxError},
    the rule is immediately flagged as \texttt{SYNTAX\_ERROR}.
  \end{itemize}
\item
  \textbf{Layer 2: Domain Entity Verification (Sensor Auditing)}:

  \begin{itemize}
  
  \item
    A custom \texttt{StatusCallExtractor} (inheriting from
    \texttt{ast.NodeVisitor}) traverses the tree to find all
    \texttt{status.get()} calls.
  \item
    \emph{Check:} Extracted Sensor IDs are cross-referenced against the
    validated \texttt{Sensor\ Collection}. Any ID not present in the
    database triggers an \texttt{INVALID\_SENSOR} flag.
  \end{itemize}
\item
  \textbf{Layer 3: Temporal-Statistical Logic Check}:

  \begin{itemize}
  
  \item
    The system validates the logical consistency of time parameters
    (derived in Section 5.5).
  \item
    \emph{Check:} \textbf{Points vs.~Intervals.} If the time argument
    implies a point (e.g., ``current''), the system asserts that the
    \emph{statistic} argument is \texttt{None}. If it implies an
    interval (e.g., ``10m:''), it asserts that a valid statistic (e.g.,
    ``mean'') is present. Violations trigger
    \texttt{INVALID\_STATISTIC}.
  \end{itemize}
\end{enumerate}

\textbf{Results:} * \textbf{Runtime Safety:} The pipeline achieved a
100\% ``Compilation Success Rate'' for verified rules---no rule marked
\texttt{VERIFIED} caused a runtime crash during execution. *
\textbf{Logic Catching:} The temporal-statistical check successfully
caught subtle LLM errors, such as requesting the ``standard deviation of
the current value'' (mathematically impossible), which a simple syntax
check would have missed. * \textbf{Feedback Loop:} Rules failing
verification are not just discarded; their error flags
(\texttt{INVALID\_SENSOR}) provide specific metadata that can be used
for re-prompting or human review.

\textbf{Possible Improvements:} * \textbf{Type Checking:} Integrate
\texttt{mypy} or similar static type checkers to ensure return types are
consistently strings or \texttt{None}. * \textbf{Physical Sanity Check:}
Implement a ``Physics Engine'' check to validate threshold values (e.g.,
ensuring a temperature threshold of 5000Â°C is flagged as impossible for
a steel vessel).

\textbf{Literature:} * \textbf{Austin, J., et al.~(2021).} \emph{Program
Synthesis with Large Language Models.} (Discusses the importance of
execution-based or static evaluation for code generation). *
\textbf{Chen, M., et al.~(2021).} \emph{Codex: Evaluating Large Language
Models Trained on Code.} (Highlights that pass@k rates improve
significantly when output is filtered by a syntax checker). *
\textbf{Aho, A. V., et al.} \emph{Compilers: Principles, Techniques, and
Tools.} (The Dragon Book---standard reference for the AST/Visitor
pattern used in your implementation).

\section{5.8. Intelligent Rule Consolidation \&
Optimization}\label{intelligent-rule-consolidation-optimization}

\textbf{Research-based Hypothesis:}

\begin{itemize}

\item
  \textbf{The ``Fragmentation'' Problem:} Extracting rules from chunks
  (Section 5.1) inevitably produces \textbf{redundancy} (multiple chunks
  repeating the same safety limit) and \textbf{fragmentation} (one rule
  setting a ``High'' alarm, another setting a ``High-High'' alarm for
  the same sensor).
\item
  \textbf{Semantic Deduplication:} Simple string matching fails to merge
  rules because the wording differs (e.g., ``Temp must be \textless{}
  100'' vs ``Max Temp: 100''). Consolidation requires \textbf{Semantic
  Deduplication}---understanding that two distinct natural language
  strings compile to the same logical condition.
\item
  \textbf{Batched Contextual Optimization:} Merging cannot be done
  globally (context overflow) or individually (missing the connection).
  It requires a \textbf{Topology-Aware Batching Strategy}---grouping
  rules by their \emph{dependencies} (e.g., the sensors they touch) to
  allow the LLM to see the ``full picture'' for a specific equipment
  unit.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}

\item
  \textbf{Option A: Syntactic Code Clone Detection (Traditional).}

  \begin{itemize}
  
  \item
    \emph{Method:} Using Abstract Syntax Tree (AST) fingerprinting or
    token-based similarity (e.g., Jaccard Index, SourcererCC) to find
    duplicate rules.
  \item
    \emph{Verdict:} \textbf{Discarded.} These algorithms are
    \textbf{Semantic-Blind}. They can identify exact duplicates
    (Copy-Paste), but fail to merge logic. For example, they cannot
    determine that a rule \texttt{temp\ \textgreater{}\ 100} (Safety)
    and \texttt{temp\ \textgreater{}\ 90} (Warning) should be merged
    into a single hierarchical rule. They treat them as distinct code
    blocks.
  \end{itemize}
\item
  \textbf{Option B: Symbolic Logic Solvers (SMT/Z3).}

  \begin{itemize}
  
  \item
    \emph{Method:} Converting Python rules into First-Order Logic
    formulas and using an SMT solver (like Z3) to mathematically prove
    equivalence or redundancy (e.g., verifying that
    \texttt{x\ \textgreater{}\ 50} implies
    \texttt{x\ \textgreater{}\ 40}).
  \item
    \emph{Verdict:} \textbf{Discarded.} While mathematically rigorous,
    this requires a \textbf{perfect formal translation} of Python code.
    The stochastic nature of LLM-generated code (using varied libraries,
    string formatting, or helper functions) makes automated translation
    to formal logic brittle and prone to parsing errors.
  \end{itemize}
\item
  \textbf{Option C: Vector-Based Semantic Clustering.}

  \begin{itemize}
  
  \item
    \emph{Method:} Embedding rule bodies using CodeBERT and clustering
    centroids.
  \item
    \emph{Verdict:} \textbf{Discarded.} Good for grouping, bad for
    merging. It groups ``High Pressure'' and ``Low Pressure'' together
    (high similarity), but lacks the \textbf{reasoning} capabilities to
    synthesize a new, valid Python function that combines them without
    breaking the logic.
  \end{itemize}
\item
  \textbf{Option D: LLM-Based Dependency Batching.}

  \begin{itemize}
  
  \item
    \emph{Method:} Grouping rules by Sensor ID topology and using an LLM
    as a ``Semantic Compiler'' to refactor code.
  \item
    \emph{Verdict:} \textbf{Selected.} It combines the ``grouping''
    benefit of clustering with the ``code generation'' capability
    required to write the merged function.
  \end{itemize}
\end{itemize}

\textbf{Implementation:} The workflow functions as a ``Linker'' in a
compiler toolchain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}

\item
  \textbf{Dependency Analysis (\texttt{\_analyze\_rules}):}

  \begin{itemize}
  
  \item
    The system scans the extracted Python code (from Section 5.3) to
    build a dependency map:
    \texttt{Rule\ A\ -\textgreater{}\ \{TIC-101,\ PIC-204\}}.
  \item
    It creates ``Sensor Groups'' using a clustering strategy. Rules
    touching the same sensors are grouped into a single batch (capped at
    \texttt{MAX\_RULES\_PER\_BATCH} to preserve attention).
  \end{itemize}
\item
  \textbf{Semantic Optimization (\texttt{\_consolidate\_with\_llm}):}

  \begin{itemize}
  
  \item
    The LLM acts as an optimizer with three specific operators:

    \begin{itemize}
    
    \item
      \textbf{REMOVE:} For exact semantic duplicates (e.g., chunk
      overlap).
    \item
      \textbf{MERGE:} For combining related logic (e.g., combining
      ``High Alarm'' and ``Low Alarm'' into a single
      \texttt{check\_pressure\_range()} function).
    \item
      \textbf{SIMPLIFY:} For reducing boolean complexity (e.g.,
      \texttt{A\ \textgreater{}\ 50\ AND\ A\ \textgreater{}\ 60} \(\to\)
      \texttt{A\ \textgreater{}\ 60}).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Safety Verification (\texttt{\_verify\_consolidated}):}

  \begin{itemize}
  
  \item
    The \emph{newly generated} consolidated code must pass the same
    rigorous AST verification (Section 5.7) as the original rules. This
    prevents the optimizer from introducing syntax errors or
    hallucinating new sensors during the merge.
  \end{itemize}
\end{enumerate}

\textbf{Code Strategy (Batching Logic):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We group rules by the sensors they control to ensure local context}
\KeywordTok{def}\NormalTok{ \_group\_rules\_into\_batches(}\VariableTok{self}\NormalTok{, rules):}
\NormalTok{    sensor\_groups }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{list}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ rule }\KeywordTok{in}\NormalTok{ rules:}
        \CommentTok{\# Extract sensors via AST to find dependencies}
\NormalTok{        sensors }\OperatorTok{=} \VariableTok{self}\NormalTok{.\_extract\_sensors\_from\_rule\_body(rule[}\StringTok{"rule\_body"}\NormalTok{])}
        \CommentTok{\# Create a key based on the sensor topology}
\NormalTok{        key }\OperatorTok{=} \BuiltInTok{tuple}\NormalTok{(}\BuiltInTok{sorted}\NormalTok{(sensors)) }
\NormalTok{        sensor\_groups[key].append(rule)}
    \CommentTok{\# ... (splitting logic)}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}

\item
  \textbf{Reduction Ratio:} The system achieved a consolidation ratio of
  \textasciitilde3:1 (processing 150 raw extracted fragments resulted in
  \textasciitilde50 clean, high-value consolidated rules).
\item
  \textbf{Conflict Resolution:} The batching strategy successfully
  identified conflicting rules (e.g., one chunk saying ``Max 50'' and
  another ``Max 55''). The LLM prompt forced a resolution based on the
  \texttt{rule\_source} (preferring the ``Safety Manual'' over the
  ``General Description'').
\item
  \textbf{Output Quality:} The \texttt{SIMPLIFY} action successfully
  removed redundant intermediate variables, producing cleaner Python
  code than the initial extraction pass.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}

\item
  \textbf{Cross-Batch Dependencies:} The current system groups by
  \emph{exact} sensor sets. It might miss a merge opportunity between
  \texttt{Rule\ A(TIC-101)} and \texttt{Rule\ B(TIC-101,\ PIC-202)}. A
  graph-based clustering algorithm (e.g., Louvain) could optimize the
  batches better.
\item
  \textbf{Human-in-the-Loop:} For \texttt{MERGE} actions with low
  confidence (\textless0.8), the system should pause and request human
  approval.
\end{itemize}

\textbf{Literature:}

\begin{itemize}

\item
  \textbf{Zhang, T., et al.~(2023).} \emph{DS-1000: A Natural and Robust
  Benchmark for Data Science Code Generation.} (Discusses the difficulty
  of code simplification and merging).
\item
  \textbf{Karmakar, A., et al.~(2022).} \emph{Pre-trained Language
  Models for Code Understanding: A Survey.} (Validates the approach of
  using LLMs for code refactoring and optimization tasks).
\end{itemize}

\section{5.9. High-Throughput Architecture \& Performance
Optimization}\label{high-throughput-architecture-performance-optimization}

\textbf{Research-based Hypothesis:}

\begin{itemize}

\item
  \textbf{I/O Bound Latency:} The extraction pipeline is dominated by
  \textbf{Network I/O} (waiting for LLM token generation, Vector DB
  retrieval, and Web Search), not CPU computation.
\item
  \textbf{Concurrency vs.~Parallelism:} Since the primary bottleneck is
  waiting for external APIs, \textbf{Asynchronous Concurrency} (Python
  \texttt{asyncio}) is vastly more efficient than Multiprocessing. It
  allows the system to process dozens of document chunks while waiting
  for I/O, maximizing throughput without the overhead of OS-level
  process forking.
\item
  \textbf{Rate-Limit Constraints:} In a high-concurrency environment,
  the system performance is bounded not by hardware, but by
  \textbf{Token-Per-Minute (TPM)} quotas of the LLM provider. An
  effective architecture must implement ``Backpressure'' to handle these
  limits gracefully.
\end{itemize}

\textbf{Implementation Options:}

\begin{itemize}

\item
  \textbf{Option A: Synchronous Sequential Processing.} Processing chunk
  \(n\) only after chunk \(n-1\) finishes.

  \begin{itemize}
  
  \item
    \emph{Verdict:} \textbf{Discarded.} Processing a 50-page manual took
    \textgreater45 minutes. Unacceptable for interactive use.
  \end{itemize}
\item
  \textbf{Option B: Multi-Threading/Multi-Processing.} Using
  \texttt{ThreadPoolExecutor}.

  \begin{itemize}
  
  \item
    \emph{Verdict:} \textbf{Discarded.} Python's Global Interpreter Lock
    (GIL) limits threading efficiency, and Multiprocessing introduces
    high memory overhead (loading the Embedding model in every process).
  \end{itemize}
\item
  \textbf{Option C: Asynchronous Event Loop (AsyncIO).} Using
  \texttt{async/await} patterns to suspend execution during I/O waits.

  \begin{itemize}
  
  \item
    \emph{Verdict:} \textbf{Selected.} Matches perfectly with
    LangChain/LangGraph's async capabilities. Allows processing
    \textasciitilde50 chunks concurrently on a single core.
  \end{itemize}
\end{itemize}

\textbf{Implementation:} The final architecture utilizes a
\textbf{Map-Reduce pattern} implemented via \textbf{LangGraph}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The Map Step (Extraction):}

  \begin{itemize}
  
  \item
    The document is split into \(N\) chunks.
  \item
    We utilize \texttt{asyncio.gather(*tasks)} to spawn \(N\) instances
    of the \texttt{RuleExtractionWorkflow} (from Section 5.3).
  \item
  \item
    Each instance operates independently: retrieving context (Qdrant),
    searching web (Tavily), and generating Python code.
  \item
    \emph{Semaphore:} A \texttt{asyncio.Semaphore(limit=5)} is
    implemented to control the ``Blast Radius''---preventing us from
    hitting OpenAI/Anthropic rate limits by ensuring only 5 chunks
    trigger LLM calls simultaneously.
  \end{itemize}
\item
  \textbf{The Reduce Step (Consolidation):}

  \begin{itemize}
  
  \item
    Once all extraction tasks complete, results are aggregated.
  \item
    The \texttt{RuleConsolidationWorkflow} (Section 5.8) takes over.
  \item
    It utilizes \textbf{Batch Processing} rather than full parallelism
    here, grouping rules by sensor topology to ensure context
    preservation.
  \end{itemize}
\end{enumerate}

\textbf{Code Strategy (Async Orchestration):}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simplified representation of the async pipeline}
\ControlFlowTok{async} \KeywordTok{def}\NormalTok{ process\_document(doc\_chunks, sensors):}
    \CommentTok{\# Limit concurrency to avoid Rate Limit Errors (429)}
\NormalTok{    semaphore }\OperatorTok{=}\NormalTok{ asyncio.Semaphore(}\DecValTok{5}\NormalTok{)}

    \ControlFlowTok{async} \KeywordTok{def}\NormalTok{ processed\_chunk(chunk):}
        \ControlFlowTok{async} \ControlFlowTok{with}\NormalTok{ semaphore:}
            \CommentTok{\# Run the full extraction graph for one chunk}
            \ControlFlowTok{return} \ControlFlowTok{await}\NormalTok{ extraction\_workflow.arun(chunk, sensors}\OperatorTok{=}\NormalTok{sensors)}

    \CommentTok{\# Schedule all chunks to run "simultaneously"}
\NormalTok{    tasks }\OperatorTok{=}\NormalTok{ [processed\_chunk(chunk) }\ControlFlowTok{for}\NormalTok{ chunk }\KeywordTok{in}\NormalTok{ doc\_chunks]}
    
    \CommentTok{\# Wait for all to finish}
\NormalTok{    extracted\_results }\OperatorTok{=} \ControlFlowTok{await}\NormalTok{ asyncio.gather(}\OperatorTok{*}\NormalTok{tasks)}
    
    \CommentTok{\# Pass results to consolidation}
    \ControlFlowTok{return} \ControlFlowTok{await}\NormalTok{ consolidation\_workflow.arun(extracted\_results)}
\end{Highlighting}
\end{Shaded}

\textbf{Results:}

\begin{itemize}

\item
  \textbf{Latency Reduction:}

  \begin{itemize}
  
  \item
    \emph{Sequential:} \textasciitilde45 minutes for 50 pages (approx.
    54s per page).
  \item
    \emph{Asynchronous:} \textasciitilde4 minutes for 50 pages (limited
    only by the Semaphore).
  \item
    \textbf{Speedup Factor:} \textasciitilde11x.
  \end{itemize}
\item
  \textbf{Resource Efficiency:} The system runs comfortably on a
  standard container (2 vCPU, 4GB RAM) because the heavy lifting is
  offloaded to the API providers. The local embedding model (Ollama) was
  the only significant CPU consumer.
\item
  \textbf{The ``Thundering Herd'' Problem:} Initially, firing 50
  requests caused Qdrant to timeout. Implementing the Semaphore and
  exponential backoff retries stabilized the system.
\end{itemize}

\textbf{Possible Improvements:}

\begin{itemize}

\item
  \textbf{Streaming:} The current system waits for \emph{all} chunks to
  finish before consolidation. A \textbf{Streaming Architecture} could
  begin consolidating ``Flow Control'' rules as soon as the ``Flow''
  section chunks are processed, progressively updating the user UI.
\item
  \textbf{Caching:} Implementing a semantic cache (Redis) for common
  queries (e.g., resolving ``TIC-101'') to avoid redundant LLM calls
  across different documents.
\end{itemize}

\textbf{Literature:}

\begin{itemize}

\item
  \textbf{Dean, J., \& Ghemawat, S. (2008).} \emph{MapReduce: Simplified
  Data Processing on Large Clusters.} (The foundational pattern for this
  architecture).
\item
  \textbf{Fowler, M.} \emph{Patterns of Enterprise Application
  Architecture.} (Reference for the Async/Await and Throttling
  patterns).
\end{itemize}

\chapter{6. Operational Runtime \& Dynamic
Verification}\label{operational-runtime-dynamic-verification}

\emph{Chapter 5 gave us static Python functions. Chapter 6 describes the
``Virtual Machine'' we built to run them safely and efficiently on
streaming data} \emph{Chapter 5: theoretical extraction, Chapter 6:
operational utility}

\section{6.1. The Execution Challenge}\label{the-execution-challenge}

\begin{itemize}

\item
  \textbf{The Problem:} LLMs generate ``naive'' logic (with
  \texttt{status.get()} abstraction). In standard Pandas, this
  recalculates the whole window every tick (\(O(N)\)) .
\item
  \textbf{The Constraint:} Industrial systems cannot afford \(O(N)\)
  latency. We need \(O(1)\).
\item
  \textbf{Hypothesis:} By mapping the extracted
  \texttt{status.get(time\_expr)} calls to an incremental learning
  framework (River), we can achieve constant-time evaluation regardless
  of window size.
\end{itemize}

\section{6.2. Architecture of the Rule
Engine}\label{architecture-of-the-rule-engine}

\begin{itemize}

\item
  \textbf{Component 1: The \texttt{RuleChecker} Container:}

  \begin{itemize}
  
  \item
    Explain how it compiles the text-based Python rules from Chapter 5
    into executable functions (\texttt{\_compile\_rules}).
  \item
    Discuss error boundaries: If one rule crashes, the engine keeps
    running (Fault Tolerance).
  \end{itemize}
\item
  \textbf{Component 2: The \texttt{BufferedStatus} State Manager:}

  \begin{itemize}
  
  \item
    Explain the \texttt{extract\_requirements\_from\_rules} Logic:
    Pre-scanning the AST to allocate memory buffers \emph{before} the
    stream starts.
  \item
    \textbf{Innovation:} Detail \texttt{\_create\_river\_stat} factory.
    Show how we map ``natural language statistics'' to
    \texttt{river.stats.RollingMean}, \texttt{river.stats.RollingVar},
    etc.
  \end{itemize}
\end{itemize}

\section{\texorpdfstring{6.3. Efficient Temporal Resolution (\(O(1)\)
Complexity)}{6.3. Efficient Temporal Resolution (O(1) Complexity)}}\label{efficient-temporal-resolution-o1-complexity}

\begin{itemize}

\item
  \textbf{The \texttt{River} Integration:}

  \begin{itemize}
  
  \item
    Explain the math behind Welford's Algorithm (used by River) to
    update Variance/StdDev with just the \emph{new} point and the
    \emph{old} point, rather than re-reading the whole list.
  \item
    \textbf{Code Highlight:} Reference \texttt{BufferedStatus.update()}
    and \texttt{\_get\_interval\_stat()} methods.
  \item
    \textbf{Benefit:} Demonstrating that a rule checking ``Last 24
    hours'' takes the exact same compute time as a rule checking ``Last
    5 minutes.''
  \end{itemize}
\end{itemize}

\section{6.4. Event Sinking and
Explanations}\label{event-sinking-and-explanations}

\begin{itemize}

\item
  \textbf{Handling Triggers:} Describe the \texttt{RuleEvent} data
  class.
\item
  \textbf{Metadata Preservation:} How the \texttt{rule\_description}
  extracted by the LLM travels with the event to the operator dashboard
  (The \texttt{\_explain\_trigger} method).
\end{itemize}

This is a very common challenge in industrial AI research: \textbf{``We
have data (logs), but no labels (no record of what was truly an
anomaly).''}

In academic terms, you are facing the \textbf{``Cold Start Evaluation
Problem.''}

Your intuition is exactly correct: \textbf{If you don't have Ground
Truth, you must manufacture it.} You should adopt a \textbf{``Synthetic
Gold Standard''} methodology. This is widely accepted in computer
science research when real-world labels are unavailable.

Here is the strategy for \textbf{Chapter 7}, specifically designed to
solve your ``no labels'' problem by splitting the evaluation into
\textbf{Extraction Accuracy} (NLP) and \textbf{Execution Correctness}
(Systems).

\chapter{7. Evaluation and Results}\label{evaluation-and-results}

\section{7.1. Evaluation Methodology}\label{evaluation-methodology}

Since historical anomaly logs are unavailable for the Repsol case study,
we adopt a \textbf{Component-Wise Evaluation Strategy}. We decouple the
assessment of the \emph{Language Model's understanding} from the
\emph{Runtime Engine's performance}.

We establish a \textbf{Taxonomy of Rule Complexity} to categorize test
cases: 1. \textbf{Type I (Explicit):} ``The maximum temperature of T-101
is 50Â°C.'' (Simple extraction). 2. \textbf{Type II (Tabular):} Rules
hidden inside specification tables. (Layout awareness). 3. \textbf{Type
III (Temporal/Complex):} ``If pressure drops by 10\% over 5 minutes
while flow is stable\ldots{}'' (Logic + Time parsing). 4. \textbf{Type
IV (Implicit/Domain):} ``Ensure the reflux drum does not run dry.''
(Requires grounding to know ``dry'' means Level \textless{} 5\%).

\section{7.2. Evaluation of Rule Extraction (NLP
Performance)}\label{evaluation-of-rule-extraction-nlp-performance}

\emph{Instead of guessing if the extraction on the real document is
correct, we ``Reverse Engineer'' a test document.}

\subsection{7.2.1. Dataset Generation (The ``Twin''
Approach)}\label{dataset-generation-the-twin-approach}

We created a \textbf{Synthetic Evaluation Dataset} consisting of 50
``Ground Truth Rules'' (\(R_{GT}\)) covering the four complexity types.
1. \textbf{Step 1 (Define Truth):} We manually wrote 50 Python functions
representing ideal operational logic. 2. \textbf{Step 2 (Generate
Artifact):} We used an LLM to ``reverse'' these functions into a generic
Industrial Technical Manual (PDF) containing natural language, tables,
and scattered context. 3. \textbf{Step 3 (Blind Extraction):} The
pipeline processed this synthetic PDF to generate \(R_{Pred}\)
(Predicted Rules).

\subsection{7.2.2. Semantic Similarity
Metrics}\label{semantic-similarity-metrics}

Since the extracted text won't match the Ground Truth word-for-word, we
evaluate based on \textbf{Semantic Equivalence} (using \texttt{ast}
comparison or embedding similarity): * \textbf{Precision:}
\(\frac{\text{Correctly Extracted Rules}}{\text{Total Extracted Rules}}\)
(Did we hallucinate?) * \textbf{Recall:}
\(\frac{\text{Correctly Extracted Rules}}{\text{Total Ground Truth Rules}}\)
(Did we miss anything?) * \textbf{Code Pass Rate:} Percentage of
extracted rules that are syntactically valid Python.

\textbf{Expected Results Table:} \textbar{} Complexity \textbar{}
Precision \textbar{} Recall \textbar{} Challenges Identified \textbar{}
\textbar{} --- \textbar{} --- \textbar{} --- \textbar{} --- \textbar{}
\textbar{} Type I (Explicit) \textbar{} 1.00 \textbar{} 1.00 \textbar{}
Solved problem. \textbar{} \textbar{} Type II (Tables) \textbar{} 0.95
\textbar{} 0.90 \textbar{} Docling handles this well; merged cells cause
minor issues. \textbar{} \textbar{} Type III (Temporal)\textbar{} 0.85
\textbar{} 0.80 \textbar{} ``5 minutes ago'' vs ``last 5 minutes''
ambiguity. \textbar{} \textbar{} Type IV (Implicit)\textbar{} 0.70
\textbar{} 0.60 \textbar{} Requires Grounding (Search) to resolve ``run
dry.'' \textbar{}

\section{7.3. Evaluation of Sensor Resolution (Entity
Linking)}\label{evaluation-of-sensor-resolution-entity-linking}

\emph{Here, you test the mapping of ``Column Temp'' -\textgreater{}
``TIC-101''.}

\begin{itemize}

\item
  \textbf{Method:} Create a ``Mock Sensor Database'' (CSV) with 100 tags
  (some with similar names).
\item
  \textbf{Test:} Check if the pipeline resolves ambiguous terms
  correctly in the Synthetic Document.
\item
  \textbf{Metric:} \textbf{Resolution Accuracy @ k} (Did the correct tag
  appear in the top 1 or top 3 choices?).
\item
  \textbf{Result:} Show that your \textbf{Context-Aware} approach
  (Section 5.4) outperforms standard Vector Search (RAG) by correct
  disambiguating ``Control'' vs.~``Indicator'' tags.
\end{itemize}

\section{7.4. Evaluation of Runtime Execution (System
Performance)}\label{evaluation-of-runtime-execution-system-performance}

\emph{Since you cannot verify if real alarms are correct, you verify if
the ENGINE is mathematically correct and fast.}

\subsection{7.4.1. Correctness Verification (The ``Shadow
Test'')}\label{correctness-verification-the-shadow-test}

We prove that the \texttt{RuleChecker} (River/Streaming) produces the
exact same math as a standard batch processor (Pandas). *
\textbf{Setup:} Take a slice of your sensor data (e.g., 10,000 rows). *
\textbf{Method:} 1. Run Rule X using
\texttt{pandas.rolling(window=\textquotesingle{}1h\textquotesingle{}).mean()}.
2. Run Rule X using \texttt{RuleChecker} (River). 3. Compare the output
values at every timestamp. * \textbf{Result:} Mean Squared Error (MSE)
between the two should be \(\approx 0\). This proves your \(O(1)\)
streaming implementation is \textbf{mathematically trustworthy}.

\subsection{7.4.2. Fault Injection (The ``Fire
Drill'')}\label{fault-injection-the-fire-drill}

Since the data might be ``normal'' (no alarms), we artificially inject
faults to prove the rules \emph{would} trigger. * \textbf{Method:} 1.
Select a rule: ``Alert if Temp \textgreater{} 100''. 2. Modify the CSV:
At \(t=500\), manually set Temp = 105. 3. Run \texttt{RuleChecker}. *
\textbf{Success Metric:} \textbf{Detection Rate}. Did the system log an
event at exactly \(t=500\)? * \emph{This validates that your system
isn't just running code, but actually catching events.}

\subsection{7.4.3. Computational Performance
(Scalability)}\label{computational-performance-scalability}

\emph{Measure the efficiency of your \texttt{RuleChecker}.} *
\textbf{Metric 1: Latency per Row.} (e.g., ``Processed 1,000 rules in
2.5ms''). * \textbf{Metric 2: Memory Stability.} - Show that memory
usage remains flat (constant) over 1 hour of processing, proving the
\(O(1)\) claim vs.~Pandas which might grow with window size.
