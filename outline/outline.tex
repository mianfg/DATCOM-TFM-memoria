\chapter{0. Abstract}\label{abstract}

\part{Introduction}

\emph{Brief summary of problem, approach, contributions, and results}

\chapter{1. Introduction}\label{introduction}

\section{1.1. Motivation and Industrial
Context}\label{motivation-and-industrial-context}

\begin{itemize}
\tightlist
\item
  Industrial process monitoring and anomaly detection challenges
\item
  \textbf{Case Study Introduction}: C3/C4 splitter separation at Repsol

  \begin{itemize}
  \tightlist
  \item
    Process description (what can be shared publicly)
  \item
    Why this is a representative problem: complexity, usage
  \item
    The knowledge documentation challenge
  \end{itemize}
\item
  The gap: No consistent automated rule extraction framework from
  technical documents
\end{itemize}

\section{1.2. Problem Statement}\label{problem-statement}

\begin{itemize}
\tightlist
\item
  Knowledge trapped in unstructured documentation (PDFs, manuals, specs)
\item
  Domain experts required for rule definition (expensive, not scalable)
\item
  Usefulness and application of these rules (control of processes,
  alarms, tracking of ideal behavior\ldots)
\item
  Need for: automated extraction, explainability, maintainability,
  traceability
\end{itemize}

\section{1.3. Research Questions}\label{research-questions}

\emph{These questions will be addressed and referenced throughout the
whole work}

\begin{itemize}
\tightlist
\item
  \emph{RQ1}: How can operational knowledge embedded in unstructured
  industrial documentation be automatically extracted and formalized
  into executable rules? Can LLMs effectively extract these rules? / Can
  domain-specific operational knowledge be effectively extracted from
  unstructured technical documentation without domain experts?

  \begin{itemize}
  \tightlist
  \item
    Opens up: is automated extraction feasible? what representations
    work? what extraction methods? what are the challenges?
  \item
    Answer: proposed method (LLMs + multi-stage pipeline with retrieval
    and grounding + Python functions)
  \end{itemize}
\item
  \emph{RQ2}: What are the fundamental challenges in translating natural
  language process specifications into machine-executable monitoring
  rules, and how can they be addressed? / What are the key technical
  barriers to automating industrial rule extraction, and what approaches
  can address them?

  \begin{itemize}
  \tightlist
  \item
    Opens up: sensor ambiguity and retrieval, time expressions,
    validation, domain knowledge gaps
  \item
    Answer: proposed method (sensor resolution, time parsing, grounding,
    verification)
  \end{itemize}
\item
  \emph{RQ3}: To what extent can large language models reason about
  industrial processes? / What role can external knowledge and context
  play in improving the accuracy and completeness of LLM-based rule
  extraction?

  \begin{itemize}
  \tightlist
  \item
    Opens up: do LLMs understand industrial concepts? do they need
    context or grounding?
  \item
    Answer: RAG + grounding approach proposed, interesting to talk about
    RAG vs.~fine-tuning here and general-purpose (foundational) models
  \end{itemize}
\item
  \emph{RQ4}: What properties must an automated rule extraction system
  possess to be trustworthy and deployable in industrial settings? / How
  can automatically extracted rules achieve the level of trustworthiness
  required for industrial deployment?

  \begin{itemize}
  \tightlist
  \item
    Opens up: explainability, traceability; as future work: HITL and
    human validation/oversight
  \item
    Answer: traceability, explainability in extraction and consolidation
  \end{itemize}
\item
  \emph{RQ5}: How can the quality and consistency of automatically
  extracted rules be assessed and improved without extensive manual
  review?

  \begin{itemize}
  \tightlist
  \item
    Opens up: quality metrics, redundancy detection, consolidation
  \item
    Answer: consolidation workflow
  \end{itemize}
\end{itemize}

TODO: To what extent can this approach generalize to real-world
industrial use cases? (not as research question, but in conclusions)

\section{1.4. Research Approach and
Objectives}\label{research-approach-and-objectives}

\textbf{Research approach:} - Iterative design methodology - Focus on
modular, swappable components: analyze implementation options and
justify chosen one, for each component - Validation with real industrial
documentation

\textbf{Primary Objective:} Design and validate an end-to-end framework
for automated extraction of operational monitoring rules from
unstructured industrial documentation

\textbf{Specific Objectives:} - \emph{O1}: Develop a rule extraction
methodology that bridges the gap between natural language specifications
and executable code - \emph{O1.1}: Define a formal rule representation
suitable for industrial monitoring (executable Python functions) -
\emph{O1.2}: Design a multi-stage extraction pipeline that handles the
complexity of industrial documentation - \emph{O1.3}: Ensure rules are
syntactically valid, semantically correct, and traceable to source
documents - \emph{O2}: Investigate and implement mechanisms to overcome
LLM limitations in domain-specific industrial contexts - \emph{O2.1}:
Evaluate the impact of Retrieval-Augmented Generation (RAG) on
extraction quality - \emph{O2.2}: Assess the value of external knowledge
grounding for industrial rule extraction - \emph{O2.3}: Identify which
stages of the pipeline benefit most from contextual information -
\emph{O3}: Address the semantic ambiguity inherent in industrial
documentation - \emph{O3.1}: Develop methods for resolving natural
language sensor references to concrete identifiers - \emph{O3.2}: Create
a framework for parsing temporal expressions into structured time-series
queries - \emph{O3.3}: Design validation mechanisms to catch and correct
semantic errors - \emph{O4}: Establish quality assurance mechanisms for
automatically extracted rules - \emph{O4.1}: Implement multi-level
verification (syntactic, semantic, domain-specific) - \emph{O4.2}:
Design and evaluate automatic rule consolidation to reduce redundancy -
\emph{O4.3}: Quantify the confidence and reliability of extracted rules
- \emph{O5}: Ensure trustworthiness through explainability and
traceability - \emph{O5.1}: Implement complete provenance tracking from
documents to rules - \emph{O5.2}: Enable inspection of intermediate
processing steps and reasoning - \emph{O5.3}: Provide mechanisms for
human review and intervention - \emph{O6}: Validate the approach
theoretically (mock cases) and on real-world industrial use cases -
\emph{O6.1}: Apply the framework to the C3/C4 splitter separation
process at Repsol - \emph{O6.2}: Evaluate practical viability,
limitations, and generalizability - \emph{O6.3}: Identify success
factors and failure modes

\textbf{Mapping from objectives to implementation}

\chapter{Final Research Objectives and Implementation
Components}\label{final-research-objectives-and-implementation-components}

\section{Research Objectives (O)}\label{research-objectives-o}

\textbf{O1: Develop a rule extraction methodology that bridges the gap
between natural language specifications and executable code} - O1.1:
Define a formal rule representation suitable for industrial monitoring
(executable Python functions) - O1.2: Design a multi-stage extraction
pipeline that handles the complexity of industrial documentation - O1.3:
Ensure rules are syntactically valid, semantically correct, and
traceable to source documents

\textbf{O2: Investigate and implement mechanisms to overcome LLM
limitations in domain-specific industrial contexts} - O2.1: Implement
Retrieval-Augmented Generation (RAG) for context enrichment - O2.2:
Integrate external knowledge grounding for domain-specific information -
O2.3: Identify and leverage contextual information at appropriate
pipeline stages

\textbf{O3: Address the semantic ambiguity inherent in industrial
documentation} - O3.1: Develop methods for resolving natural language
sensor references to concrete identifiers - O3.2: Create a framework for
parsing temporal expressions into structured time-series queries - O3.3:
Design validation mechanisms to catch and correct semantic errors

\textbf{O4: Establish quality assurance mechanisms for automatically
extracted rules} - O4.1: Implement multi-level verification (syntactic,
semantic, domain-specific) - O4.2: Design and implement automatic rule
consolidation to reduce redundancy - O4.3: Quantify the confidence and
reliability of extracted rules

\textbf{O5: Ensure trustworthiness through explainability and
traceability} - O5.1: Implement complete provenance tracking from
documents to rules - O5.2: Enable inspection of intermediate processing
steps and reasoning - O5.3: Provide mechanisms for human review and
intervention

\textbf{O6: Validate the approach on mock cases and real-world
industrial use cases} - O6.1: Develop mock test cases for system
validation - O6.2: Apply the framework to the C3/C4 splitter separation
process at Repsol - O6.3: Evaluate practical viability, limitations, and
generalizability

\textbf{O7: Design a scalable, production-ready system architecture} -
O7.1: Implement multi-tenant operation via collection isolation - O7.2:
Design modular, protocol-based components for maintainability and
swappability - O7.3: Enable real-time monitoring, observability, and
asynchronous processing

\textbf{O8: Establish evaluation methodology and conduct systematic
assessment} \emph{(WIP - not primary focus)} - O8.1: Define metrics for
assessing rule extraction quality - O8.2: Design ablation studies to
understand component contributions - O8.3: Document success factors,
failure modes, and limitations

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Implementation Components
(I)}\label{implementation-components-i}

\textbf{I1: Rule Representation and Formal Specification} - I1.1: Python
function format with \texttt{status.get()} API for time-series queries -
I1.2: Rule data model (SQLAlchemy ORM) with lifecycle states (extracted
→ consolidated → superseded → active) - I1.3: Sensor data model and CSV
ingestion system - I1.4: Time expression Domain-Specific Language
(TimeDelta, TimeDeltaInterval, Statistic enums)

\textbf{I2: Document Processing Infrastructure} - I2.1: Docling-based
document loader for heterogeneous formats (PDF, Markdown, DOCX) - I2.2:
Document chunking strategy with RecursiveCharacterTextSplitter - I2.3:
Document and Chunk data models with content hashing for deduplication -
I2.4: File storage system with path management

\textbf{I3: Vector Store and Retrieval System} - I3.1: Qdrant vector
store integration with collection management - I3.2: Embedding
generation using mxbai-embed-large via Ollama - I3.3: Collection-scoped
semantic retrieval with configurable top-k - I3.4: Context gathering
workflow stage (self-context expansion pattern)

\textbf{I4: External Knowledge Grounding System} - I4.1: Grounding
workflow stage with conditional execution - I4.2: Tavily web search API
integration - I4.3: LLM-driven search query generation (2-4 focused
queries) - I4.4: Search result synthesis and context enrichment

\textbf{I5: LLM Orchestration and Workflow Engine} - I5.1: LangGraph
state machine implementation (RuleExtractionWorkflow) - I5.2:
Multi-stage workflow pipeline (gather context → ground → extract →
resolve sensors → parse time → verify) - I5.3: State persistence,
checkpointing, and resumability - I5.4: Langfuse integration for
observability and tracing

\textbf{I6: Rule Extraction Engine} - I6.1: Extraction workflow stage
with structured Pydantic outputs - I6.2: Prompt engineering with
few-shot examples and rule templates - I6.3: Output schemas (PythonRule,
ExtractedRules models) - I6.4: LLM provider abstraction layer (DeepSeek,
OpenAI, Ollama)

\textbf{I7: Sensor Resolution System} - I7.1: AST-based extraction of
\texttt{status.get()} calls using StatusCallExtractor visitor pattern -
I7.2: LLM-based sensor name → sensor ID mapping with available sensor
list - I7.3: Rule body transformation via string replacement - I7.4:
Sensor parsing status tracking (no\_sensors \textbar{}
sensors\_not\_found \textbar{} ok)

\textbf{I8: Time Expression Parsing System} - I8.1: Time expression
domain models (unit.py, delta.py, interval.py, statistic.py) - I8.2:
LLM-guided natural language time expression parsing - I8.3: Formal
validation via TimeDeltaInterval.from\_str() parser - I8.4: Point
vs.~interval semantic distinction and enforcement - I8.5: Statistic
requirement validation (required for intervals, forbidden for points)

\textbf{I9: Rule Verification Framework} - I9.1: Python syntax
validation via AST parsing - I9.2: Sensor ID validation against
collection sensor inventory - I9.3: Time expression format validation -
I9.4: Statistic validation with interval/point semantics - I9.5:
Verification status tracking per rule (ok \textbar{} syntax\_error
\textbar{} invalid\_sensor \textbar{} invalid\_time \textbar{}
invalid\_statistic)

\textbf{I10: Rule Consolidation System} - I10.1: Consolidation workflow
(RuleConsolidationWorkflow with LangGraph) - I10.2: Rule batching
strategy (max 50 rules per batch to prevent context overflow) - I10.3:
LLM-driven consolidation actions (remove \textbar{} merge \textbar{}
simplify) - I10.4: Confidence scoring (0.0-1.0) and configurable
thresholding - I10.5: Rule lifecycle management and superseded rule
tracking - I10.6: Verification of consolidated rules through same
validation pipeline

\textbf{I11: Traceability and Provenance System} - I11.1: Full lineage
tracking via foreign keys (Rule → Task → Chunk → Document → Collection)
- I11.2: Context chunk storage (RuleContextChunk join table with
relevance scores) - I11.3: Grounding search storage (RuleGroundingSearch
table with query and results) - I11.4: LangGraph thread ID tracking for
workflow replay and debugging - I11.5: Metadata storage for all
intermediate processing steps

\textbf{I12: Job Execution and Orchestration} - I12.1: ProcessingJob and
ProcessingTask data models with status tracking - I12.2: Parallel task
execution with asyncio and configurable concurrency limits - I12.3: Job
executor with error handling, retries, and recovery mechanisms - I12.4:
Real-time progress tracking and statistics aggregation

\textbf{I13: API and Integration Layer} - I13.1: REST API (FastAPI) with
modular routers (collections, documents, jobs, rules, consolidation,
sensors) - I13.2: WebSocket manager for real-time job progress updates -
I13.3: Background job execution with async task queuing - I13.4:
Multi-tenancy via collection isolation (separate Qdrant collections per
project)

\textbf{I14: Architecture and Infrastructure} - I14.1: Centralized
configuration management (Pydantic Settings with .env support) - I14.2:
Dependency injection containers for agent and API layers - I14.3:
Protocol-based design for component swappability (EmbeddingProvider,
VectorStoreProvider, LLMProvider, DocumentLoader) - I14.4: Hexagonal
architecture pattern (domain/application/infrastructure separation)

\textbf{I15: Validation Data and Test Cases} - I15.1: Mock test
resources (algae\_mock, c3c4\_splitter examples) - I15.2: Real C3/C4
Repsol collection data (confidential, results-only) - I15.3: Storage
structure for multi-collection data management - I15.4: Usage examples
and notebooks (usage.ipynb, usage\_examples.ipynb, api.ipynb)

\textbf{I16: Evaluation Framework} \emph{(WIP)} - I16.1: Metric
definitions (precision, recall, rule validity rate, consolidation
quality) - I16.2: Ablation study infrastructure (with/without RAG,
grounding, consolidation) - I16.3: Baseline comparison framework -
I16.4: Results analysis and visualization tools

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Objective-to-Implementation
Mapping}\label{objective-to-implementation-mapping}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Objective & Implementation Components & Status \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{O1: Rule extraction methodology} & & \\
O1.1: Formal representation & I1.1, I1.2, I1.4 & ✅ Complete \\
O1.2: Multi-stage pipeline & I2.1-I2.4, I5.1, I5.2 & ✅ Complete \\
O1.3: Validity \& traceability & I9.1-I9.5, I11.1-I11.5 & ✅ Complete \\
\textbf{O2: Overcome LLM limitations} & & \\
O2.1: RAG implementation & I3.1-I3.4 & ✅ Complete \\
O2.2: Grounding integration & I4.1-I4.4 & ✅ Complete \\
O2.3: Stage-wise leverage & I5.2, I5.3, I11.5 & ✅ Complete \\
\textbf{O3: Semantic ambiguity} & & \\
O3.1: Sensor resolution & I7.1-I7.4 & ✅ Complete \\
O3.2: Temporal parsing & I8.1-I8.5 & ✅ Complete \\
O3.3: Validation mechanisms & I9.1-I9.5 & ✅ Complete \\
\textbf{O4: Quality assurance} & & \\
O4.1: Multi-level verification & I9.1-I9.5 & ✅ Complete \\
O4.2: Consolidation & I10.1-I10.6 & ✅ Complete \\
O4.3: Confidence quantification & I10.4 & ✅ Complete \\
\textbf{O5: Trustworthiness} & & \\
O5.1: Provenance tracking & I11.1-I11.3 & ✅ Complete \\
O5.2: Inspect intermediates & I11.4, I11.5, I5.3, I5.4 & ✅ Complete \\
O5.3: Human review & I13.1, I13.2 & ✅ Complete (API) \\
\textbf{O6: Validation} & & \\
O6.1: Mock cases & I15.1 & ✅ Complete \\
O6.2: C3/C4 case study & I15.2, I15.3 & ✅ Complete \\
O6.3: Viability evaluation & I15.4, I16.1-I16.4 & �� Partial \\
\textbf{O7: Scalable architecture} & & \\
O7.1: Multi-tenancy & I13.4 & ✅ Complete \\
O7.2: Modular design & I14.2, I14.3, I14.4 & ✅ Complete \\
O7.3: Observability & I5.4, I12.4, I13.2 & ✅ Complete \\
\textbf{O8: Evaluation methodology} & & \\
O8.1: Metric definitions & I16.1 & �� WIP \\
O8.2: Ablation studies & I16.2 & �� WIP \\
O8.3: Success/failure analysis & I16.3, I16.4 & �� WIP \\
\end{longtable}
}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Summary Statistics}\label{summary-statistics}

\begin{itemize}
\tightlist
\item
  \textbf{Total Objectives}: 8 (O1-O8)
\item
  \textbf{Total Sub-objectives}: 25
\item
  \textbf{Total Implementation Components}: 16 (I1-I16)
\item
  \textbf{Total Implementation Details}: 80+
\end{itemize}

\textbf{Implementation Status}: - ✅ \textbf{Complete}: O1-O7 (core
system) - �� \textbf{Work in Progress}: O8 (evaluation - not primary
focus)

\textbf{Coverage}: - All core objectives (O1-O7) have corresponding
implementations - All implementations map to at least one objective -
Evaluation framework (I16) is partially implemented to support future
work

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

This structure clearly shows that your \textbf{system is complete and
production-ready} (O1-O7), while \textbf{evaluation is ongoing work}
(O8) that won't be the central focus of the thesis but can be mentioned
as future validation work.
